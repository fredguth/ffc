{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# !pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('src/blip')\n",
    "# sys.path.append('src/clip')\n",
    "\n",
    "import clip\n",
    "# import gradio as gr\n",
    "import hashlib\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.blip import blip_decoder\n",
    "\n",
    "\n",
    "from share_btn import community_icon_html, loading_icon_html, share_js\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading BLIP model...\")\n",
    "blip_image_eval_size = 384\n",
    "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'        \n",
    "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='large', med_config='./src/blip/configs/med_config.json')\n",
    "blip_model.eval()\n",
    "blip_model = blip_model.to(device)\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "clip_model_name = 'ViT-L/14' # https://huggingface.co/openai/clip-vit-large-patch14\n",
    "clip_model, clip_preprocess = clip.load(clip_model_name, device=device)\n",
    "clip_model.to(device).eval()\n",
    "\n",
    "chunk_size = 2048\n",
    "flavor_intermediate_count = 2048\n",
    "\n",
    "\n",
    "class LabelTable():\n",
    "    def __init__(self, labels, desc):\n",
    "        self.labels = labels\n",
    "        self.embeds = []\n",
    "\n",
    "        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n",
    "\n",
    "        os.makedirs('./cache', exist_ok=True)\n",
    "        cache_filepath = f\"./cache/{desc}.pkl\"\n",
    "        if desc is not None and os.path.exists(cache_filepath):\n",
    "            with open(cache_filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                if data['hash'] == hash:\n",
    "                    self.labels = data['labels']\n",
    "                    self.embeds = data['embeds']\n",
    "\n",
    "        if len(self.labels) != len(self.embeds):\n",
    "            self.embeds = []\n",
    "            chunks = np.array_split(self.labels, max(1, len(self.labels)/chunk_size))\n",
    "            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None):\n",
    "                text_tokens = clip.tokenize(chunk).to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_features = clip_model.encode_text(text_tokens).float()\n",
    "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features.half().cpu().numpy()\n",
    "                for i in range(text_features.shape[0]):\n",
    "                    self.embeds.append(text_features[i])\n",
    "\n",
    "            with open(cache_filepath, 'wb') as f:\n",
    "                pickle.dump({\"labels\":self.labels, \"embeds\":self.embeds, \"hash\":hash}, f)\n",
    "    \n",
    "    def _rank(self, image_features, text_embeds, top_count=1):\n",
    "        top_count = min(top_count, len(text_embeds))\n",
    "        similarity = torch.zeros((1, len(text_embeds))).to(device)\n",
    "        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).float().to(device)\n",
    "        for i in range(image_features.shape[0]):\n",
    "            similarity += (image_features[i].unsqueeze(0) @ text_embeds.T).softmax(dim=-1)\n",
    "        _, top_labels = similarity.cpu().topk(top_count, dim=-1)\n",
    "        return [top_labels[0][i].numpy() for i in range(top_count)]\n",
    "\n",
    "    def rank(self, image_features, top_count=1):\n",
    "        if len(self.labels) <= chunk_size:\n",
    "            tops = self._rank(image_features, self.embeds, top_count=top_count)\n",
    "            return [self.labels[i] for i in tops]\n",
    "\n",
    "        num_chunks = int(math.ceil(len(self.labels)/chunk_size))\n",
    "        keep_per_chunk = int(chunk_size / num_chunks)\n",
    "\n",
    "        top_labels, top_embeds = [], []\n",
    "        for chunk_idx in tqdm(range(num_chunks)):\n",
    "            start = chunk_idx*chunk_size\n",
    "            stop = min(start+chunk_size, len(self.embeds))\n",
    "            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk)\n",
    "            top_labels.extend([self.labels[start+i] for i in tops])\n",
    "            top_embeds.extend([self.embeds[start+i] for i in tops])\n",
    "\n",
    "        tops = self._rank(image_features, top_embeds, top_count=top_count)\n",
    "        return [top_labels[i] for i in tops]\n",
    "\n",
    "def generate_caption(pil_image):\n",
    "    gpu_image = T.Compose([\n",
    "        T.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=TF.InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
    "    return caption[0]\n",
    "\n",
    "def load_list(filename):\n",
    "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        items = [line.strip() for line in f.readlines()]\n",
    "    return items\n",
    "\n",
    "def rank_top(image_features, text_array):\n",
    "    text_tokens = clip.tokenize([text for text in text_array]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = torch.zeros((1, len(text_array)), device=device)\n",
    "    for i in range(image_features.shape[0]):\n",
    "        similarity += (image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    _, top_labels = similarity.cpu().topk(1, dim=-1)\n",
    "    return text_array[top_labels[0][0].numpy()]\n",
    "\n",
    "def similarity(image_features, text):\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()       \n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    return similarity[0][0]\n",
    "\n",
    "def interrogate(image):\n",
    "    caption = generate_caption(image)\n",
    "\n",
    "    images = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(images).float()\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    flaves = flavors.rank(image_features, flavor_intermediate_count)\n",
    "    best_medium = mediums.rank(image_features, 1)[0]\n",
    "    best_artist = artists.rank(image_features, 1)[0]\n",
    "    best_trending = trendings.rank(image_features, 1)[0]\n",
    "    best_movement = movements.rank(image_features, 1)[0]\n",
    "\n",
    "    best_prompt = caption\n",
    "    best_sim = similarity(image_features, best_prompt)\n",
    "\n",
    "    def check(addition):\n",
    "        nonlocal best_prompt, best_sim\n",
    "        prompt = best_prompt + \", \" + addition\n",
    "        sim = similarity(image_features, prompt)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_prompt = prompt\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_multi_batch(opts):\n",
    "        nonlocal best_prompt, best_sim\n",
    "        prompts = []\n",
    "        for i in range(2**len(opts)):\n",
    "            prompt = best_prompt\n",
    "            for bit in range(len(opts)):\n",
    "                if i & (1 << bit):\n",
    "                    prompt += \", \" + opts[bit]\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        prompt = rank_top(image_features, prompts)\n",
    "        sim = similarity(image_features, prompt)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_prompt = prompt\n",
    "\n",
    "    check_multi_batch([best_medium, best_artist, best_trending, best_movement])\n",
    "\n",
    "    extended_flavors = set(flaves)\n",
    "    for _ in tqdm(range(25), desc=\"Flavor chain\"):\n",
    "        try:\n",
    "            best = rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
    "            flave = best[len(best_prompt)+2:]\n",
    "            if not check(flave):\n",
    "                break\n",
    "            extended_flavors.remove(flave)\n",
    "        except:\n",
    "            # exceeded max prompt length\n",
    "            break\n",
    "\n",
    "    return best_prompt\n",
    "\n",
    "\n",
    "sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
    "trending_list = [site for site in sites]\n",
    "trending_list.extend([\"trending on \"+site for site in sites])\n",
    "trending_list.extend([\"featured on \"+site for site in sites])\n",
    "trending_list.extend([site+\" contest winner\" for site in sites])\n",
    "\n",
    "raw_artists = load_list('data/artists.txt')\n",
    "artists = [f\"by {a}\" for a in raw_artists]\n",
    "artists.extend([f\"inspired by {a}\" for a in raw_artists])\n",
    "\n",
    "artists = LabelTable(artists, \"artists\")\n",
    "flavors = LabelTable(load_list('data/flavors.txt'), \"flavors\")\n",
    "mediums = LabelTable(load_list('data/mediums.txt'), \"mediums\")\n",
    "movements = LabelTable(load_list('data/movements.txt'), \"movements\")\n",
    "trendings = LabelTable(trending_list, \"trendings\")\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "    return interrogate(image), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
    "\n",
    "title = \"\"\"\n",
    "    <div style=\"text-align: center; max-width: 650px; margin: 0 auto;\">\n",
    "        <div\n",
    "        style=\"\n",
    "            display: inline-flex;\n",
    "            align-items: center;\n",
    "            gap: 0.8rem;\n",
    "            font-size: 1.75rem;\n",
    "        \"\n",
    "        >\n",
    "        <h1 style=\"font-weight: 900; margin-bottom: 7px;\">\n",
    "            CLIP Interrogator\n",
    "        </h1>\n",
    "        </div>\n",
    "        <p style=\"margin-bottom: 10px; font-size: 94%\">\n",
    "        Want to figure out what a good prompt might be to create new images like an existing one? The CLIP Interrogator is here to get you answers!\n",
    "        </p>\n",
    "    </div>\n",
    "\"\"\"\n",
    "article = \"\"\"\n",
    "<div style=\"text-align: center; max-width: 650px; margin: 0 auto;\">\n",
    "    <p>\n",
    "    Example art by <a href=\"https://pixabay.com/illustrations/watercolour-painting-art-effect-4799014/\">Layers</a> \n",
    "    and <a href=\"https://pixabay.com/illustrations/animal-painting-cat-feline-pet-7154059/\">Lin Tong</a> \n",
    "    from pixabay.com\n",
    "    </p>\n",
    "\n",
    "    <p>\n",
    "    Server busy? You can also run on <a href=\"https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb\">Google Colab</a>\n",
    "    </p>\n",
    "\n",
    "    <p>\n",
    "    Has this been helpful to you? Follow me on twitter \n",
    "    <a href=\"https://twitter.com/pharmapsychotic\">@pharmapsychotic</a> \n",
    "    and check out more tools at my\n",
    "    <a href=\"https://pharmapsychotic.com/tools.html\">Ai generative art tools list</a>\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "css = '''\n",
    "#col-container {max-width: 700px; margin-left: auto; margin-right: auto;}\n",
    "a {text-decoration-line: underline; font-weight: 600;}\n",
    ".animate-spin {\n",
    "    animation: spin 1s linear infinite;\n",
    "}\n",
    "@keyframes spin {\n",
    "    from {\n",
    "        transform: rotate(0deg);\n",
    "    }\n",
    "    to {\n",
    "        transform: rotate(360deg);\n",
    "    }\n",
    "}\n",
    "#share-btn-container {\n",
    "    display: flex; padding-left: 0.5rem !important; padding-right: 0.5rem !important; background-color: #000000; justify-content: center; align-items: center; border-radius: 9999px !important; width: 13rem;\n",
    "}\n",
    "#share-btn {\n",
    "    all: initial; color: #ffffff;font-weight: 600; cursor:pointer; font-family: 'IBM Plex Sans', sans-serif; margin-left: 0.5rem !important; padding-top: 0.25rem !important; padding-bottom: 0.25rem !important;\n",
    "}\n",
    "#share-btn * {\n",
    "    all: unset;\n",
    "}\n",
    "#share-btn-container div:nth-child(-n+2){\n",
    "    width: auto !important;\n",
    "    min-height: 0px !important;\n",
    "}\n",
    "#share-btn-container .wrap {\n",
    "    display: none !important;\n",
    "}\n",
    "'''\n",
    "\n",
    "with gr.Blocks(css=css) as block:\n",
    "    with gr.Column(elem_id=\"col-container\"):\n",
    "        gr.HTML(title)\n",
    "\n",
    "        input_image = gr.Image(type='pil', elem_id=\"input-img\")\n",
    "        submit_btn = gr.Button(\"Submit\")\n",
    "        output_text = gr.Textbox(label=\"Output\", elem_id=\"output-txt\")\n",
    "\n",
    "        with gr.Group(elem_id=\"share-btn-container\"):\n",
    "            community_icon = gr.HTML(community_icon_html, visible=False)\n",
    "            loading_icon = gr.HTML(loading_icon_html, visible=False)\n",
    "            share_button = gr.Button(\"Share to community\", elem_id=\"share-btn\", visible=False)\n",
    "\n",
    "        examples=[['example01.jpg'], ['example02.jpg']]\n",
    "        ex = gr.Examples(examples=examples, fn=inference, inputs=input_image, outputs=[output_text, share_button, community_icon, loading_icon], cache_examples=True, run_on_click=True)\n",
    "        ex.dataset.headers = [\"\"]\n",
    "        \n",
    "        gr.HTML(article)\n",
    "\n",
    "    submit_btn.click(fn=inference, inputs=input_image, outputs=[output_text, share_button, community_icon, loading_icon])\n",
    "    share_button.click(None, [], [], _js=share_js)\n",
    "\n",
    "block.queue(max_size=32).launch(show_api=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b806adfb64333d0ca5c14ed2dbf613d5d551ec856d702e8a01588c05fb48e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
