[
  {
    "objectID": "02-sd-concepts-Copy2.html",
    "href": "02-sd-concepts-Copy2.html",
    "title": "ffc",
    "section": "",
    "text": "from PIL import Image\nfrom fastcore.all import concat\nimport torch, logging\nfrom pathlib import Path\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nlogging.disable(logging.WARNING)\n\nimport pathlib\nfrom fastai.vision.all import *\n\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\n/home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n  warn(f\"Failed to load image Python extension: {e}\")\nThe weights are cached in your home directory by default.\nDisable safety checker"
  },
  {
    "objectID": "02-sd-concepts-Copy2.html#cultural-bias",
    "href": "02-sd-concepts-Copy2.html#cultural-bias",
    "title": "ffc",
    "section": "Cultural Bias",
    "text": "Cultural Bias\n\nLabrador example\n\nbaseprompt = \"Labrador in the style of \"\n\n\nimgsB = pipe([baseprompt+a for a in Brazilian], num_inference_steps=30).images\nimgsI = pipe([baseprompt+a for a in International], num_inference_steps=30).images\n\n\n\n\n\n\n\n\nimage_grid(imgsB, 2,3)\n\n\n\n\n\nimage_grid(imgsI, 2,3)\n\n\n\n\n\n\nAstronaut example\n\nastroprompt = \"a painting of an astronaut riding a horse in the style of \"\n\n\nastroB = pipe([astroprompt+a for a in Brazilian], num_inference_steps=30).images\nastroI = pipe([astroprompt+a for a in International], num_inference_steps=30).images\n\n\n\n\n\n\n\n\nimage_grid(astroB, 2,3)\n\n\n\n\n\nimage_grid(astroI, 2,3)\n\n\n\n\n\n\nUnderstanding Portuguese prompts\n\nptBR_imgs = pipe([\"uma foto de um astronauta montando um cavalo\"]*6, num_inference_steps=30).images\n\n\n\n\n\nimage_grid(ptBR_imgs, 2,3)\n\n\n\n\n\nprompts = [\"uma foto de um astronauta andando a cavalo\"]*6\nptBR_imgs = pipe(prompts, num_inference_steps=30).images\nimage_grid(ptBR_imgs, 2,3)"
  },
  {
    "objectID": "02-sd-concepts-Copy2.html#textual-inversions",
    "href": "02-sd-concepts-Copy2.html#textual-inversions",
    "title": "ffc",
    "section": "Textual Inversions",
    "text": "Textual Inversions\nBased on Huggingface example\n\nSetup images for training\n\npath = Path(\"/home/fredguth/datasets/concepts\")\n\n\nos.listdir(path)\n\n['Volpi',\n 'Calma',\n 'Athos',\n 'ErikBrunn',\n 'GilvanSamico',\n 'BurleMarx',\n 'SemFreio',\n 'ColetivoMuda']\n\n\n\nfnames = get_image_files(path); fnames\n\n(#761) [Path('/home/fredguth/datasets/concepts/Volpi/723250021424807632_[Fachada com Nossa Senhora Aparecida] déc_ de 50 _ Alfredo Volpi têmpera sobre tela, c_i_d_ 65 x 50 cm Coleção particular Reprodução fotografica Horst Merkel.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813007_Veja obras do pintor Alfredo Volpi, morto há 20 anos_Alfredo Volpi.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807638_[Arcos e Bandeirinha] déc_ de 60 _ Alfredo Volpi têmpera sobre cartão 33 x 24 cm Reprodução fotografica autoria desconhecida.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807588_Obras de Arte de Alfredo Volpi - Catálogo das Artes.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807498_Flags(1958) - Alfredo Volpi_.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813114_www_iarremate_com_br Leilão Bolsa de Arte dia 08 de maio as 21h! Lote 0060 Volpi - Têmpera sobre cartão - 33 x 23 cm #volpi #bolsadearte #arte #design #decoração #iarremate #leilão #auction #leilaodearte #leilaoonline #leil....jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807575_Veja obras do pintor Alfredo Volpi, morto há 20 anos.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807485_Alfredo Volpi - Biografia e Principais Obras_EU & VC FAZENDO ARTE_ Alfredo Volpi - Biografia e Principais Obras.jpg'),Path(\"/home/fredguth/datasets/concepts/Volpi/723250021424807627_Alfredo Volpi - Obras, biografia e vida_Alfredo Volpi, 'Fachada', têmpera sobre tela, dec_ 70.jpg\"),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807670_Leilão em 25_05_2015.jpg')...]\n\n\n\ndef label_func(x): return x.parent.name;\n\n\nlabel_func(fnames[132])\n\n'Athos'\n\n\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func, item_tfms=Resize(512), device=\"cuda\")\n\n\ndls.show_batch(max_n=64)\n\n\n\n\n\n# from datasets import load_dataset\n\n\n# dataset = load_dataset(\"imagefolder\", data_dir=\"/home/fredguth/datasets/concepts\")\n\n\n# dsd = load_dataset(\"imagefolder\", data_dir=\"/home/fredguth/datasets/miniconcepts\")\n\n\nimport torch\n\n\ntorch.__version__\n\n'1.13.0+cu117'\n\n\n\nimport transformers\ntransformers.__version__\n\n'4.24.0'\n\n\n\nimport fastai\n\n\nfastai.__version__\n\n'2.7.10'\n\n\n\nimport huggingface_hub\nhuggingface_hub.__version__\n\n'0.10.1'\n\n\n\nimport diffusers\n\n\ndiffusers.__version__\n\n'0.6.0'\n\n\n\nimport os\nfrom fastdownload import FastDownload\nfrom fastai.vision.all import *\n\n/home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\nd = FastDownload()\nd.get(\"https://github.com/fredguth/artistas_brasileiros/blob/main/brasileiros.tgz?raw=true\")\npath = d.data_path()/'brasileiros'\nos.listdir(path)\n\n\n\n\n\n\n    \n      \n      100.00% [98025472/98022364 00:25<00:00]\n    \n    \n\n\n['ColetivoMuda',\n 'SemFreio',\n 'BurleMarx',\n 'GilvanSamico',\n 'Athos',\n 'Calma',\n 'Volpi']\n\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\nimgrids=[]\nfor artist in os.listdir(path): \n    artist_path = get_image_files(path/artist)\n    artist_imgs = [PILImage.create(i).resize((224,224)) for i in artist_path[:6*12]]\n    rows = -(len(artist_imgs)//-12) # ceil division\n    imgrids.append(image_grid(artist_imgs, rows, 12))\n\n\nimgrids[0]\n\n\n\n\n\nimgrids[1]\n\n\n\n\n\nimgrids[2]\n\n\n\n\n\nimgrids[3]\n\n\n\n\n\nimgrids[4]\n\n\n\n\n\nimgrids[5]\n\n\n\n\n\nimgrids[6]\n\n\n\n\n\n\nSetup prompt templates for training\n\nplaceholder_token=\"<fredguth/alfredo-volpi>\"\ninitializer_token=\"artist\"\n\n#@title Setup the prompt templates for training \nimagenet_style_templates_small = [\n    \"an image in the style of {}\",\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a mural in the style of {}\",\n    \"an art piece in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n    \"a wall in the style of {}\",\n]\n\nimport PIL\nfrom torch.utils.data import Dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPModel, CLIPTextModel,CLIPVisionModel, CLIPProcessor, CLIPTokenizer\nfrom transformers import logging\nfrom torchvision import transforms\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"style\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path)\n                            for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL.Image.LINEAR,\n            \"bilinear\": PIL.Image.BILINEAR,\n            \"bicubic\": PIL.Image.BICUBIC,\n            \"lanczos\": PIL.Image.LANCZOS,\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            h, w, = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example\n\n\npretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    subfolder=\"tokenizer\",\n)\n\n\n# Add the placeholder token in tokenizer\nnum_added_tokens = tokenizer.add_tokens(placeholder_token);\n\ntoken_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n\n# Check if initializer_token is a single token or a sequence of tokens\nif len(token_ids) > 1:\n    raise ValueError(\"The initializer token must be a single token.\")\n\ninitializer_token_id = token_ids[0]\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\nplaceholder_token_id\n\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n)\nvae = AutoencoderKL.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"vae\"\n)\nunet = UNet2DConditionModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"unet\"\n)\n\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\n\ntoken_embeds = text_encoder.get_input_embeddings().weight.data\ntoken_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n\nEmbedding(49409, 768)\n\n\nIn Textual-Inversion we only train the newly added embedding vector, so lets freeze rest of the model parameters here\n\ndef freeze_params(params):\n    for param in params:\n        param.requires_grad = False\n\n# Freeze vae and unet\nfreeze_params(vae.parameters())\nfreeze_params(unet.parameters())\n# Freeze all parameters except for the token embeddings in text encoder\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)\n\nCreating our training data\n\ntrain_dataset = TextualInversionDataset(\n      data_root=path/'Volpi',\n      tokenizer=tokenizer,\n      size=512,\n      placeholder_token=placeholder_token,\n      repeats=100,\n      learnable_property=\"style\", #Option selected above between object and style\n      center_crop=False,\n      set=\"train\",\n)\n\n/tmp/ipykernel_7460/2145756544.py:34: DeprecationWarning: LINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"linear\": PIL.Image.LINEAR,\n/tmp/ipykernel_7460/2145756544.py:35: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"bilinear\": PIL.Image.BILINEAR,\n/tmp/ipykernel_7460/2145756544.py:36: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  \"bicubic\": PIL.Image.BICUBIC,\n/tmp/ipykernel_7460/2145756544.py:37: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n  \"lanczos\": PIL.Image.LANCZOS,\n\n\n\ndef create_dataloader(train_batch_size=1):\n    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n\n\nSchedulerMixin??\n\nObject `SchedulerMixin` not found.\n\n\n\nnoise_scheduler = DDPMScheduler(\n    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\", \n    num_train_timesteps=1000, \n#     tensor_format=\"pt\"\n)\n\n\nhyperparameters = {\n    \"learning_rate\": 5e-04,\n    \"scale_lr\": True,\n    \"max_train_steps\": 3000,\n    \"train_batch_size\": 1,\n    \"gradient_accumulation_steps\": 4,\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output\"\n}\n\n\ndef training_function(text_encoder, vae, unet):\n    logger = get_logger(__name__)\n\n    train_batch_size = hyperparameters[\"train_batch_size\"]\n    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n    learning_rate = hyperparameters[\"learning_rate\"]\n    max_train_steps = hyperparameters[\"max_train_steps\"]\n    output_dir = hyperparameters[\"output_dir\"]\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n\n    train_dataloader = create_dataloader(train_batch_size)\n\n    if hyperparameters[\"scale_lr\"]:\n        learning_rate = (\n            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    optimizer = torch.optim.AdamW(\n        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n        lr=learning_rate,\n    )\n\n\n    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n        text_encoder, optimizer, train_dataloader\n    )\n\n    # Move vae and unet to device\n    vae.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    # Keep vae and unet in eval model as we don't train these\n    vae.eval()\n    unet.eval()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(text_encoder):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n                accelerator.backward(loss)\n\n                # Zero out the gradients for all token embeddings except the newly added\n                # embeddings for the concept, as we only want to optimize the concept embeddings\n                if accelerator.num_processes > 1:\n                    grads = text_encoder.module.get_input_embeddings().weight.grad\n                else:\n                    grads = text_encoder.get_input_embeddings().weight.grad\n                # Get the index for tokens that we want to zero the grads for\n                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n            logs = {\"loss\": loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = StableDiffusionPipeline(\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n            ),\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        )\n        pipeline.save_pretrained(output_dir)\n        # Also save the newly trained embeddings\n        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n        learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n        torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom tqdm.auto import tqdm\n\n\ntraining_function(text_encoder, vae, unet)\n\n\n\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 8.95 GiB already allocated; 476.38 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\npath\n\nPath('/home/fredguth/datasets/concepts')"
  },
  {
    "objectID": "03-CLIP-embeddings.html",
    "href": "03-CLIP-embeddings.html",
    "title": "ffc",
    "section": "",
    "text": "#!jt -l\n\n\n#!jt -t oceans16\n\n\nimport torch\nfrom torch import autocast\nfrom transformers import CLIPModel, CLIPVisionModel, CLIPProcessor\nfrom transformers import logging\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom torchvision import transforms as tfms\nimport requests\n\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"; torch_device\n\n/home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n'cuda'\n\n\n\nfrom fastdownload import FastDownload\nimport transformers\ntransformers.__version__\n\n'4.24.0'"
  },
  {
    "objectID": "03-CLIP-embeddings.html#loading-the-models",
    "href": "03-CLIP-embeddings.html#loading-the-models",
    "title": "ffc",
    "section": "Loading the models",
    "text": "Loading the models\n\n# Load the autoencoder model which will be used to decode the latents into image space. \nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n# The CLIP Model for generating the embeddings\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n# The UNet model for generating the latents.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n# The noise scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\n\n# To the GPU we go!\nvae = vae.to(torch_device)\nmodel = model.to(torch_device)\nunet = unet.to(torch_device)\n\n\nUnderstanding CLIP embeddings\n\nmodel.vision_model.config.projection_dim, model.text_model.config.projection_dim\n\n(768, 768)\n\n\n\nmodel.vision_model.embeddings, model.text_model.embeddings\n\n(CLIPVisionEmbeddings(\n   (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n   (position_embedding): Embedding(257, 1024)\n ),\n CLIPTextEmbeddings(\n   (token_embedding): Embedding(49408, 768)\n   (position_embedding): Embedding(77, 768)\n ))\n\n\n\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\ninit_image = Image.open(p).convert(\"RGB\")\ninit_image\n\n\n\n\n\nprompts = [\"Wolf howling at the moon, photorealistic 4K\",\"\"]\n\n\nprocessor??\n\n\ninputs = processor(text=prompts, images=init_image, return_tensors=\"pt\", padding=True)\n padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\n\nwith torch.no_grad():\n    vision_outputs = model.vision_model(pixel_values=inputs.pixel_values.to(torch_device))\n    text_outputs = model.text_model(input_ids=inputs.input_ids.to(torch_device))\n    img_embeds = model.visual_projection(vision_outputs[1])\n    txt_embeds = model.text_projection(text_outputs[1])\n    img_embeds = img_embeds / img_embeds.norm(p=2, dim=-1, keepdim=True)\n    txt_embeds = txt_embeds / txt_embeds.norm(p=2, dim=-1, keepdim=True)\nimg_embeds.shape, txt_embeds.shape\n\n(torch.Size([1, 768]), torch.Size([2, 768]))\n\n\n\n# cosine similarity as logits\nlogit_scale = model.logit_scale.exp()\nlogits_per_text = torch.matmul(txt_embeds, img_embeds.t()) * logit_scale\nlogits_per_image = logits_per_text.t()\n\n\nlogits_per_image\n\ntensor([[19.7719, 19.0408]], device='cuda:0', grad_fn=<TBackward0>)\n\n\n\nprobs = logits_per_image.softmax(dim=1);probs\n\ntensor([[0.6750, 0.3250]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n\n\nEmbeddings are the last layers of CLIP model, to become comparable they need to be “projected” to the same (1,768) output.\n\ninit_image.size\n\n(400, 400)\n\n\n\nimg = init_image.resize((400,400))\n\n\nimg_tensor = tfms.ToTensor()(img)\nnul_img = torch.zeros_like(img_tensor);nul_img.shape\n\ntorch.Size([3, 400, 400])"
  },
  {
    "objectID": "03-CLIP-embeddings.html#latent-to-image-to-latent-convertions",
    "href": "03-CLIP-embeddings.html#latent-to-image-to-latent-convertions",
    "title": "ffc",
    "section": "Latent to Image to Latent convertions",
    "text": "Latent to Image to Latent convertions\n\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # bath of latents -> list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\n# from diffusers import StableDiffusionPipeline\n# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
  },
  {
    "objectID": "03-CLIP-embeddings.html#diffusion-loop",
    "href": "03-CLIP-embeddings.html#diffusion-loop",
    "title": "ffc",
    "section": "Diffusion Loop",
    "text": "Diffusion Loop\n\n# Some settings\nprompts = [\"A watercolor painting of an otter\", \"\"]\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 50            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\n# generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\nimg = init_image # two cats\nnul = nul_img #null image\nimages = [img_tensor, nul_img]\n\n\ninputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True)\n\n\nwith torch.no_grad():\n    text_embeddings = model.text_model(input_ids=inputs.input_ids.to(torch_device))[0].half()\n    image_embeddings = model.vision_model(pixel_values=inputs.pixel_values.to(torch_device))[0].half()\ntext_embeddings.shape, image_embeddings.shape, text_embeddings.type()\n\n(torch.Size([2, 8, 768]), torch.Size([2, 257, 1024]), 'torch.cuda.HalfTensor')\n\n\n\nmax_length = inputs.input_ids.shape[-1]; max_length\n\n8\n\n\n\nuncond_input = processor.tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = model.text_model(uncond_input.input_ids.to(torch_device))[0].half()\n\n\nuncond_embeddings, uncond_embeddings.shape\n\n(tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4900, -0.3066,  0.0674],\n          [-0.3711, -1.4492, -0.3401,  ...,  0.9487,  0.1868, -1.1035],\n          [-0.5107, -1.4629, -0.2925,  ...,  1.0420,  0.0701, -1.0283],\n          ...,\n          [-0.5645, -1.3877, -0.2568,  ...,  1.1553, -0.1278, -1.0488],\n          [-0.5586, -1.3857, -0.2512,  ...,  1.1953, -0.1794, -1.0723],\n          [-0.5596, -1.3682, -0.2421,  ...,  1.2295, -0.2568, -1.1035]]],\n        device='cuda:0', dtype=torch.float16),\n torch.Size([1, 8, 768]))\n\n\n\ntext_embeddings[1]\n\ntensor([[-0.3884,  0.0229, -0.0522,  ..., -0.4900, -0.3066,  0.0674],\n        [-0.3711, -1.4492, -0.3401,  ...,  0.9487,  0.1868, -1.1035],\n        [-0.5107, -1.4629, -0.2925,  ...,  1.0420,  0.0701, -1.0283],\n        ...,\n        [-0.5645, -1.3877, -0.2568,  ...,  1.1553, -0.1278, -1.0488],\n        [-0.5586, -1.3857, -0.2512,  ...,  1.1953, -0.1794, -1.0723],\n        [-0.5596, -1.3682, -0.2421,  ...,  1.2295, -0.2568, -1.1035]],\n       device='cuda:0', dtype=torch.float16)\n\n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n\nlatents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\nlatents = latents.to(\"cuda\").half()\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nscheduler.set_timesteps(num_inference_steps)\nlatents = latents * scheduler.init_noise_sigma\n\n\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nlatent_model_input = torch.cat([latents] * 2);latent_model_input.shape\n\ntorch.Size([2, 4, 64, 64])\n\n\n\n# Loop\nwith autocast(\"cuda\"):\n    for i, t in tqdm(enumerate(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i].half()\n        # Scale the latents (preconditioning):\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n    # scale and decode the image latents with vae\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n\n    # Display\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n\n\n\n\n\npil_images[0]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The FFC lib",
    "section": "",
    "text": "FFC (Fred’s fastai cookbook) is not your usual library. It does not intend to fill a gap in the development ecosystem, its purpose is to fill a gap in my own brain.\nInspired by Zach Muller’s Walk with fastai, FFC aims to collect notebooks, snippets, ideas and learnings of my personal fastai journey."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "The FFC lib",
    "section": "Install",
    "text": "Install\npip install -e '.[dev]'"
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "The FFC lib",
    "section": "Project structure",
    "text": "Project structure\nThis project is built on nbdev. The nbs folder is where the content is produced:\n\nnbs/(root): nbdev and quarto config files\n\ncustom.css: my tweaks on the default cosmo website theme\n\nnbs/blog: blog content \n\nNbdev automatically creates the proc, doc and ffc folders."
  },
  {
    "objectID": "02-cultural_bias.html",
    "href": "02-cultural_bias.html",
    "title": "ffc",
    "section": "",
    "text": "from PIL import Image\nfrom fastcore.all import concat\nimport torch, logging\nfrom pathlib import Path\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nlogging.disable(logging.WARNING)\nimport os\nimport pathlib\nfrom fastai.vision.all import *\n\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\n/home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n  warn(f\"Failed to load image Python extension: {e}\")\nThe weights are cached in your home directory by default.\nDisable safety checker"
  },
  {
    "objectID": "02-cultural_bias.html#cultural-bias",
    "href": "02-cultural_bias.html#cultural-bias",
    "title": "ffc",
    "section": "Cultural Bias",
    "text": "Cultural Bias\n\nLabrador example\n\nbaseprompt = \"Labrador in the style of \"\n\n\nimgsB = pipe([baseprompt+a for a in Brazilian], num_inference_steps=30).images\nimgsI = pipe([baseprompt+a for a in International], num_inference_steps=30).images\n\n\n\n\n\n\n\n\nimage_grid(imgsB, 2,3)\n\n\n\n\n\nimage_grid(imgsI, 2,3)\n\n\n\n\n\n\nAstronaut example\n\nastroprompt = \"a painting of an astronaut riding a horse in the style of \"\n\n\nastroB = pipe([astroprompt+a for a in Brazilian], num_inference_steps=30).images\nastroI = pipe([astroprompt+a for a in International], num_inference_steps=30).images\n\n\n\n\n\n\n\n\nimage_grid(astroB, 2,3)\n\n\n\n\n\nimage_grid(astroI, 2,3)\n\n\n\n\n\n\nUnderstanding Portuguese prompts\n\ntokenizer=pipe.tokenizer\ntext_input = tokenizer(\"astronauta\", padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\ntext_input.input_ids\n\ntensor([[49406,  7982,   627,  1397, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\n\nwith torch.no_grad():\n    text_embeddings = pipe.text_encoder(text_input.input_ids.to(torch_device))[0]\n\n\ntext_embeddings, text_embeddings.shape\n\n(tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4902, -0.3066,  0.0674],\n          [-0.0055,  1.4160,  0.5757,  ..., -1.1904,  0.9229,  0.7910],\n          [ 0.3730,  0.9302,  1.1465,  ..., -2.0938, -1.6436,  0.6299],\n          ...,\n          [ 0.3132,  0.4246, -0.3845,  ...,  0.2549, -1.1631, -0.4790],\n          [ 0.3108,  0.4094, -0.3538,  ...,  0.2795, -1.1846, -0.4805],\n          [ 0.2216,  0.4631, -0.3718,  ...,  0.2646, -1.1357, -0.5366]]],\n        device='cuda:0', dtype=torch.float16),\n torch.Size([1, 77, 768]))\n\n\n\ntokenizer.decode([7982,   627 ,   1397])\n\n'astronauta'\n\n\n\ntext_input = tokenizer(\"astronaut\", padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\ntext_input.input_ids\n\ntensor([[49406, 18376, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\n\ntokenizer.decode(18376)\n\n'astronaut'\n\n\n\nvocab = tokenizer.get_vocab()\n\n\nptBR_imgs = pipe([\"uma foto de um astronauta montando um cavalo\"]*6, num_inference_steps=30).images\n\n\n\n\n\nimage_grid(ptBR_imgs, 2,3)\n\n\n\n\n\nprompts = [\"uma foto de um astronauta andando a cavalo\"]*6\nptBR_imgs = pipe(prompts, num_inference_steps=30).images\nimage_grid(ptBR_imgs, 2,3)"
  },
  {
    "objectID": "02-cultural_bias.html#textual-inversions",
    "href": "02-cultural_bias.html#textual-inversions",
    "title": "ffc",
    "section": "Textual Inversions",
    "text": "Textual Inversions\nBased on Huggingface example\n\nSetup images for training\n\npath = Path(\"/home/fredguth/datasets/concepts\")\n\n\nos.listdir(path)\n\n['Volpi',\n 'Calma',\n 'Athos',\n 'ErikBrunn',\n 'GilvanSamico',\n 'BurleMarx',\n 'SemFreio',\n 'ColetivoMuda']\n\n\n\nfnames = get_image_files(path); fnames\n\n(#761) [Path('/home/fredguth/datasets/concepts/Volpi/723250021424807632_[Fachada com Nossa Senhora Aparecida] déc_ de 50 _ Alfredo Volpi têmpera sobre tela, c_i_d_ 65 x 50 cm Coleção particular Reprodução fotografica Horst Merkel.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813007_Veja obras do pintor Alfredo Volpi, morto há 20 anos_Alfredo Volpi.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807638_[Arcos e Bandeirinha] déc_ de 60 _ Alfredo Volpi têmpera sobre cartão 33 x 24 cm Reprodução fotografica autoria desconhecida.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807588_Obras de Arte de Alfredo Volpi - Catálogo das Artes.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807498_Flags(1958) - Alfredo Volpi_.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813114_www_iarremate_com_br Leilão Bolsa de Arte dia 08 de maio as 21h! Lote 0060 Volpi - Têmpera sobre cartão - 33 x 23 cm #volpi #bolsadearte #arte #design #decoração #iarremate #leilão #auction #leilaodearte #leilaoonline #leil....jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807575_Veja obras do pintor Alfredo Volpi, morto há 20 anos.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807485_Alfredo Volpi - Biografia e Principais Obras_EU & VC FAZENDO ARTE_ Alfredo Volpi - Biografia e Principais Obras.jpg'),Path(\"/home/fredguth/datasets/concepts/Volpi/723250021424807627_Alfredo Volpi - Obras, biografia e vida_Alfredo Volpi, 'Fachada', têmpera sobre tela, dec_ 70.jpg\"),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807670_Leilão em 25_05_2015.jpg')...]\n\n\n\ndef label_func(x): return x.parent.name;\n\n\nlabel_func(fnames[132])\n\n'Athos'\n\n\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func, item_tfms=Resize(512), device=\"cuda\")\n\n\ndls.show_batch(max_n=64)\n\n\n\n\n\nimgrids=[]\nfor artist in os.listdir(path): \n    artist_path = get_image_files(path/artist)\n    artist_imgs = [PILImage.create(i).resize((224,224)) for i in artist_path[:6*12]]\n    rows = -(len(artist_imgs)//-12) # ceil division\n    imgrids.append(image_grid(artist_imgs, rows, 12))\n\n\nimgrids[0]\n\n\n\n\n\nimgrids[1]\n\n\n\n\n\nimgrids[2]\n\n\n\n\n\nimgrids[3]\n\n\n\n\n\nimgrids[4]\n\n\n\n\n\nimgrids[5]\n\n\n\n\n\nimgrids[6]\n\n\n\n\n\nimgrids[7]\n\n\n\n\n\n\nSetup prompt templates for training\n\nplaceholder_token=\"<fredguth/alfredo-volpi>\"\ninitializer_token=\"artist\"\n\n\n#@title Setup the prompt templates for training \nimagenet_style_templates_small = [\n    \"an image in the style of {}\",\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a mural in the style of {}\",\n    \"an art piece in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n    \"a wall in the style of {}\",\n]\n\n\n\nHuggingface\n\nimport PIL\nfrom torch.utils.data import Dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPModel, CLIPTextModel,CLIPVisionModel, CLIPProcessor, CLIPTokenizer\nfrom transformers import logging\nfrom torchvision import transforms\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"style\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path)\n                            for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL.Image.LINEAR,\n            \"bilinear\": PIL.Image.BILINEAR,\n            \"bicubic\": PIL.Image.BICUBIC,\n            \"lanczos\": PIL.Image.LANCZOS,\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            h, w, = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example\n\n\n# model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n# tokenizer = processor.tokenizer\n# text_encoder = model.text_model.to(torch_device)\n\nLoad the tokenizer and add the placeholder token as a additional special token.\n\npretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    subfolder=\"tokenizer\",\n)\n\n\nplaceholder_token\n\n'<fredguth/alfredo-volpi>'\n\n\n\n# Add the placeholder token in tokenizer\nnum_added_tokens = tokenizer.add_tokens(placeholder_token);\nnum_added_tokens\n\n1\n\n\n\nif num_added_tokens == 0:\n    raise ValueError(\n        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n        \" `placeholder_token` that is not already in the tokenizer.\"\n    )\n\nGet token ids for our placeholder and initializer token. This code block will complain if initializer string is not a single token\n\ninitializer_token\n\n'artist'\n\n\n\ntoken_ids = tokenizer.encode(initializer_token, add_special_tokens=False);token_ids\n\n[2456]\n\n\n\n# Check if initializer_token is a single token or a sequence of tokens\nif len(token_ids) > 1:\n    raise ValueError(\"The initializer token must be a single token.\")\n\ninitializer_token_id = token_ids[0]\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\nplaceholder_token_id\n\n49408\n\n\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n)\nvae = AutoencoderKL.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"vae\"\n)\nunet = UNet2DConditionModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"unet\"\n)\n\nWe have added the placeholder_token in the tokenizer so we resize the token embeddings here, this will a new embedding vector in the token embeddings for our placeholder_token\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\n\nEmbedding(49409, 768)\n\n\nInitialise the newly added placeholder token with the embeddings of the initializer token\n\ntoken_embeds = text_encoder.get_input_embeddings().weight.data\ntoken_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n\nIn Textual-Inversion we only train the newly added embedding vector, so lets freeze rest of the model parameters here\n\ndef freeze_params(params):\n    for param in params:\n        param.requires_grad = False\n\n# Freeze vae and unet\nfreeze_params(vae.parameters())\nfreeze_params(unet.parameters())\n# Freeze all parameters except for the token embeddings in text encoder\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)\n\nCreating our training data\n\ntrain_dataset = TextualInversionDataset(\n      data_root=path/'Volpi',\n      tokenizer=tokenizer,\n      size=512,\n      placeholder_token=placeholder_token,\n      repeats=100,\n      learnable_property=\"style\", #Option selected above between object and style\n      center_crop=False,\n      set=\"train\",\n)\n\n/tmp/ipykernel_7460/2145756544.py:34: DeprecationWarning: LINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"linear\": PIL.Image.LINEAR,\n/tmp/ipykernel_7460/2145756544.py:35: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"bilinear\": PIL.Image.BILINEAR,\n/tmp/ipykernel_7460/2145756544.py:36: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  \"bicubic\": PIL.Image.BICUBIC,\n/tmp/ipykernel_7460/2145756544.py:37: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n  \"lanczos\": PIL.Image.LANCZOS,\n\n\n\ndef create_dataloader(train_batch_size=1):\n    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n\n\nSchedulerMixin??\n\nObject `SchedulerMixin` not found.\n\n\n\nnoise_scheduler = DDPMScheduler(\n    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\", \n    num_train_timesteps=1000, \n#     tensor_format=\"pt\"\n)\n\n\nhyperparameters = {\n    \"learning_rate\": 5e-04,\n    \"scale_lr\": True,\n    \"max_train_steps\": 3000,\n    \"train_batch_size\": 1,\n    \"gradient_accumulation_steps\": 4,\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output\"\n}\n\n\ndef training_function(text_encoder, vae, unet):\n    logger = get_logger(__name__)\n\n    train_batch_size = hyperparameters[\"train_batch_size\"]\n    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n    learning_rate = hyperparameters[\"learning_rate\"]\n    max_train_steps = hyperparameters[\"max_train_steps\"]\n    output_dir = hyperparameters[\"output_dir\"]\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n\n    train_dataloader = create_dataloader(train_batch_size)\n\n    if hyperparameters[\"scale_lr\"]:\n        learning_rate = (\n            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    optimizer = torch.optim.AdamW(\n        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n        lr=learning_rate,\n    )\n\n\n    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n        text_encoder, optimizer, train_dataloader\n    )\n\n    # Move vae and unet to device\n    vae.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    # Keep vae and unet in eval model as we don't train these\n    vae.eval()\n    unet.eval()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(text_encoder):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n                accelerator.backward(loss)\n\n                # Zero out the gradients for all token embeddings except the newly added\n                # embeddings for the concept, as we only want to optimize the concept embeddings\n                if accelerator.num_processes > 1:\n                    grads = text_encoder.module.get_input_embeddings().weight.grad\n                else:\n                    grads = text_encoder.get_input_embeddings().weight.grad\n                # Get the index for tokens that we want to zero the grads for\n                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n            logs = {\"loss\": loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = StableDiffusionPipeline(\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n            ),\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        )\n        pipeline.save_pretrained(output_dir)\n        # Also save the newly trained embeddings\n        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n        learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n        torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom tqdm.auto import tqdm\n\n\ntraining_function(text_encoder, vae, unet)\n\n\n\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 8.95 GiB already allocated; 476.38 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\npath\n\nPath('/home/fredguth/datasets/concepts')"
  },
  {
    "objectID": "textual-inversion.html",
    "href": "textual-inversion.html",
    "title": "ffc",
    "section": "",
    "text": "from PIL import Image\nfrom fastcore.all import concat\nimport torch, logging\nfrom pathlib import Path\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nlogging.disable(logging.WARNING)\nimport os\nimport pathlib\nfrom fastai.vision.all import *\n\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\n/home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n  warn(f\"Failed to load image Python extension: {e}\")\nThe weights are cached in your home directory by default.\nDisable safety checker"
  },
  {
    "objectID": "textual-inversion.html#cultural-bias",
    "href": "textual-inversion.html#cultural-bias",
    "title": "ffc",
    "section": "Cultural Bias",
    "text": "Cultural Bias\n\nLabrador example\n\nbaseprompt = \"Labrador in the style of \"\n\n\nimgsB = pipe([baseprompt+a for a in Brazilian], num_inference_steps=30).images\nimgsI = pipe([baseprompt+a for a in International], num_inference_steps=30).images\n\n\n\n\n\n\n\n\nimage_grid(imgsB, 2,3)\n\n\n\n\n\nimage_grid(imgsI, 2,3)\n\n\n\n\n\n\nAstronaut example\n\nastroprompt = \"a painting of an astronaut riding a horse in the style of \"\n\n\nastroB = pipe([astroprompt+a for a in Brazilian], num_inference_steps=30).images\nastroI = pipe([astroprompt+a for a in International], num_inference_steps=30).images\n\n\n\n\n\n\n\n\nimage_grid(astroB, 2,3)\n\n\n\n\n\nimage_grid(astroI, 2,3)\n\n\n\n\n\n\nUnderstanding Portuguese prompts\n\nptBR_imgs = pipe([\"uma foto de um astronauta montando um cavalo\"]*6, num_inference_steps=30).images\n\n\n\n\n\nimage_grid(ptBR_imgs, 2,3)\n\n\n\n\n\nprompts = [\"uma foto de um astronauta andando a cavalo\"]*6\nptBR_imgs = pipe(prompts, num_inference_steps=30).images\nimage_grid(ptBR_imgs, 2,3)"
  },
  {
    "objectID": "textual-inversion.html#textual-inversions",
    "href": "textual-inversion.html#textual-inversions",
    "title": "ffc",
    "section": "Textual Inversions",
    "text": "Textual Inversions\nBased on Huggingface example\n\nSetup images for training\n\npath = Path(\"/home/fredguth/datasets/concepts\")\n\n\nos.listdir(path)\n\n['Volpi',\n 'Calma',\n 'Athos',\n 'ErikBrunn',\n 'GilvanSamico',\n 'BurleMarx',\n 'SemFreio',\n 'ColetivoMuda']\n\n\n\nfnames = get_image_files(path); fnames\n\n(#761) [Path('/home/fredguth/datasets/concepts/Volpi/723250021424807632_[Fachada com Nossa Senhora Aparecida] déc_ de 50 _ Alfredo Volpi têmpera sobre tela, c_i_d_ 65 x 50 cm Coleção particular Reprodução fotografica Horst Merkel.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813007_Veja obras do pintor Alfredo Volpi, morto há 20 anos_Alfredo Volpi.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807638_[Arcos e Bandeirinha] déc_ de 60 _ Alfredo Volpi têmpera sobre cartão 33 x 24 cm Reprodução fotografica autoria desconhecida.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807588_Obras de Arte de Alfredo Volpi - Catálogo das Artes.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807498_Flags(1958) - Alfredo Volpi_.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813114_www_iarremate_com_br Leilão Bolsa de Arte dia 08 de maio as 21h! Lote 0060 Volpi - Têmpera sobre cartão - 33 x 23 cm #volpi #bolsadearte #arte #design #decoração #iarremate #leilão #auction #leilaodearte #leilaoonline #leil....jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807575_Veja obras do pintor Alfredo Volpi, morto há 20 anos.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807485_Alfredo Volpi - Biografia e Principais Obras_EU & VC FAZENDO ARTE_ Alfredo Volpi - Biografia e Principais Obras.jpg'),Path(\"/home/fredguth/datasets/concepts/Volpi/723250021424807627_Alfredo Volpi - Obras, biografia e vida_Alfredo Volpi, 'Fachada', têmpera sobre tela, dec_ 70.jpg\"),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807670_Leilão em 25_05_2015.jpg')...]\n\n\n\ndef label_func(x): return x.parent.name;\n\n\nlabel_func(fnames[132])\n\n'Athos'\n\n\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func, item_tfms=Resize(512), device=\"cuda\")\n\n\ndls.show_batch(max_n=64)\n\n\n\n\n\nimgrids=[]\nfor artist in os.listdir(path): \n    artist_path = get_image_files(path/artist)\n    artist_imgs = [PILImage.create(i).resize((224,224)) for i in artist_path[:6*12]]\n    rows = -(len(artist_imgs)//-12) # ceil division\n    imgrids.append(image_grid(artist_imgs, rows, 12))\n\n\nimgrids[0]\n\n\n\n\n\nimgrids[1]\n\n\n\n\n\nimgrids[2]\n\n\n\n\n\nimgrids[3]\n\n\n\n\n\nimgrids[4]\n\n\n\n\n\nimgrids[5]\n\n\n\n\n\nimgrids[6]\n\n\n\n\n\nimgrids[7]\n\n\n\n\n\n\nSetup prompt templates for training\n\nplaceholder_token=\"<fredguth/alfredo-volpi>\"\ninitializer_token=\"artist\"\n\n\n#@title Setup the prompt templates for training \nimagenet_style_templates_small = [\n    \"an image in the style of {}\",\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a mural in the style of {}\",\n    \"an art piece in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n    \"a wall in the style of {}\",\n]\n\n\n\nHuggingface\n\nimport PIL\nfrom torch.utils.data import Dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPModel, CLIPTextModel,CLIPVisionModel, CLIPProcessor, CLIPTokenizer\nfrom transformers import logging\nfrom torchvision import transforms\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"style\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path)\n                            for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL.Image.LINEAR,\n            \"bilinear\": PIL.Image.BILINEAR,\n            \"bicubic\": PIL.Image.BICUBIC,\n            \"lanczos\": PIL.Image.LANCZOS,\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            h, w, = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example\n\n\n# model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n# tokenizer = processor.tokenizer\n# text_encoder = model.text_model.to(torch_device)\n\nLoad the tokenizer and add the placeholder token as a additional special token.\n\npretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    subfolder=\"tokenizer\",\n)\n\n\nplaceholder_token\n\n'<fredguth/alfredo-volpi>'\n\n\n\n# Add the placeholder token in tokenizer\nnum_added_tokens = tokenizer.add_tokens(placeholder_token);\nnum_added_tokens\n\n1\n\n\n\nif num_added_tokens == 0:\n    raise ValueError(\n        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n        \" `placeholder_token` that is not already in the tokenizer.\"\n    )\n\nGet token ids for our placeholder and initializer token. This code block will complain if initializer string is not a single token\n\ninitializer_token\n\n'artist'\n\n\n\ntoken_ids = tokenizer.encode(initializer_token, add_special_tokens=False);token_ids\n\n[2456]\n\n\n\n# Check if initializer_token is a single token or a sequence of tokens\nif len(token_ids) > 1:\n    raise ValueError(\"The initializer token must be a single token.\")\n\ninitializer_token_id = token_ids[0]\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\nplaceholder_token_id\n\n49408\n\n\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n)\nvae = AutoencoderKL.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"vae\"\n)\nunet = UNet2DConditionModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"unet\"\n)\n\nWe have added the placeholder_token in the tokenizer so we resize the token embeddings here, this will a new embedding vector in the token embeddings for our placeholder_token\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\n\nEmbedding(49409, 768)\n\n\nInitialise the newly added placeholder token with the embeddings of the initializer token\n\ntoken_embeds = text_encoder.get_input_embeddings().weight.data\ntoken_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n\nIn Textual-Inversion we only train the newly added embedding vector, so lets freeze rest of the model parameters here\n\ndef freeze_params(params):\n    for param in params:\n        param.requires_grad = False\n\n# Freeze vae and unet\nfreeze_params(vae.parameters())\nfreeze_params(unet.parameters())\n# Freeze all parameters except for the token embeddings in text encoder\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)\n\nCreating our training data\n\ntrain_dataset = TextualInversionDataset(\n      data_root=path/'Volpi',\n      tokenizer=tokenizer,\n      size=512,\n      placeholder_token=placeholder_token,\n      repeats=100,\n      learnable_property=\"style\", #Option selected above between object and style\n      center_crop=False,\n      set=\"train\",\n)\n\n/tmp/ipykernel_7460/2145756544.py:34: DeprecationWarning: LINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"linear\": PIL.Image.LINEAR,\n/tmp/ipykernel_7460/2145756544.py:35: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"bilinear\": PIL.Image.BILINEAR,\n/tmp/ipykernel_7460/2145756544.py:36: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  \"bicubic\": PIL.Image.BICUBIC,\n/tmp/ipykernel_7460/2145756544.py:37: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n  \"lanczos\": PIL.Image.LANCZOS,\n\n\n\ndef create_dataloader(train_batch_size=1):\n    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n\n\nSchedulerMixin??\n\nObject `SchedulerMixin` not found.\n\n\n\nnoise_scheduler = DDPMScheduler(\n    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\", \n    num_train_timesteps=1000, \n#     tensor_format=\"pt\"\n)\n\n\nhyperparameters = {\n    \"learning_rate\": 5e-04,\n    \"scale_lr\": True,\n    \"max_train_steps\": 3000,\n    \"train_batch_size\": 1,\n    \"gradient_accumulation_steps\": 4,\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output\"\n}\n\n\ndef training_function(text_encoder, vae, unet):\n    logger = get_logger(__name__)\n\n    train_batch_size = hyperparameters[\"train_batch_size\"]\n    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n    learning_rate = hyperparameters[\"learning_rate\"]\n    max_train_steps = hyperparameters[\"max_train_steps\"]\n    output_dir = hyperparameters[\"output_dir\"]\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n\n    train_dataloader = create_dataloader(train_batch_size)\n\n    if hyperparameters[\"scale_lr\"]:\n        learning_rate = (\n            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    optimizer = torch.optim.AdamW(\n        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n        lr=learning_rate,\n    )\n\n\n    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n        text_encoder, optimizer, train_dataloader\n    )\n\n    # Move vae and unet to device\n    vae.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    # Keep vae and unet in eval model as we don't train these\n    vae.eval()\n    unet.eval()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(text_encoder):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n                accelerator.backward(loss)\n\n                # Zero out the gradients for all token embeddings except the newly added\n                # embeddings for the concept, as we only want to optimize the concept embeddings\n                if accelerator.num_processes > 1:\n                    grads = text_encoder.module.get_input_embeddings().weight.grad\n                else:\n                    grads = text_encoder.get_input_embeddings().weight.grad\n                # Get the index for tokens that we want to zero the grads for\n                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n            logs = {\"loss\": loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = StableDiffusionPipeline(\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n            ),\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        )\n        pipeline.save_pretrained(output_dir)\n        # Also save the newly trained embeddings\n        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n        learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n        torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom tqdm.auto import tqdm\n\n\ntraining_function(text_encoder, vae, unet)\n\n\n\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 8.95 GiB already allocated; 476.38 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\npath\n\nPath('/home/fredguth/datasets/concepts')"
  },
  {
    "objectID": "06-textual.html",
    "href": "06-textual.html",
    "title": "ffc",
    "section": "",
    "text": "Based on Huggingface example\n\n\n\n\npath = Path(\"/home/fredguth/datasets/concepts\")\n\n\nos.listdir(path)\n\n['Volpi',\n 'Calma',\n 'Athos',\n 'ErikBrunn',\n 'GilvanSamico',\n 'BurleMarx',\n 'SemFreio',\n 'ColetivoMuda']\n\n\n\nfnames = get_image_files(path); fnames\n\n(#761) [Path('/home/fredguth/datasets/concepts/Volpi/723250021424807632_[Fachada com Nossa Senhora Aparecida] déc_ de 50 _ Alfredo Volpi têmpera sobre tela, c_i_d_ 65 x 50 cm Coleção particular Reprodução fotografica Horst Merkel.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813007_Veja obras do pintor Alfredo Volpi, morto há 20 anos_Alfredo Volpi.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807638_[Arcos e Bandeirinha] déc_ de 60 _ Alfredo Volpi têmpera sobre cartão 33 x 24 cm Reprodução fotografica autoria desconhecida.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807588_Obras de Arte de Alfredo Volpi - Catálogo das Artes.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807498_Flags(1958) - Alfredo Volpi_.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424813114_www_iarremate_com_br Leilão Bolsa de Arte dia 08 de maio as 21h! Lote 0060 Volpi - Têmpera sobre cartão - 33 x 23 cm #volpi #bolsadearte #arte #design #decoração #iarremate #leilão #auction #leilaodearte #leilaoonline #leil....jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807575_Veja obras do pintor Alfredo Volpi, morto há 20 anos.jpg'),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807485_Alfredo Volpi - Biografia e Principais Obras_EU & VC FAZENDO ARTE_ Alfredo Volpi - Biografia e Principais Obras.jpg'),Path(\"/home/fredguth/datasets/concepts/Volpi/723250021424807627_Alfredo Volpi - Obras, biografia e vida_Alfredo Volpi, 'Fachada', têmpera sobre tela, dec_ 70.jpg\"),Path('/home/fredguth/datasets/concepts/Volpi/723250021424807670_Leilão em 25_05_2015.jpg')...]\n\n\n\ndef label_func(x): return x.parent.name;\n\n\nlabel_func(fnames[132])\n\n'Athos'\n\n\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func, item_tfms=Resize(512), device=\"cuda\")\n\n\ndls.show_batch(max_n=64)\n\n\n\n\n\nimgrids=[]\nfor artist in os.listdir(path): \n    artist_path = get_image_files(path/artist)\n    artist_imgs = [PILImage.create(i).resize((224,224)) for i in artist_path[:6*12]]\n    rows = -(len(artist_imgs)//-12) # ceil division\n    imgrids.append(image_grid(artist_imgs, rows, 12))\n\n\nimgrids[0]\n\n\n\n\n\nimgrids[1]\n\n\n\n\n\nimgrids[2]\n\n\n\n\n\nimgrids[3]\n\n\n\n\n\nimgrids[4]\n\n\n\n\n\nimgrids[5]\n\n\n\n\n\nimgrids[6]\n\n\n\n\n\nimgrids[7]\n\n\n\n\n\nimport argparse, itertools, math, os, random, PIL\nimport numpy as np, torch, torch.nn.functional as F, torch.utils.checkpoint\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.hub_utils import init_git_repo, push_to_hub\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\nfrom PIL import Image\nfrom PIL.Image import Resampling\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\n\nfrom transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\nimport fastcore.all as fc\nfrom huggingface_hub import notebook_login\nfrom pathlib import Path\n\nimport torchvision.transforms.functional as tf\nimport accelerate\n\ntorch.manual_seed(1)\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\nmodel_nm = \"CompVis/stable-diffusion-v1-4\"\n\n/home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/fredguth/.miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\n# path = Path.home()/'Downloads/photos/'\n\n\npath\n\nPath('/home/fredguth/datasets/concepts')\n\n\n\npaths = list((path/'Volpi').iterdir())\n\n\npaths[:2]\n\n[Path('/home/fredguth/datasets/concepts/Volpi/723250021424807632_[Fachada com Nossa Senhora Aparecida] déc_ de 50 _ Alfredo Volpi têmpera sobre tela, c_i_d_ 65 x 50 cm Coleção particular Reprodução fotografica Horst Merkel.jpg'),\n Path('/home/fredguth/datasets/concepts/Volpi/723250021424813007_Veja obras do pintor Alfredo Volpi, morto há 20 anos_Alfredo Volpi.jpg')]\n\n\n\nimages = [Image.open(p).resize((512, 512), resample=Resampling.BICUBIC).convert(\"RGB\") for p in paths]\n\n\nwhat_to_teach = \"style\"\nplaceholder_token = \"<fredguth/alfredo-volpi>\"\ninitializer_token = \"artist\"\n\nstyle_templates = [\n    \"an image in the style of {}\",\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a mural in the style of {}\",\n    \"an art piece in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n    \"a wall in the style of {}\",\n]\n\n\nclass TextualInversionDataset:\n    def __init__(self, tokenizer, images, learnable_property=\"object\", size=512,\n                 repeats=100, interpolation=Resampling.BICUBIC, flip_p=0.5, set=\"train\", placeholder_token=\"*\"):\n        fc.store_attr()\n        self.num_images = len(images)\n        if set == \"train\": self._length = self.num_images * repeats\n        self.templates = style_templates if learnable_property == \"style\" else templates\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self): return self.num_images\n\n    def __getitem__(self, i):\n        image = tf.to_tensor(self.images[i%self.num_images])*2-1\n        text = random.choice(self.templates).format(self.placeholder_token)\n        ids=self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.tokenizer.model_max_length, return_tensors=\"pt\")\n        return dict(input_ids=ids.input_ids[0], pixel_values=image)\n\n\ntokenizer = CLIPTokenizer.from_pretrained(model_nm, subfolder=\"tokenizer\")\nnum_added_tokens = tokenizer.add_tokens(placeholder_token)\ntoken_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\ninitializer_token_id = token_ids[0]\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n\ntext_encoder = CLIPTextModel.from_pretrained(model_nm, subfolder=\"text_encoder\")\nvae = AutoencoderKL.from_pretrained(model_nm, subfolder=\"vae\")\nunet = UNet2DConditionModel.from_pretrained(model_nm, subfolder=\"unet\")\ntext_encoder.resize_token_embeddings(len(tokenizer))\ntoken_embeds = text_encoder.get_input_embeddings().weight.data\ntoken_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n\n\n# Freeze all parameters except for the token embeddings in text encoder\ntm = text_encoder.text_model\nfor o in (vae, unet, tm.encoder, tm.final_layer_norm, tm.embeddings.position_embedding):\n    for p in o.parameters(): p.requires_grad = False\n\ntrain_dataset = TextualInversionDataset(\n    images=images, tokenizer=tokenizer, size=512, placeholder_token=placeholder_token,\n    repeats=100, learnable_property=what_to_teach, set=\"train\")\n\ndef create_dataloader(bs=1): return DataLoader(train_dataset, batch_size=bs, shuffle=True)\n\n\nnoise_scheduler = DDPMScheduler(\n    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\", \n    num_train_timesteps=1000, \n#     tensor_format=\"pt\"\n)\n\n\ndef training_function(text_encoder, vae, unet, train_batch_size, gradient_accumulation_steps,\n                      lr, max_train_steps, scale_lr, output_dir):\n    accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, mixed_precision='fp16')\n    train_dataloader = create_dataloader(train_batch_size)\n    if scale_lr: lr = (lr * gradient_accumulation_steps * train_batch_size * accelerator.num_processes)\n    optimizer = torch.optim.AdamW(text_encoder.get_input_embeddings().parameters(), lr=lr)\n    text_encoder, optimizer, train_dataloader = accelerator.prepare(text_encoder, optimizer, train_dataloader)\n    vae.to(accelerator.device).eval()\n    unet.to(accelerator.device).eval()\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(text_encoder):\n                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach() * 0.18215\n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n                accelerator.backward(loss)\n\n                # We only want to optimize the concept embeddings\n                grads = text_encoder.get_input_embeddings().weight.grad\n                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n                optimizer.step()\n                optimizer.zero_grad()\n\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n            progress_bar.set_postfix(loss=loss.detach().item())\n            if global_step >= max_train_steps: break\n\n    pipeline = StableDiffusionPipeline(\n        text_encoder=accelerator.unwrap_model(text_encoder),\n        vae=vae, unet=unet, tokenizer=tokenizer,\n        scheduler=PNDMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True),\n        safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n        feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"))\n    pipeline.enable_attention_slicing()\n    pipeline.save_pretrained(output_dir)\n    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n    learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n    torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n\n\ntorch.manual_seed(42)\ntraining_function(text_encoder, vae, unet, train_batch_size=1, gradient_accumulation_steps=4, lr=5e-04,\n      max_train_steps=3000, scale_lr=True, output_dir=\"sd-concept-output\")\n\n\n\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 8.93 GiB already allocated; 443.56 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\ntorch.cuda.memory_summary()\n\n'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    4240 MB |    9541 MB |  194591 MB |  190351 MB |\\n|       from large pool |    4195 MB |    9435 MB |  192232 MB |  188037 MB |\\n|       from small pool |      45 MB |     132 MB |    2359 MB |    2313 MB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    4240 MB |    9541 MB |  194591 MB |  190351 MB |\\n|       from large pool |    4195 MB |    9435 MB |  192232 MB |  188037 MB |\\n|       from small pool |      45 MB |     132 MB |    2359 MB |    2313 MB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    9652 MB |    9652 MB |    9654 MB |    2048 KB |\\n|       from large pool |    9516 MB |    9516 MB |    9516 MB |       0 KB |\\n|       from small pool |     136 MB |     136 MB |     138 MB |    2048 KB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  427732 KB |    2323 MB |  131346 MB |  130928 MB |\\n|       from large pool |  424877 KB |    2274 MB |  128725 MB |  128310 MB |\\n|       from small pool |    2855 KB |      50 MB |    2620 MB |    2618 MB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1133    |    2238    |   20357    |   19224    |\\n|       from large pool |     345    |     803    |    8474    |    8129    |\\n|       from small pool |     788    |    1435    |   11883    |   11095    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1133    |    2238    |   20357    |   19224    |\\n|       from large pool |     345    |     803    |    8474    |    8129    |\\n|       from small pool |     788    |    1435    |   11883    |   11095    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     208    |     208    |     209    |       1    |\\n|       from large pool |     140    |     140    |     140    |       0    |\\n|       from small pool |      68    |      68    |      69    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      49    |     145    |    8667    |    8618    |\\n|       from large pool |      35    |      55    |    4425    |    4390    |\\n|       from small pool |      14    |      97    |    4242    |    4228    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'\n\n\n\n\n\n\nplaceholder_token=\"<fredguth/alfredo-volpi>\"\ninitializer_token=\"artist\"\n\n\n#@title Setup the prompt templates for training \nimagenet_style_templates_small = [\n    \"an image in the style of {}\",\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a mural in the style of {}\",\n    \"an art piece in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n    \"a wall in the style of {}\",\n]\n\n\n\n\n\nimport PIL\nfrom torch.utils.data import Dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPModel, CLIPTextModel,CLIPVisionModel, CLIPProcessor, CLIPTokenizer\nfrom transformers import logging\nfrom torchvision import transforms\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"style\",  # [object, style]\n        size=512,\n        repeats=100,\n        interpolation=\"bicubic\",\n        flip_p=0.5,\n        set=\"train\",\n        placeholder_token=\"*\",\n        center_crop=False,\n    ):\n\n        self.data_root = data_root\n        self.tokenizer = tokenizer\n        self.learnable_property = learnable_property\n        self.size = size\n        self.placeholder_token = placeholder_token\n        self.center_crop = center_crop\n        self.flip_p = flip_p\n\n        self.image_paths = [os.path.join(self.data_root, file_path)\n                            for file_path in os.listdir(self.data_root)]\n\n        self.num_images = len(self.image_paths)\n        self._length = self.num_images\n\n        if set == \"train\":\n            self._length = self.num_images * repeats\n\n        self.interpolation = {\n            \"linear\": PIL.Image.LINEAR,\n            \"bilinear\": PIL.Image.BILINEAR,\n            \"bicubic\": PIL.Image.BICUBIC,\n            \"lanczos\": PIL.Image.LANCZOS,\n        }[interpolation]\n\n        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, i):\n        example = {}\n        image = Image.open(self.image_paths[i % self.num_images])\n\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n\n        placeholder_string = self.placeholder_token\n        text = random.choice(self.templates).format(placeholder_string)\n\n        example[\"input_ids\"] = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids[0]\n\n        # default to score-sde preprocessing\n        img = np.array(image).astype(np.uint8)\n\n        if self.center_crop:\n            crop = min(img.shape[0], img.shape[1])\n            h, w, = (\n                img.shape[0],\n                img.shape[1],\n            )\n            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n\n        image = Image.fromarray(img)\n        image = image.resize((self.size, self.size), resample=self.interpolation)\n\n        image = self.flip_transform(image)\n        image = np.array(image).astype(np.uint8)\n        image = (image / 127.5 - 1.0).astype(np.float32)\n\n        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n        return example\n\n\n# model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n# tokenizer = processor.tokenizer\n# text_encoder = model.text_model.to(torch_device)\n\nLoad the tokenizer and add the placeholder token as a additional special token.\n\npretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    subfolder=\"tokenizer\",\n)\n\n\nplaceholder_token\n\n'<fredguth/alfredo-volpi>'\n\n\n\n# Add the placeholder token in tokenizer\nnum_added_tokens = tokenizer.add_tokens(placeholder_token);\nnum_added_tokens\n\n1\n\n\n\nif num_added_tokens == 0:\n    raise ValueError(\n        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n        \" `placeholder_token` that is not already in the tokenizer.\"\n    )\n\nGet token ids for our placeholder and initializer token. This code block will complain if initializer string is not a single token\n\ninitializer_token\n\n'artist'\n\n\n\ntoken_ids = tokenizer.encode(initializer_token, add_special_tokens=False);token_ids\n\n[2456]\n\n\n\n# Check if initializer_token is a single token or a sequence of tokens\nif len(token_ids) > 1:\n    raise ValueError(\"The initializer token must be a single token.\")\n\ninitializer_token_id = token_ids[0]\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\nplaceholder_token_id\n\n49408\n\n\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n)\nvae = AutoencoderKL.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"vae\"\n)\nunet = UNet2DConditionModel.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"unet\"\n)\n\nWe have added the placeholder_token in the tokenizer so we resize the token embeddings here, this will a new embedding vector in the token embeddings for our placeholder_token\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\n\nEmbedding(49409, 768)\n\n\nInitialise the newly added placeholder token with the embeddings of the initializer token\n\ntoken_embeds = text_encoder.get_input_embeddings().weight.data\ntoken_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n\nIn Textual-Inversion we only train the newly added embedding vector, so lets freeze rest of the model parameters here\n\ndef freeze_params(params):\n    for param in params:\n        param.requires_grad = False\n\n# Freeze vae and unet\nfreeze_params(vae.parameters())\nfreeze_params(unet.parameters())\n# Freeze all parameters except for the token embeddings in text encoder\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)\n\nCreating our training data\n\ntrain_dataset = TextualInversionDataset(\n      data_root=path/'Volpi',\n      tokenizer=tokenizer,\n      size=512,\n      placeholder_token=placeholder_token,\n      repeats=100,\n      learnable_property=\"style\", #Option selected above between object and style\n      center_crop=False,\n      set=\"train\",\n)\n\n/tmp/ipykernel_7460/2145756544.py:34: DeprecationWarning: LINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"linear\": PIL.Image.LINEAR,\n/tmp/ipykernel_7460/2145756544.py:35: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n  \"bilinear\": PIL.Image.BILINEAR,\n/tmp/ipykernel_7460/2145756544.py:36: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n  \"bicubic\": PIL.Image.BICUBIC,\n/tmp/ipykernel_7460/2145756544.py:37: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n  \"lanczos\": PIL.Image.LANCZOS,\n\n\n\ndef create_dataloader(train_batch_size=1):\n    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n\n\nSchedulerMixin??\n\nObject `SchedulerMixin` not found.\n\n\n\nnoise_scheduler = DDPMScheduler(\n    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\", \n    num_train_timesteps=1000, \n#     tensor_format=\"pt\"\n)\n\n\nhyperparameters = {\n    \"learning_rate\": 5e-04,\n    \"scale_lr\": True,\n    \"max_train_steps\": 3000,\n    \"train_batch_size\": 1,\n    \"gradient_accumulation_steps\": 4,\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output\"\n}\n\n\ndef training_function(text_encoder, vae, unet):\n    logger = get_logger(__name__)\n\n    train_batch_size = hyperparameters[\"train_batch_size\"]\n    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n    learning_rate = hyperparameters[\"learning_rate\"]\n    max_train_steps = hyperparameters[\"max_train_steps\"]\n    output_dir = hyperparameters[\"output_dir\"]\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n\n    train_dataloader = create_dataloader(train_batch_size)\n\n    if hyperparameters[\"scale_lr\"]:\n        learning_rate = (\n            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n        )\n\n    # Initialize the optimizer\n    optimizer = torch.optim.AdamW(\n        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n        lr=learning_rate,\n    )\n\n\n    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n        text_encoder, optimizer, train_dataloader\n    )\n\n    # Move vae and unet to device\n    vae.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    # Keep vae and unet in eval model as we don't train these\n    vae.eval()\n    unet.eval()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(num_train_epochs):\n        text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(text_encoder):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n                accelerator.backward(loss)\n\n                # Zero out the gradients for all token embeddings except the newly added\n                # embeddings for the concept, as we only want to optimize the concept embeddings\n                if accelerator.num_processes > 1:\n                    grads = text_encoder.module.get_input_embeddings().weight.grad\n                else:\n                    grads = text_encoder.get_input_embeddings().weight.grad\n                # Get the index for tokens that we want to zero the grads for\n                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n            logs = {\"loss\": loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = StableDiffusionPipeline(\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=PNDMScheduler(\n                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n            ),\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n        )\n        pipeline.save_pretrained(output_dir)\n        # Also save the newly trained embeddings\n        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n        learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n        torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom tqdm.auto import tqdm\n\n\ntraining_function(text_encoder, vae, unet)\n\n\n\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 8.95 GiB already allocated; 476.38 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\npath\n\nPath('/home/fredguth/datasets/concepts')"
  },
  {
    "objectID": "07-diffedit.html",
    "href": "07-diffedit.html",
    "title": "DiffEdit: Diffusion-Based Semantic Image Editing with Mask Guidance",
    "section": "",
    "text": "Let us properly define the problem and introduce the notation. Given: * \\(x_0\\), an image, \\(e.g.\\)  * \\(R\\), a Text Reference (aka caption), \\(e.g.\\) “A bowl of fruits”, and * \\(Q\\), a Textual Transformation Query, \\(e.g.\\) \\(\\text{fruits} \\to \\text{pears}\\)\nGenerate: * \\(y_0\\), a minimally modified version of \\(x_0\\) in accordance to the transformation query \\(q\\)"
  },
  {
    "objectID": "07-diffedit.html#method-overview",
    "href": "07-diffedit.html#method-overview",
    "title": "DiffEdit: Diffusion-Based Semantic Image Editing with Mask Guidance",
    "section": "Method overview",
    "text": "Method overview\nThe method consist of 3 main steps: 1. \\(M_{\\eta}(x_0,R,Q)\\to M\\), compute mask (with noise ratio \\(\\eta\\)); 2. \\(E_{r}(x_0,Q = \\emptyset) \\to x_r\\), encode input image with edit strength \\(r\\) and no-conditioning (\\(Q=\\emptyset\\)); 3. \\(D_{r,M}(x_t, Q) \\to y_0\\), decode with edit strength \\(r\\), mask guidance and prompt conditioning.\n\nUtility functions\n\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # bath of latents -> list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\ndef generate(prompt=\"\", width=512, height=512, steps=30, guidance=7.5, seed=42):\n    prompts = [\"\", prompt]\n    inputs = processor(text=prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        t_e = text_encoder(input_ids=inputs.input_ids.to(torch_device))[0].half()\n    generator = torch.manual_seed(seed)\n    latents = generate_seed_latent(generator, width, height)\n    latents = torch.randn((1, unet.in_channels, height // 8, width // 8), generator=generator)\n    latents = latents.to(torch_device)\n    latents = latents * scheduler.init_noise_sigma\n    scheduler.set_timesteps(steps)\n    frames = []\n    # denoising loop\n    with autocast(cast_device):\n        for i, t in tqdm(enumerate(scheduler.timesteps)):\n            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n            latent_model_input = torch.cat([latents] * 2)\n            sigma = scheduler.sigmas[i]\n            # Scale the latents (preconditioning):\n            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n            # predict the noise residual\n            with torch.no_grad():\n                ts = t.type(torch.float32) if torch.has_mps else t\n                noise_pred = unet(latent_model_input, ts, encoder_hidden_states=t_e).sample\n            # perform guidance\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            noise_pred = noise_pred_uncond + guidance * (noise_pred_text - noise_pred_uncond)\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = scheduler.step(noise_pred, t, latents).prev_sample\n            frames.append(latents)\n    return latents_to_pil(latents), seed, frames\n\n\nimage, seed, frames = generate(\"an astronaut riding a horse\")\n\n\n\n\n\nframes\n\n/Users/fredguth/Downloads/.miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor_str.py:115: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n  nonzero_finite_vals = torch.masked_select(\n\n\n[tensor([[[[-1.5453e+01,  1.7413e+01,  3.0450e+00,  ..., -1.5863e+00,\n            -1.6960e+01, -2.0946e+00],\n           [ 1.0661e+01, -3.4488e+00, -5.8246e+00,  ..., -2.4999e-02,\n             9.8572e+00,  2.4667e+00],\n           [ 1.1475e+01, -7.4601e+00,  6.6181e+00,  ...,  4.6589e+00,\n            -5.9077e+00,  8.0520e+00],\n           ...,\n           [-2.1826e+01, -4.3789e+00,  6.0778e+00,  ...,  1.2621e+01,\n            -1.2625e+01, -8.4941e+00],\n           [ 7.3615e+00,  5.1121e+00, -1.1762e+01,  ...,  1.3410e+00,\n            -9.3642e+00,  3.2393e+00],\n           [-6.9359e+00, -3.9992e+00,  5.9301e+00,  ...,  3.0132e+01,\n             1.6475e+01,  5.6991e+00]],\n \n          [[-3.4725e+00, -4.3543e+00, -1.9941e+01,  ..., -9.1618e+00,\n            -2.1897e+01,  7.1817e+00],\n           [-8.0178e+00, -4.2485e+00, -1.8010e+01,  ...,  8.7064e+00,\n             2.8139e+00, -2.7796e+00],\n           [-7.9254e+00,  5.1075e+00,  1.6894e+00,  ..., -2.4413e+00,\n             1.2449e+01, -1.5403e+01],\n           ...,\n           [-9.9487e-01,  1.3196e+01, -1.3559e+01,  ..., -1.8390e+01,\n             3.9752e+00,  5.1135e+00],\n           [ 1.8274e+01, -2.7136e+00, -9.1893e+00,  ..., -7.6098e+00,\n            -1.8293e+00, -7.7107e+00],\n           [-6.3497e+00,  1.0547e+01,  2.4986e+00,  ..., -1.0024e+01,\n            -2.3785e+01, -1.0966e+01]],\n \n          [[ 2.2342e+01, -4.8415e+00, -3.3017e+00,  ...,  8.7194e+00,\n             6.4381e+00, -5.6379e+00],\n           [-1.3589e+01, -1.0281e+01, -3.7533e-01,  ..., -7.9999e+00,\n            -6.5636e+00, -1.2097e+01],\n           [-1.0061e+01, -8.7708e-01,  1.9163e+01,  ...,  7.6227e-01,\n             1.4672e+01,  1.8488e+01],\n           ...,\n           [ 2.2476e+00, -1.3407e+01,  1.2733e+01,  ..., -1.9247e+01,\n             1.0642e+01,  2.1533e+01],\n           [ 2.7314e+01,  3.4414e+00,  6.8520e+00,  ...,  4.4721e+00,\n             6.2163e+00,  2.1262e+01],\n           [-1.1293e+01, -7.6256e+00,  8.0725e+00,  ...,  1.9572e+01,\n             2.0533e+01,  6.3161e+00]],\n \n          [[ 4.2537e+00,  3.7238e-01,  9.3724e+00,  ..., -5.5085e+00,\n             1.0602e+00,  4.9283e+00],\n           [-3.7456e+00,  1.2996e+01,  1.4493e+01,  ...,  5.3041e+00,\n             2.0087e+01,  2.3654e+01],\n           [-1.8519e+01,  1.5347e+01, -2.8284e+00,  ..., -7.5194e+00,\n            -9.2108e+00, -2.5814e-01],\n           ...,\n           [ 4.4015e+00,  7.7440e-01, -1.5955e+00,  ...,  1.2532e+01,\n             1.4838e+01,  9.4562e+00],\n           [ 8.5771e+00, -2.0229e+01,  6.1842e+00,  ...,  1.6141e+01,\n             3.1263e+00, -1.1435e+01],\n           [ 7.1680e+00,  1.9018e+01, -1.0569e+01,  ...,  1.8612e+01,\n             2.2762e+01, -3.1612e+00]]]], device='mps:0'),\n tensor([[[[-12.6301,  14.4523,   2.6256,  ...,  -1.2306, -13.8885,  -1.6424],\n           [  8.8725,  -2.7327,  -4.6785,  ...,   0.0637,   8.2133,   2.1122],\n           [  9.5254,  -6.0370,   5.5695,  ...,   3.9315,  -4.7778,   6.7347],\n           ...,\n           [-17.8628,  -3.5111,   5.1166,  ...,  10.4396, -10.3792,  -6.9940],\n           [  6.1332,   4.3065,  -9.5868,  ...,   1.1421,  -7.6890,   2.6607],\n           [ -5.6839,  -3.2133,   4.9611,  ...,  24.8270,  13.5481,   4.6088]],\n \n          [[ -2.8243,  -3.5580, -16.3851,  ...,  -7.5391, -18.0564,   5.9454],\n           [ -6.6063,  -3.5039, -14.8501,  ...,   7.1559,   2.2751,  -2.3139],\n           [ -6.5445,   4.2055,   1.3897,  ...,  -2.0331,  10.2109, -12.6861],\n           ...,\n           [ -0.7831,  10.9088, -11.1590,  ..., -15.2622,   3.1675,   4.0832],\n           [ 15.0913,  -2.2056,  -7.5328,  ...,  -6.3793,  -1.6250,  -6.4355],\n           [ -5.1905,   8.7436,   2.0920,  ...,  -8.3540, -19.7072,  -9.1252]],\n \n          [[ 18.4081,  -3.9760,  -2.7130,  ...,   7.1737,   5.2864,  -4.6693],\n           [-11.1455,  -8.4549,  -0.2863,  ...,  -6.5834,  -5.4028,  -9.9615],\n           [ -8.2429,  -0.7059,  15.8050,  ...,   0.6189,  12.0684,  15.1803],\n           ...,\n           [  1.9027, -11.0278,  10.5110,  ..., -15.7934,   8.8143,  17.7690],\n           [ 22.5333,   2.8478,   5.6561,  ...,   3.7413,   5.1421,  17.5218],\n           [ -9.2443,  -6.2581,   6.6718,  ...,  16.2059,  16.9940,   5.2404]],\n \n          [[  3.4721,   0.3345,   7.7241,  ...,  -4.5231,   0.8686,   4.0629],\n           [ -3.0939,  10.7353,  11.9491,  ...,   4.3986,  16.5848,  19.5096],\n           [-15.2476,  12.6682,  -2.3087,  ...,  -6.1634,  -7.5519,  -0.1573],\n           ...,\n           [  3.5980,   0.6310,  -1.3280,  ...,  10.3429,  12.2541,   7.8041],\n           [  7.0651, -16.6569,   5.0803,  ...,  13.3365,   2.6216,  -9.3537],\n           [  5.8102,  15.6443,  -8.7328,  ...,  15.3824,  18.7940,  -2.5886]]]],\n        device='mps:0'),\n tensor([[[[-10.4102,  12.1186,   2.2884,  ...,  -0.9579, -11.4810,  -1.2919],\n           [  7.4635,  -2.1734,  -3.7859,  ...,   0.1235,   6.9118,   1.8321],\n           [  7.9895,  -4.9211,   4.7350,  ...,   3.3499,  -3.8996,   5.6881],\n           ...,\n           [-14.7551,  -2.8306,   4.3563,  ...,   8.7075,  -8.6248,  -5.8287],\n           [  5.1670,   3.6689,  -7.8804,  ...,   0.9723,  -6.3865,   2.1887],\n           [ -4.6863,  -2.5924,   4.2060,  ...,  20.6360,  11.2300,   3.7357]],\n \n          [[ -2.3032,  -2.9194, -13.5842,  ...,  -6.2590, -15.0269,   4.9778],\n           [ -5.4774,  -2.9059, -12.3568,  ...,   5.9419,   1.8648,  -1.9391],\n           [ -5.4404,   3.5054,   1.1614,  ...,  -1.7084,   8.4576, -10.5508],\n           ...,\n           [ -0.6119,   9.1165,  -9.2612,  ..., -12.8066,   2.5266,   3.2651],\n           [ 12.5957,  -1.7943,  -6.2201,  ...,  -5.4166,  -1.4694,  -5.4443],\n           [ -4.2669,   7.3319,   1.7843,  ...,  -7.0521, -16.5058,  -7.6879]],\n \n          [[ 15.3170,  -3.2936,  -2.2442,  ...,   5.9581,   4.3798,  -3.9082],\n           [ -9.2257,  -7.0143,  -0.2102,  ...,  -5.4688,  -4.4879,  -8.2779],\n           [ -6.8172,  -0.5699,  13.1659,  ...,   0.5046,  10.0188,  12.5863],\n           ...,\n           [  1.6217,  -9.1615,   8.7613,  ..., -13.0793,   7.3766,  14.8186],\n           [ 18.7686,   2.3754,   4.7159,  ...,   3.1652,   4.3030,  14.5937],\n           [ -7.6354,  -5.1809,   5.5737,  ...,  13.5570,  14.2137,   4.4136]],\n \n          [[  2.8694,   0.2984,   6.4277,  ...,  -3.7451,   0.7229,   3.3919],\n           [ -2.5789,   8.9499,   9.9480,  ...,   3.6889,  13.8308,  16.2578],\n           [-12.6744,  10.5575,  -1.8992,  ...,  -5.0929,  -6.2497,  -0.0772],\n           ...,\n           [  2.9595,   0.5046,  -1.1288,  ...,   8.6175,  10.2169,   6.5005],\n           [  5.8708, -13.8590,   4.2008,  ...,  11.1266,   2.2202,  -7.7260],\n           [  4.7506,  12.9747,  -7.2990,  ...,  12.8332,  15.6658,  -2.1469]]]],\n        device='mps:0'),\n tensor([[[[-8.6475e+00,  1.0258e+01,  2.0113e+00,  ..., -7.4474e-01,\n            -9.5715e+00, -1.0119e+00],\n           [ 6.3413e+00, -1.7365e+00, -3.0870e+00,  ...,  1.6607e-01,\n             5.8731e+00,  1.6127e+00],\n           [ 6.7658e+00, -4.0388e+00,  4.0644e+00,  ...,  2.8848e+00,\n            -3.2053e+00,  4.8564e+00],\n           ...,\n           [-1.2291e+01, -2.2893e+00,  3.7508e+00,  ...,  7.3229e+00,\n            -7.2337e+00, -4.9107e+00],\n           [ 4.4062e+00,  3.1626e+00, -6.5250e+00,  ...,  8.3374e-01,\n            -5.3562e+00,  1.8074e+00],\n           [-3.8811e+00, -2.0942e+00,  3.6161e+00,  ...,  1.7301e+01,\n             9.3855e+00,  3.0381e+00]],\n \n          [[-1.8901e+00, -2.4094e+00, -1.1361e+01,  ..., -5.2375e+00,\n            -1.2614e+01,  4.2084e+00],\n           [-4.5753e+00, -2.4257e+00, -1.0377e+01,  ...,  4.9849e+00,\n             1.5515e+00, -1.6347e+00],\n           [-4.5581e+00,  2.9521e+00,  9.8144e-01,  ..., -1.4426e+00,\n             7.0740e+00, -8.8499e+00],\n           ...,\n           [-4.7254e-01,  7.7007e+00, -7.7419e+00,  ..., -1.0849e+01,\n             2.0225e+00,  2.6166e+00],\n           [ 1.0621e+01, -1.4567e+00, -5.1663e+00,  ..., -4.6470e+00,\n            -1.3410e+00, -4.6575e+00],\n           [-3.5240e+00,  6.2191e+00,  1.5535e+00,  ..., -6.0155e+00,\n            -1.3959e+01, -6.5460e+00]],\n \n          [[ 1.2857e+01, -2.7547e+00, -1.8708e+00,  ...,  4.9850e+00,\n             3.6521e+00, -3.3105e+00],\n           [-7.7037e+00, -5.8685e+00, -1.4706e-01,  ..., -4.5863e+00,\n            -3.7626e+00, -6.9416e+00],\n           [-5.6894e+00, -4.6313e-01,  1.1069e+01,  ...,  4.0744e-01,\n             8.3851e+00,  1.0527e+01],\n           ...,\n           [ 1.3916e+00, -7.6815e+00,  7.3741e+00,  ..., -1.0926e+01,\n             6.2330e+00,  1.2480e+01],\n           [ 1.5777e+01,  1.9965e+00,  3.9700e+00,  ...,  2.7049e+00,\n             3.6381e+00,  1.2276e+01],\n           [-6.3610e+00, -4.3264e+00,  4.7048e+00,  ...,  1.1449e+01,\n             1.2003e+01,  3.7641e+00]],\n \n          [[ 2.4030e+00,  2.6898e-01,  5.4034e+00,  ..., -3.1229e+00,\n             6.1190e-01,  2.8674e+00],\n           [-2.1650e+00,  7.5323e+00,  8.3639e+00,  ...,  3.1297e+00,\n             1.1646e+01,  1.3682e+01],\n           [-1.0629e+01,  8.8847e+00, -1.5689e+00,  ..., -4.2373e+00,\n            -5.2161e+00, -7.6518e-03],\n           ...,\n           [ 2.4480e+00,  3.9515e-01, -9.7911e-01,  ...,  7.2475e+00,\n             8.5988e+00,  5.4626e+00],\n           [ 4.9196e+00, -1.1646e+01,  3.4947e+00,  ...,  9.3733e+00,\n             1.9023e+00, -6.4382e+00],\n           [ 3.9160e+00,  1.0843e+01, -6.1688e+00,  ...,  1.0804e+01,\n             1.3179e+01, -1.8001e+00]]]], device='mps:0'),\n tensor([[[[ -7.2360,   8.7594,   1.7801,  ...,  -0.5764,  -8.0411,  -0.7827],\n           [  5.4376,  -1.3950,  -2.5363,  ...,   0.1980,   5.0365,   1.4422],\n           [  5.7826,  -3.3354,   3.5213,  ...,   2.5127,  -2.6487,   4.1925],\n           ...,\n           [-10.3215,  -1.8585,   3.2587,  ...,   6.2121,  -6.1150,  -4.1752],\n           [  3.8022,   2.7550,  -5.4427,  ...,   0.7265,  -4.5270,   1.5040],\n           [ -3.2250,  -1.6913,   3.1468,  ...,  14.6347,   7.9160,   2.4900]],\n \n          [[ -1.5677,  -2.0059,  -9.5842,  ...,  -4.4166, -10.6791,   3.5893],\n           [ -3.8581,  -2.0425,  -8.7934,  ...,   4.2207,   1.3049,  -1.3834],\n           [ -3.8574,   2.5061,   0.8339,  ...,  -1.2230,   5.9689,  -7.4785],\n           ...,\n           [ -0.3571,   6.5723,  -6.5127,  ...,  -9.2609,   1.6327,   2.1174],\n           [  9.0468,  -1.1780,  -4.3110,  ...,  -4.0135,  -1.2233,  -4.0085],\n           [ -2.9216,   5.3325,   1.3769,  ...,  -5.1662, -11.9013,  -5.6113]],\n \n          [[ 10.8811,  -2.3273,  -1.5738,  ...,   4.1962,   3.0600,  -2.8368],\n           [ -6.4898,  -4.9512,  -0.0974,  ...,  -3.8821,  -3.1849,  -5.8724],\n           [ -4.7932,  -0.3790,   9.3884,  ...,   0.3234,   7.0705,   8.8776],\n           ...,\n           [  1.2097,  -6.4895,   6.2699,  ...,  -9.1988,   5.3213,  10.6165],\n           [ 13.3859,   1.6964,   3.3788,  ...,   2.3396,   3.1112,  10.4299],\n           [ -5.3411,  -3.6351,   4.0171,  ...,   9.7633,  10.2349,   3.2540]],\n \n          [[  2.0407,   0.2463,   4.5883,  ...,  -2.6235,   0.5216,   2.4496],\n           [ -1.8297,   6.3977,   7.0991,  ...,   2.6829,   9.8919,  11.6209],\n           [ -8.9869,   7.5498,  -1.2997,  ...,  -3.5479,  -4.3910,   0.0492],\n           ...,\n           [  2.0363,   0.3050,  -0.8608,  ...,   6.1502,   7.3008,   4.6276],\n           [  4.1562,  -9.8766,   2.9282,  ...,   7.9694,   1.6446,  -5.4144],\n           [  3.2512,   9.1314,  -5.2654,  ...,   9.1760,  11.1813,  -1.5291]]]],\n        device='mps:0'),\n tensor([[[[ -6.0963,   7.5464,   1.5909,  ...,  -0.4470,  -6.8076,  -0.6012],\n           [  4.7095,  -1.1190,  -2.0913,  ...,   0.2197,   4.3546,   1.3013],\n           [  4.9923,  -2.7657,   3.0817,  ...,   2.2081,  -2.2029,   3.6485],\n           ...,\n           [ -8.7377,  -1.5154,   2.8516,  ...,   5.3226,  -5.2015,  -3.5741],\n           [  3.3178,   2.4224,  -4.5719,  ...,   0.6505,  -3.8487,   1.2658],\n           [ -2.6877,  -1.3629,   2.7671,  ...,  12.4967,   6.7434,   2.0702]],\n \n          [[ -1.3155,  -1.6862,  -8.1514,  ...,  -3.7563,  -9.1197,   3.0874],\n           [ -3.2846,  -1.7328,  -7.5102,  ...,   3.6004,   1.1020,  -1.1727],\n           [ -3.2939,   2.1467,   0.7154,  ...,  -1.0431,   5.0732,  -6.3635],\n           ...,\n           [ -0.2613,   5.6617,  -5.5174,  ...,  -7.9592,   1.3281,   1.7444],\n           [  7.7778,  -0.9505,  -3.6188,  ...,  -3.4840,  -1.1130,  -3.4590],\n           [ -2.4326,   4.6153,   1.2351,  ...,  -4.4621, -10.2246,  -4.8287]],\n \n          [[  9.2820,  -1.9800,  -1.3328,  ...,   3.5618,   2.5848,  -2.4437],\n           [ -5.5155,  -4.2080,  -0.0584,  ...,  -3.3068,  -2.7135,  -4.9997],\n           [ -4.0743,  -0.3076,   8.0312,  ...,   0.2610,   6.0128,   7.5586],\n           ...,\n           [  1.0719,  -5.5150,   5.3881,  ...,  -7.7940,   4.5959,   9.1244],\n           [ 11.4649,   1.4629,   2.9098,  ...,   2.0541,   2.6964,   8.9543],\n           [ -4.5181,  -3.0655,   3.4713,  ...,   8.4085,   8.8161,   2.8582]],\n \n          [[  1.7559,   0.2300,   3.9337,  ...,  -2.2171,   0.4480,   2.1129],\n           [ -1.5540,   5.4800,   6.0781,  ...,   2.3253,   8.4713,   9.9555],\n           [ -7.6569,   6.4735,  -1.0802,  ...,  -2.9856,  -3.7254,   0.0924],\n           ...,\n           [  1.6977,   0.2268,  -0.7681,  ...,   5.2612,   6.2457,   3.9430],\n           [  3.5336,  -8.4534,   2.4664,  ...,   6.8312,   1.4257,  -4.6034],\n           [  2.7106,   7.7440,  -4.5374,  ...,   7.8543,   9.5543,  -1.3196]]]],\n        device='mps:0'),\n tensor([[[[-5.1709,  6.5639,  1.4412,  ..., -0.3472, -5.8086, -0.4605],\n           [ 4.1217, -0.8849, -1.7195,  ...,  0.2347,  3.7972,  1.1790],\n           [ 4.3541, -2.3008,  2.7254,  ...,  1.9504, -1.8491,  3.1919],\n           ...,\n           [-7.4439, -1.2399,  2.5148,  ...,  4.6066, -4.4479, -3.0748],\n           [ 2.9340,  2.1503, -3.8591,  ...,  0.5972, -3.2906,  1.0789],\n           [-2.2353, -1.0829,  2.4642,  ..., 10.7734,  5.8017,  1.7545]],\n \n          [[-1.1104, -1.4263, -6.9848,  ..., -3.2196, -7.8530,  2.6827],\n           [-2.8227, -1.4800, -6.4628,  ...,  3.0934,  0.9326, -0.9972],\n           [-2.8257,  1.8616,  0.6262,  ..., -0.8958,  4.3420, -5.4501],\n           ...,\n           [-0.1752,  4.9224, -4.7017,  ..., -6.8908,  1.0824,  1.4592],\n           [ 6.7521, -0.7637, -3.0533,  ..., -3.0421, -1.0137, -2.9966],\n           [-2.0333,  4.0285,  1.1203,  ..., -3.8836, -8.8550, -4.1765]],\n \n          [[ 7.9807, -1.6900, -1.1319,  ...,  3.0612,  2.2138, -2.1031],\n           [-4.7291, -3.6038, -0.0320,  ..., -2.8286, -2.3194, -4.2751],\n           [-3.4908, -0.2472,  6.9268,  ...,  0.2231,  5.1636,  6.5037],\n           ...,\n           [ 0.9615, -4.7144,  4.6729,  ..., -6.6454,  4.0103,  7.9135],\n           [ 9.8976,  1.2774,  2.5274,  ...,  1.8230,  2.3596,  7.7571],\n           [-3.8636, -2.5972,  3.0235,  ...,  7.3029,  7.6582,  2.5393]],\n \n          [[ 1.5128,  0.2118,  3.3949,  ..., -1.8855,  0.3853,  1.8373],\n           [-1.3276,  4.7307,  5.2439,  ...,  2.0384,  7.3158,  8.6002],\n           [-6.5747,  5.5903, -0.9046,  ..., -2.5260, -3.1838,  0.1204],\n           ...,\n           [ 1.4136,  0.1523, -0.7003,  ...,  4.5342,  5.3798,  3.3790],\n           [ 3.0144, -7.3052,  2.0810,  ...,  5.8985,  1.2364, -3.9539],\n           [ 2.2579,  6.6094, -3.9461,  ...,  6.7746,  8.2177, -1.1489]]]],\n        device='mps:0'),\n tensor([[[[-4.4050,  5.7665,  1.3282,  ..., -0.2635, -4.9947, -0.3476],\n           [ 3.6455, -0.6800, -1.4027,  ...,  0.2502,  3.3398,  1.0744],\n           [ 3.8375, -1.9153,  2.4367,  ...,  1.7354, -1.5658,  2.8089],\n           ...,\n           [-6.3622, -1.0104,  2.2384,  ...,  4.0206, -3.8215, -2.6644],\n           [ 2.6355,  1.9301, -3.2658,  ...,  0.5542, -2.8296,  0.9220],\n           [-1.8455, -0.8367,  2.2224,  ...,  9.3551,  5.0244,  1.4928]],\n \n          [[-0.9460, -1.2118, -6.0283,  ..., -2.7875, -6.8298,  2.3418],\n           [-2.4561, -1.2757, -5.6076,  ...,  2.6759,  0.7910, -0.8628],\n           [-2.4351,  1.6361,  0.5592,  ..., -0.7730,  3.7396, -4.7068],\n           ...,\n           [-0.1003,  4.3148, -4.0319,  ..., -6.0195,  0.8751,  1.2114],\n           [ 5.9113, -0.6128, -2.5934,  ..., -2.6861, -0.9375, -2.6347],\n           [-1.7180,  3.5311,  1.0127,  ..., -3.4259, -7.7469, -3.6678]],\n \n          [[ 6.9087, -1.4495, -0.9673,  ...,  2.6633,  1.9246, -1.8080],\n           [-4.0986, -3.1196, -0.0230,  ..., -2.4409, -1.9978, -3.6804],\n           [-3.0203, -0.2022,  6.0167,  ...,  0.1972,  4.4739,  5.6496],\n           ...,\n           [ 0.8616, -4.0606,  4.0806,  ..., -5.7116,  3.5203,  6.9088],\n           [ 8.5907,  1.1228,  2.2071,  ...,  1.6219,  2.0723,  6.7651],\n           [-3.3418, -2.2118,  2.6489,  ...,  6.3859,  6.6929,  2.2682]],\n \n          [[ 1.3061,  0.1983,  2.9525,  ..., -1.6112,  0.3332,  1.6166],\n           [-1.1313,  4.1258,  4.5653,  ...,  1.8105,  6.3778,  7.4979],\n           [-5.6809,  4.8686, -0.7570,  ..., -2.1481, -2.7375,  0.1406],\n           ...,\n           [ 1.1904,  0.0963, -0.6422,  ...,  3.9565,  4.6859,  2.9313],\n           [ 2.5911, -6.3583,  1.7671,  ...,  5.1491,  1.0981, -3.4034],\n           [ 1.8939,  5.6898, -3.4528,  ...,  5.9019,  7.1322, -0.9876]]]],\n        device='mps:0'),\n tensor([[[[-3.7630,  5.1067,  1.2389,  ..., -0.1894, -4.3167, -0.2545],\n           [ 3.2586, -0.4990, -1.1322,  ...,  0.2680,  2.9636,  0.9840],\n           [ 3.4156, -1.5877,  2.2021,  ...,  1.5603, -1.3297,  2.4883],\n           ...,\n           [-5.4579, -0.8115,  2.0167,  ...,  3.5364, -3.2986, -2.3223],\n           [ 2.3908,  1.7523, -2.7633,  ...,  0.5185, -2.4414,  0.7963],\n           [-1.5145, -0.6252,  2.0261,  ...,  8.1729,  4.3768,  1.2761]],\n \n          [[-0.8162, -1.0379, -5.2401,  ..., -2.4424, -5.9972,  2.0453],\n           [-2.1702, -1.1202, -4.9134,  ...,  2.3205,  0.6653, -0.7634],\n           [-2.1197,  1.4434,  0.4982,  ..., -0.6803,  3.2274, -4.1151],\n           ...,\n           [-0.0567,  3.7956, -3.4941,  ..., -5.3165,  0.6833,  0.9750],\n           [ 5.1903, -0.5071, -2.2361,  ..., -2.4135, -0.8953, -2.3660],\n           [-1.4851,  3.0921,  0.8993,  ..., -3.0703, -6.8495, -3.2765]],\n \n          [[ 6.0210, -1.2470, -0.8278,  ...,  2.3422,  1.6964, -1.5540],\n           [-3.5860, -2.7239, -0.0256,  ..., -2.1306, -1.7420, -3.1968],\n           [-2.6351, -0.1662,  5.2604,  ...,  0.1815,  3.9064,  4.9498],\n           ...,\n           [ 0.7773, -3.5168,  3.5917,  ..., -4.9404,  3.1047,  6.0640],\n           [ 7.4919,  0.9978,  1.9449,  ...,  1.4517,  1.8319,  5.9365],\n           [-2.9075, -1.8892,  2.3396,  ...,  5.6185,  5.8820,  2.0330]],\n \n          [[ 1.1444,  0.1948,  2.5930,  ..., -1.3782,  0.2989,  1.4401],\n           [-0.9545,  3.6424,  4.0149,  ...,  1.6290,  5.6079,  6.5911],\n           [-4.9253,  4.2836, -0.6254,  ..., -1.8300, -2.3630,  0.1623],\n           ...,\n           [ 1.0163,  0.0579, -0.5912,  ...,  3.4967,  4.1303,  2.5810],\n           [ 2.2448, -5.5673,  1.5101,  ...,  4.5447,  1.0056, -2.9220],\n           [ 1.6110,  4.9368, -3.0385,  ...,  5.1944,  6.2507, -0.8295]]]],\n        device='mps:0'),\n tensor([[[[-3.2111e+00,  4.5536e+00,  1.1668e+00,  ..., -8.3436e-02,\n            -3.7056e+00, -1.5352e-01],\n           [ 2.9438e+00, -3.3666e-01, -9.0060e-01,  ...,  3.2253e-01,\n             2.6754e+00,  9.2095e-01],\n           [ 3.0573e+00, -1.3271e+00,  1.9967e+00,  ...,  1.4364e+00,\n            -1.1047e+00,  2.2346e+00],\n           ...,\n           [-4.6765e+00, -6.2413e-01,  1.8504e+00,  ...,  3.1389e+00,\n            -2.8501e+00, -2.0281e+00],\n           [ 2.2058e+00,  1.6188e+00, -2.3278e+00,  ...,  5.0077e-01,\n            -2.1043e+00,  7.0135e-01],\n           [-1.2227e+00, -4.3217e-01,  1.8783e+00,  ...,  7.1859e+00,\n             3.8405e+00,  1.0962e+00]],\n \n          [[-6.9232e-01, -8.7959e-01, -4.5683e+00,  ..., -2.1189e+00,\n            -5.2661e+00,  1.8280e+00],\n           [-1.9570e+00, -1.0121e+00, -4.3523e+00,  ...,  2.0214e+00,\n             5.5416e-01, -6.7912e-01],\n           [-1.8613e+00,  1.2820e+00,  4.5213e-01,  ..., -5.9039e-01,\n             2.8142e+00, -3.6052e+00],\n           ...,\n           [-2.4055e-02,  3.3549e+00, -3.0459e+00,  ..., -4.7373e+00,\n             5.1593e-01,  7.6274e-01],\n           [ 4.5836e+00, -4.2088e-01, -1.9404e+00,  ..., -2.1943e+00,\n            -8.6957e-01, -2.1519e+00],\n           [-1.2818e+00,  2.7230e+00,  8.0799e-01,  ..., -2.7736e+00,\n            -6.1009e+00, -2.9542e+00]],\n \n          [[ 5.2593e+00, -1.0718e+00, -7.1334e-01,  ...,  2.0642e+00,\n             1.5048e+00, -1.3257e+00],\n           [-3.1935e+00, -2.4232e+00, -6.6004e-02,  ..., -1.9073e+00,\n            -1.5638e+00, -2.8131e+00],\n           [-2.3391e+00, -1.5357e-01,  4.6074e+00,  ...,  1.4899e-01,\n             3.4172e+00,  4.3564e+00],\n           ...,\n           [ 6.9471e-01, -3.0693e+00,  3.1725e+00,  ..., -4.3152e+00,\n             2.7381e+00,  5.3430e+00],\n           [ 6.5544e+00,  8.8260e-01,  1.7135e+00,  ...,  1.2874e+00,\n             1.6092e+00,  5.2264e+00],\n           [-2.5574e+00, -1.6225e+00,  2.0745e+00,  ...,  4.9567e+00,\n             5.1830e+00,  1.8183e+00]],\n \n          [[ 9.8098e-01,  1.7276e-01,  2.2700e+00,  ..., -1.1917e+00,\n             2.5791e-01,  1.2692e+00],\n           [-7.9561e-01,  3.2561e+00,  3.5634e+00,  ...,  1.4974e+00,\n             4.9702e+00,  5.8422e+00],\n           [-4.2857e+00,  3.7951e+00, -5.1820e-01,  ..., -1.5624e+00,\n            -2.0484e+00,  1.7224e-01],\n           ...,\n           [ 8.4747e-01,  3.2336e-03, -5.7286e-01,  ...,  3.1091e+00,\n             3.6610e+00,  2.2856e+00],\n           [ 1.9242e+00, -4.9324e+00,  1.2682e+00,  ...,  4.0362e+00,\n             9.2262e-01, -2.5248e+00],\n           [ 1.3413e+00,  4.2777e+00, -2.7161e+00,  ...,  4.5956e+00,\n             5.5122e+00, -6.9847e-01]]]], device='mps:0'),\n tensor([[[[-2.7468e+00,  4.1155e+00,  1.1310e+00,  ...,  3.0909e-03,\n            -3.1997e+00, -6.4461e-02],\n           [ 2.6133e+00, -2.6388e-01, -7.4431e-01,  ...,  2.9649e-01,\n             2.3784e+00,  8.0909e-01],\n           [ 2.7209e+00, -1.1320e+00,  1.7932e+00,  ...,  1.2596e+00,\n            -9.6952e-01,  1.9667e+00],\n           ...,\n           [-4.0465e+00, -4.6095e-01,  1.6995e+00,  ...,  2.8059e+00,\n            -2.4531e+00, -1.7551e+00],\n           [ 2.0142e+00,  1.4824e+00, -1.9863e+00,  ...,  4.8962e-01,\n            -1.8145e+00,  6.2969e-01],\n           [-1.0224e+00, -3.0895e-01,  1.7277e+00,  ...,  6.3140e+00,\n             3.3870e+00,  9.2916e-01]],\n \n          [[-5.8116e-01, -7.2866e-01, -3.9707e+00,  ..., -1.8727e+00,\n            -4.6688e+00,  1.6064e+00],\n           [-1.8088e+00, -9.5303e-01, -3.9101e+00,  ...,  1.6945e+00,\n             4.0815e-01, -6.8929e-01],\n           [-1.7150e+00,  1.0608e+00,  3.4119e-01,  ..., -6.2769e-01,\n             2.3763e+00, -3.3343e+00],\n           ...,\n           [-2.0032e-02,  2.9956e+00, -2.6628e+00,  ..., -4.2187e+00,\n             4.0249e-01,  6.2118e-01],\n           [ 4.0691e+00, -3.3824e-01, -1.6748e+00,  ..., -1.9987e+00,\n            -8.3789e-01, -1.9443e+00],\n           [-1.0736e+00,  2.4496e+00,  7.6464e-01,  ..., -2.4699e+00,\n            -5.4129e+00, -2.6286e+00]],\n \n          [[ 4.5823e+00, -9.3425e-01, -6.3369e-01,  ...,  1.8206e+00,\n             1.3462e+00, -1.1182e+00],\n           [-2.8839e+00, -2.1862e+00, -1.0336e-01,  ..., -1.7303e+00,\n            -1.4128e+00, -2.4988e+00],\n           [-2.1548e+00, -1.9815e-01,  4.0017e+00,  ...,  6.8500e-02,\n             2.9483e+00,  3.8020e+00],\n           ...,\n           [ 6.0241e-01, -2.6980e+00,  2.8044e+00,  ..., -3.7830e+00,\n             2.4408e+00,  4.7567e+00],\n           [ 5.7730e+00,  8.0398e-01,  1.5087e+00,  ...,  1.1827e+00,\n             1.4535e+00,  4.6700e+00],\n           [-2.2453e+00, -1.3886e+00,  1.8485e+00,  ...,  4.4211e+00,\n             4.6220e+00,  1.6622e+00]],\n \n          [[ 7.7864e-01,  9.8872e-02,  1.9383e+00,  ..., -1.0829e+00,\n             1.6770e-01,  1.0622e+00],\n           [-6.6859e-01,  2.9064e+00,  3.1711e+00,  ...,  1.3390e+00,\n             4.4146e+00,  5.1777e+00],\n           [-3.7577e+00,  3.3662e+00, -4.5262e-01,  ..., -1.3437e+00,\n            -1.7639e+00,  1.7450e-01],\n           ...,\n           [ 6.9929e-01, -4.7792e-02, -5.5548e-01,  ...,  2.7534e+00,\n             3.2327e+00,  2.0198e+00],\n           [ 1.6457e+00, -4.3941e+00,  1.0815e+00,  ...,  3.5878e+00,\n             8.2416e-01, -2.2044e+00],\n           [ 1.1148e+00,  3.7107e+00, -2.4322e+00,  ...,  4.0393e+00,\n             4.8576e+00, -6.2542e-01]]]], device='mps:0'),\n tensor([[[[-2.3862e+00,  3.7322e+00,  1.0777e+00,  ...,  1.9456e-02,\n            -2.8120e+00, -2.9874e-02],\n           [ 2.4254e+00, -1.0387e-01, -5.5445e-01,  ...,  3.5853e-01,\n             2.1979e+00,  8.1992e-01],\n           [ 2.4805e+00, -9.3240e-01,  1.6596e+00,  ...,  1.1981e+00,\n            -8.0315e-01,  1.8247e+00],\n           ...,\n           [-3.5235e+00, -3.5286e-01,  1.5748e+00,  ...,  2.5373e+00,\n            -2.1318e+00, -1.5386e+00],\n           [ 1.8603e+00,  1.3886e+00, -1.6724e+00,  ...,  4.9068e-01,\n            -1.5687e+00,  5.6406e-01],\n           [-8.5474e-01, -1.8374e-01,  1.6128e+00,  ...,  5.6522e+00,\n             3.0195e+00,  8.4538e-01]],\n \n          [[-6.0722e-01, -6.9502e-01, -3.5923e+00,  ..., -1.7199e+00,\n            -4.2291e+00,  1.3645e+00],\n           [-1.6315e+00, -8.3896e-01, -3.4992e+00,  ...,  1.5173e+00,\n             3.6155e-01, -6.1088e-01],\n           [-1.5048e+00,  9.8087e-01,  3.2227e-01,  ..., -5.1479e-01,\n             2.1058e+00, -2.9212e+00],\n           ...,\n           [-1.4228e-03,  2.6808e+00, -2.3281e+00,  ..., -3.7868e+00,\n             3.0171e-01,  5.1535e-01],\n           [ 3.6085e+00, -2.8812e-01, -1.4708e+00,  ..., -1.8204e+00,\n            -7.8663e-01, -1.7458e+00],\n           [-9.7032e-01,  2.1557e+00,  6.7033e-01,  ..., -2.2657e+00,\n            -4.8831e+00, -2.3695e+00]],\n \n          [[ 4.0279e+00, -8.8158e-01, -6.0581e-01,  ...,  1.5746e+00,\n             1.1520e+00, -1.0482e+00],\n           [-2.6322e+00, -1.9934e+00, -1.4163e-01,  ..., -1.5632e+00,\n            -1.2796e+00, -2.2255e+00],\n           [-1.9343e+00, -1.7952e-01,  3.5585e+00,  ...,  8.2211e-02,\n             2.6276e+00,  3.4054e+00],\n           ...,\n           [ 5.6461e-01, -2.3421e+00,  2.5510e+00,  ..., -3.2818e+00,\n             2.2332e+00,  4.2872e+00],\n           [ 5.1424e+00,  7.6347e-01,  1.3884e+00,  ...,  1.1134e+00,\n             1.3413e+00,  4.2046e+00],\n           [-1.9461e+00, -1.1614e+00,  1.6910e+00,  ...,  4.0051e+00,\n             4.1885e+00,  1.5651e+00]],\n \n          [[ 7.9315e-01,  2.1204e-01,  1.8335e+00,  ..., -8.5195e-01,\n             2.5095e-01,  1.0676e+00],\n           [-5.6770e-01,  2.6331e+00,  2.8449e+00,  ...,  1.2258e+00,\n             3.9469e+00,  4.6327e+00],\n           [-3.3301e+00,  3.0019e+00, -3.8321e-01,  ..., -1.1855e+00,\n            -1.5975e+00,  1.6100e-01],\n           ...,\n           [ 6.3202e-01, -6.2860e-02, -5.0520e-01,  ...,  2.4811e+00,\n             2.8956e+00,  1.8175e+00],\n           [ 1.4554e+00, -3.9052e+00,  9.3185e-01,  ...,  3.2198e+00,\n             7.5471e-01, -1.9300e+00],\n           [ 9.6602e-01,  3.2905e+00, -2.1688e+00,  ...,  3.6176e+00,\n             4.3309e+00, -5.1542e-01]]]], device='mps:0'),\n tensor([[[[-2.0701e+00,  3.4055e+00,  1.0532e+00,  ...,  6.9241e-02,\n            -2.4574e+00,  2.2198e-02],\n           [ 2.2071e+00,  4.4096e-03, -4.1015e-01,  ...,  3.8507e-01,\n             2.0150e+00,  7.8475e-01],\n           [ 2.2411e+00, -7.5210e-01,  1.5272e+00,  ...,  1.1009e+00,\n            -6.9610e-01,  1.6751e+00],\n           ...,\n           [-3.0138e+00, -2.0292e-01,  1.5029e+00,  ...,  2.2976e+00,\n            -1.8463e+00, -1.3461e+00],\n           [ 1.7248e+00,  1.3074e+00, -1.4159e+00,  ...,  4.5660e-01,\n            -1.3748e+00,  4.7996e-01],\n           [-7.1283e-01, -9.0826e-02,  1.4896e+00,  ...,  5.0111e+00,\n             2.6562e+00,  7.2542e-01]],\n \n          [[-5.8831e-01, -6.4818e-01, -3.2250e+00,  ..., -1.5630e+00,\n            -3.8190e+00,  1.1820e+00],\n           [-1.5258e+00, -7.9427e-01, -3.2021e+00,  ...,  1.2863e+00,\n             2.5362e-01, -6.0311e-01],\n           [-1.3573e+00,  8.7865e-01,  2.8329e-01,  ..., -4.7250e-01,\n             1.8369e+00, -2.6300e+00],\n           ...,\n           [-1.4180e-02,  2.3778e+00, -2.0917e+00,  ..., -3.4227e+00,\n             1.7766e-01,  4.0021e-01],\n           [ 3.2045e+00, -2.4590e-01, -1.2992e+00,  ..., -1.6878e+00,\n            -7.9779e-01, -1.6184e+00],\n           [-8.3420e-01,  1.9575e+00,  6.3767e-01,  ..., -2.0581e+00,\n            -4.4089e+00, -2.1621e+00]],\n \n          [[ 3.5230e+00, -8.1441e-01, -5.7237e-01,  ...,  1.3658e+00,\n             9.9982e-01, -9.4060e-01],\n           [-2.4192e+00, -1.8257e+00, -1.7908e-01,  ..., -1.4361e+00,\n            -1.1720e+00, -1.9918e+00],\n           [-1.7661e+00, -1.7410e-01,  3.1546e+00,  ...,  6.0895e-02,\n             2.3193e+00,  3.0384e+00],\n           ...,\n           [ 5.3112e-01, -2.0233e+00,  2.3188e+00,  ..., -2.8600e+00,\n             2.0418e+00,  3.8723e+00],\n           [ 4.5871e+00,  7.3075e-01,  1.2879e+00,  ...,  1.0380e+00,\n             1.2563e+00,  3.7929e+00],\n           [-1.7176e+00, -9.6839e-01,  1.5635e+00,  ...,  3.6233e+00,\n             3.7872e+00,  1.4675e+00]],\n \n          [[ 7.0743e-01,  2.2721e-01,  1.6651e+00,  ..., -7.3412e-01,\n             2.2785e-01,  9.7164e-01],\n           [-5.0692e-01,  2.3742e+00,  2.5341e+00,  ...,  1.1054e+00,\n             3.5259e+00,  4.1499e+00],\n           [-2.9806e+00,  2.6958e+00, -3.4020e-01,  ..., -1.0328e+00,\n            -1.4313e+00,  1.5952e-01],\n           ...,\n           [ 5.2036e-01, -7.7014e-02, -4.6812e-01,  ...,  2.2536e+00,\n             2.6112e+00,  1.6434e+00],\n           [ 1.2665e+00, -3.4878e+00,  8.1396e-01,  ...,  2.9173e+00,\n             7.2759e-01, -1.6614e+00],\n           [ 8.0298e-01,  2.8911e+00, -1.9583e+00,  ...,  3.2686e+00,\n             3.8672e+00, -4.1448e-01]]]], device='mps:0'),\n tensor([[[[-1.8090e+00,  3.1343e+00,  1.0230e+00,  ...,  8.9947e-02,\n            -2.1603e+00,  8.2997e-02],\n           [ 2.0596e+00,  1.3237e-01, -2.7926e-01,  ...,  4.3432e-01,\n             1.8818e+00,  7.9016e-01],\n           [ 2.0406e+00, -5.9952e-01,  1.3981e+00,  ...,  1.0183e+00,\n            -6.1373e-01,  1.5466e+00],\n           ...,\n           [-2.6263e+00, -1.2240e-01,  1.4115e+00,  ...,  2.0924e+00,\n            -1.6076e+00, -1.1909e+00],\n           [ 1.6022e+00,  1.2207e+00, -1.1927e+00,  ...,  4.4433e-01,\n            -1.2027e+00,  4.1037e-01],\n           [-5.8464e-01,  1.4342e-02,  1.4116e+00,  ...,  4.5088e+00,\n             2.3831e+00,  6.7052e-01]],\n \n          [[-5.6087e-01, -5.8228e-01, -2.8971e+00,  ..., -1.4137e+00,\n            -3.4767e+00,  1.0278e+00],\n           [-1.4432e+00, -7.5059e-01, -2.9361e+00,  ...,  1.0891e+00,\n             1.6173e-01, -6.0762e-01],\n           [-1.2529e+00,  8.0268e-01,  2.5297e-01,  ..., -4.1646e-01,\n             1.5983e+00, -2.3924e+00],\n           ...,\n           [ 2.7943e-03,  2.1341e+00, -1.8391e+00,  ..., -3.1222e+00,\n             8.4659e-02,  3.1028e-01],\n           [ 2.8689e+00, -2.1549e-01, -1.1515e+00,  ..., -1.5824e+00,\n            -7.8992e-01, -1.4893e+00],\n           [-7.5640e-01,  1.7424e+00,  5.5837e-01,  ..., -1.9217e+00,\n            -4.0328e+00, -1.9717e+00]],\n \n          [[ 3.1191e+00, -7.0985e-01, -5.1711e-01,  ...,  1.2319e+00,\n             9.0772e-01, -8.0788e-01],\n           [-2.2830e+00, -1.7185e+00, -2.5617e-01,  ..., -1.3550e+00,\n            -1.1334e+00, -1.8331e+00],\n           [-1.6320e+00, -1.5081e-01,  2.8245e+00,  ...,  6.1392e-02,\n             2.0641e+00,  2.7527e+00],\n           ...,\n           [ 5.0513e-01, -1.7571e+00,  2.1181e+00,  ..., -2.4800e+00,\n             1.8984e+00,  3.5308e+00],\n           [ 4.1192e+00,  6.9377e-01,  1.1840e+00,  ...,  9.9740e-01,\n             1.1936e+00,  3.4622e+00],\n           [-1.4993e+00, -7.8438e-01,  1.4531e+00,  ...,  3.3287e+00,\n             3.4801e+00,  1.4103e+00]],\n \n          [[ 5.8571e-01,  2.1419e-01,  1.4735e+00,  ..., -6.5810e-01,\n             1.7360e-01,  8.9054e-01],\n           [-4.3561e-01,  2.1968e+00,  2.2797e+00,  ...,  1.0464e+00,\n             3.1989e+00,  3.7865e+00],\n           [-2.6881e+00,  2.4400e+00, -3.0791e-01,  ..., -9.0246e-01,\n            -1.3048e+00,  1.7062e-01],\n           ...,\n           [ 4.4632e-01, -9.9043e-02, -4.3880e-01,  ...,  2.0485e+00,\n             2.3581e+00,  1.4835e+00],\n           [ 1.1015e+00, -3.1540e+00,  6.8187e-01,  ...,  2.6304e+00,\n             6.6145e-01, -1.4769e+00],\n           [ 6.5685e-01,  2.5574e+00, -1.7792e+00,  ...,  2.9475e+00,\n             3.4629e+00, -3.3684e-01]]]], device='mps:0'),\n tensor([[[[-1.5939e+00,  2.8926e+00,  9.9562e-01,  ...,  1.0205e-01,\n            -1.9013e+00,  1.3229e-01],\n           [ 1.9214e+00,  2.5028e-01, -1.6781e-01,  ...,  4.9718e-01,\n             1.7830e+00,  8.2042e-01],\n           [ 1.8497e+00, -4.4425e-01,  1.2833e+00,  ...,  9.4607e-01,\n            -5.4630e-01,  1.4630e+00],\n           ...,\n           [-2.2527e+00, -1.4901e-02,  1.3611e+00,  ...,  1.9052e+00,\n            -1.3963e+00, -1.0529e+00],\n           [ 1.5004e+00,  1.1565e+00, -1.0027e+00,  ...,  4.1432e-01,\n            -1.0577e+00,  3.4601e-01],\n           [-4.8037e-01,  8.6775e-02,  1.3402e+00,  ...,  4.0541e+00,\n             2.1399e+00,  5.9957e-01]],\n \n          [[-5.6105e-01, -5.3828e-01, -2.6330e+00,  ..., -1.2907e+00,\n            -3.1862e+00,  8.8881e-01],\n           [-1.3710e+00, -6.9711e-01, -2.7015e+00,  ...,  9.2899e-01,\n             8.6411e-02, -6.0624e-01],\n           [-1.1651e+00,  7.6769e-01,  2.2906e-01,  ..., -3.3727e-01,\n             1.4156e+00, -2.1561e+00],\n           ...,\n           [ 1.2423e-02,  1.9209e+00, -1.6366e+00,  ..., -2.8481e+00,\n            -6.4369e-04,  2.2144e-01],\n           [ 2.5638e+00, -1.7928e-01, -1.0132e+00,  ..., -1.4895e+00,\n            -7.9456e-01, -1.3961e+00],\n           [-6.5420e-01,  1.5748e+00,  5.3102e-01,  ..., -1.7891e+00,\n            -3.6811e+00, -1.8203e+00]],\n \n          [[ 2.7591e+00, -6.4138e-01, -4.8510e-01,  ...,  1.0833e+00,\n             7.8949e-01, -7.2973e-01],\n           [-2.1710e+00, -1.6168e+00, -3.2783e-01,  ..., -1.2791e+00,\n            -1.1077e+00, -1.6952e+00],\n           [-1.4954e+00, -1.0461e-01,  2.5629e+00,  ...,  7.1216e-02,\n             1.8565e+00,  2.5216e+00],\n           ...,\n           [ 4.8298e-01, -1.5126e+00,  1.9488e+00,  ..., -2.1555e+00,\n             1.7599e+00,  3.2134e+00],\n           [ 3.7019e+00,  6.7077e-01,  1.1130e+00,  ...,  9.3789e-01,\n             1.1225e+00,  3.1418e+00],\n           [-1.3128e+00, -6.1622e-01,  1.3778e+00,  ...,  3.0489e+00,\n             3.1894e+00,  1.3343e+00]],\n \n          [[ 4.9952e-01,  2.3288e-01,  1.3421e+00,  ..., -5.7508e-01,\n             1.4196e-01,  8.3189e-01],\n           [-4.0031e-01,  2.0272e+00,  2.0280e+00,  ...,  9.7109e-01,\n             2.8890e+00,  3.4519e+00],\n           [-2.4659e+00,  2.2379e+00, -2.8167e-01,  ..., -7.8692e-01,\n            -1.2070e+00,  1.8702e-01],\n           ...,\n           [ 3.6573e-01, -1.2353e-01, -4.1257e-01,  ...,  1.8642e+00,\n             2.1319e+00,  1.3388e+00],\n           [ 9.4318e-01, -2.8603e+00,  5.7746e-01,  ...,  2.3686e+00,\n             6.0536e-01, -1.3134e+00],\n           [ 5.2675e-01,  2.2496e+00, -1.6076e+00,  ...,  2.6693e+00,\n             3.1142e+00, -2.7588e-01]]]], device='mps:0'),\n tensor([[[[-1.4161,  2.6903,  0.9817,  ...,  0.0954, -1.6757,  0.1954],\n           [ 1.7902,  0.3442, -0.1021,  ...,  0.5413,  1.6832,  0.8296],\n           [ 1.6790, -0.3181,  1.1471,  ...,  0.8845, -0.5077,  1.3886],\n           ...,\n           [-1.9442,  0.0579,  1.2978,  ...,  1.7440, -1.2122, -0.9292],\n           [ 1.4087,  1.0933, -0.8280,  ...,  0.4024, -0.9259,  0.2995],\n           [-0.3811,  0.1803,  1.2721,  ...,  3.6702,  1.9252,  0.5701]],\n \n          [[-0.5554, -0.4998, -2.3949,  ..., -1.1663, -2.9386,  0.7843],\n           [-1.3079, -0.6411, -2.4944,  ...,  0.8111,  0.0192, -0.6006],\n           [-1.1216,  0.7531,  0.2052,  ..., -0.2702,  1.2348, -1.9650],\n           ...,\n           [ 0.0275,  1.7257, -1.4479,  ..., -2.6139, -0.0773,  0.1543],\n           [ 2.3007, -0.1599, -0.9019,  ..., -1.4079, -0.7956, -1.2932],\n           [-0.5952,  1.4160,  0.4696,  ..., -1.6799, -3.3901, -1.6681]],\n \n          [[ 2.4603, -0.5772, -0.4518,  ...,  0.9583,  0.6820, -0.6543],\n           [-2.0604, -1.4994, -0.3947,  ..., -1.1823, -1.0826, -1.5690],\n           [-1.3864, -0.0627,  2.3127,  ...,  0.0829,  1.6655,  2.3232],\n           ...,\n           [ 0.4757, -1.2880,  1.8036,  ..., -1.8610,  1.6494,  2.9506],\n           [ 3.3457,  0.6623,  1.0488,  ...,  0.9087,  1.0773,  2.8925],\n           [-1.1500, -0.4663,  1.2954,  ...,  2.8134,  2.9367,  1.2879]],\n \n          [[ 0.4116,  0.2407,  1.2269,  ..., -0.5021,  0.1099,  0.8052],\n           [-0.3590,  1.9093,  1.8123,  ...,  0.9321,  2.6290,  3.1716],\n           [-2.2473,  2.0914, -0.2543,  ..., -0.6371, -1.0942,  0.2460],\n           ...,\n           [ 0.2987, -0.1362, -0.3902,  ...,  1.6993,  1.9250,  1.2044],\n           [ 0.8056, -2.6023,  0.4751,  ...,  2.1379,  0.5416, -1.1741],\n           [ 0.4008,  1.9998, -1.4756,  ...,  2.4190,  2.7793, -0.2192]]]],\n        device='mps:0'),\n tensor([[[[-1.2727,  2.5120,  0.9732,  ...,  0.0773, -1.4884,  0.2466],\n           [ 1.6831,  0.4220, -0.0556,  ...,  0.5737,  1.5926,  0.8226],\n           [ 1.5348, -0.2180,  1.0149,  ...,  0.8436, -0.4616,  1.3415],\n           ...,\n           [-1.6581,  0.1390,  1.2548,  ...,  1.5959, -1.0529, -0.8220],\n           [ 1.3254,  1.0444, -0.6832,  ...,  0.3784, -0.8201,  0.2423],\n           [-0.2972,  0.2354,  1.2254,  ...,  3.3093,  1.7353,  0.5125]],\n \n          [[-0.5450, -0.4680, -2.1840,  ..., -1.0537, -2.7330,  0.6930],\n           [-1.2470, -0.5795, -2.3097,  ...,  0.7222, -0.0272, -0.5830],\n           [-1.0822,  0.7558,  0.1976,  ..., -0.2305,  1.0685, -1.7958],\n           ...,\n           [ 0.0401,  1.5615, -1.2780,  ..., -2.4037, -0.1473,  0.0872],\n           [ 2.0701, -0.1377, -0.7892,  ..., -1.3400, -0.8011, -1.2165],\n           [-0.5066,  1.2812,  0.4489,  ..., -1.5799, -3.1143, -1.5454]],\n \n          [[ 2.2063, -0.5287, -0.4275,  ...,  0.8456,  0.5924, -0.5821],\n           [-1.9666, -1.3967, -0.4580,  ..., -1.0862, -1.0490, -1.4575],\n           [-1.2993, -0.0212,  2.0990,  ...,  0.0934,  1.5015,  2.1513],\n           ...,\n           [ 0.4677, -1.0858,  1.6784,  ..., -1.6056,  1.5457,  2.7104],\n           [ 3.0403,  0.6571,  0.9982,  ...,  0.8699,  1.0267,  2.6505],\n           [-0.9900, -0.3175,  1.2547,  ...,  2.6047,  2.7219,  1.2395]],\n \n          [[ 0.3323,  0.2438,  1.1230,  ..., -0.4427,  0.0748,  0.7786],\n           [-0.3203,  1.8113,  1.6128,  ...,  0.8915,  2.3999,  2.9080],\n           [-2.0603,  1.9406, -0.2519,  ..., -0.5227, -1.0148,  0.2738],\n           ...,\n           [ 0.2284, -0.1532, -0.3737,  ...,  1.5487,  1.7398,  1.0869],\n           [ 0.6811, -2.3715,  0.3872,  ...,  1.9311,  0.4912, -1.0526],\n           [ 0.2939,  1.7525, -1.3436,  ...,  2.1917,  2.5049, -0.1781]]]],\n        device='mps:0'),\n tensor([[[[-1.1477e+00,  2.3599e+00,  9.6568e-01,  ...,  5.7294e-02,\n            -1.3243e+00,  2.8784e-01],\n           [ 1.5912e+00,  5.1058e-01, -1.7645e-02,  ...,  6.0876e-01,\n             1.5273e+00,  8.1651e-01],\n           [ 1.3948e+00, -1.2919e-01,  9.1340e-01,  ...,  8.0200e-01,\n            -4.3903e-01,  1.2887e+00],\n           ...,\n           [-1.4134e+00,  1.9638e-01,  1.1981e+00,  ...,  1.4654e+00,\n            -9.0790e-01, -7.2085e-01],\n           [ 1.2560e+00,  9.9756e-01, -5.4108e-01,  ...,  3.6882e-01,\n            -7.1562e-01,  2.1076e-01],\n           [-2.2426e-01,  3.0387e-01,  1.1661e+00,  ...,  3.0134e+00,\n             1.5724e+00,  4.9450e-01]],\n \n          [[-5.2513e-01, -4.2961e-01, -1.9930e+00,  ..., -9.5284e-01,\n            -2.5463e+00,  6.1604e-01],\n           [-1.2001e+00, -5.2550e-01, -2.1368e+00,  ...,  6.4313e-01,\n            -5.9565e-02, -5.6317e-01],\n           [-1.0396e+00,  7.2770e-01,  1.9278e-01,  ..., -1.8866e-01,\n             9.3031e-01, -1.6495e+00],\n           ...,\n           [ 5.7326e-02,  1.4153e+00, -1.1278e+00,  ..., -2.2115e+00,\n            -2.0495e-01,  2.6590e-02],\n           [ 1.8666e+00, -1.1968e-01, -7.0056e-01,  ..., -1.2678e+00,\n            -7.9530e-01, -1.1378e+00],\n           [-4.5186e-01,  1.1605e+00,  4.0061e-01,  ..., -1.4897e+00,\n            -2.8766e+00, -1.4268e+00]],\n \n          [[ 1.9962e+00, -4.8070e-01, -4.2423e-01,  ...,  7.3928e-01,\n             5.1492e-01, -5.1845e-01],\n           [-1.8777e+00, -1.3092e+00, -5.1131e-01,  ..., -1.0010e+00,\n            -1.0264e+00, -1.3804e+00],\n           [-1.2424e+00,  1.0991e-03,  1.9298e+00,  ...,  1.1779e-01,\n             1.3666e+00,  2.0127e+00],\n           ...,\n           [ 4.5998e-01, -9.1141e-01,  1.5636e+00,  ..., -1.3801e+00,\n             1.4506e+00,  2.4964e+00],\n           [ 2.7594e+00,  6.4827e-01,  9.4450e-01,  ...,  8.3988e-01,\n             9.8133e-01,  2.4476e+00],\n           [-8.6142e-01, -2.0285e-01,  1.1862e+00,  ...,  2.4083e+00,\n             2.5106e+00,  1.1962e+00]],\n \n          [[ 2.6987e-01,  2.6084e-01,  1.0220e+00,  ..., -3.8688e-01,\n             4.7756e-02,  7.5568e-01],\n           [-2.8209e-01,  1.7381e+00,  1.4479e+00,  ...,  8.7441e-01,\n             2.2264e+00,  2.6846e+00],\n           [-1.8917e+00,  1.8138e+00, -2.2009e-01,  ..., -3.9706e-01,\n            -9.3624e-01,  3.1119e-01],\n           ...,\n           [ 1.6875e-01, -1.6428e-01, -3.6317e-01,  ...,  1.4086e+00,\n             1.5679e+00,  9.7720e-01],\n           [ 5.7015e-01, -2.1677e+00,  2.9925e-01,  ...,  1.7382e+00,\n             4.3183e-01, -9.4313e-01],\n           [ 1.9115e-01,  1.5518e+00, -1.2431e+00,  ...,  1.9899e+00,\n             2.2358e+00, -1.3178e-01]]]], device='mps:0'),\n tensor([[[[-1.0400,  2.2230,  0.9649,  ...,  0.0373, -1.1841,  0.3186],\n           [ 1.4995,  0.5903,  0.0099,  ...,  0.6443,  1.4653,  0.8007],\n           [ 1.2725, -0.0530,  0.8207,  ...,  0.7733, -0.4163,  1.2453],\n           ...,\n           [-1.1859,  0.2545,  1.1596,  ...,  1.3531, -0.7691, -0.6273],\n           [ 1.1986,  0.9610, -0.4105,  ...,  0.3622, -0.6202,  0.1783],\n           [-0.1549,  0.3460,  1.1181,  ...,  2.7307,  1.4250,  0.4595]],\n \n          [[-0.5090, -0.4003, -1.8243,  ..., -0.8666, -2.3860,  0.5475],\n           [-1.1603, -0.4758, -1.9899,  ...,  0.5751, -0.0810, -0.5462],\n           [-1.0077,  0.7020,  0.1924,  ..., -0.1527,  0.8091, -1.5210],\n           ...,\n           [ 0.0716,  1.2927, -0.9943,  ..., -2.0414, -0.2588, -0.0225],\n           [ 1.6944, -0.0937, -0.6077,  ..., -1.2068, -0.7934, -1.0637],\n           [-0.3812,  1.0650,  0.3766,  ..., -1.4063, -2.6563, -1.3177]],\n \n          [[ 1.8302, -0.4340, -0.4326,  ...,  0.6531,  0.4545, -0.4552],\n           [-1.7941, -1.2252, -0.5667,  ..., -0.9172, -1.0042, -1.3179],\n           [-1.2007,  0.0176,  1.7809,  ...,  0.1546,  1.2545,  1.8973],\n           ...,\n           [ 0.4523, -0.7527,  1.4688,  ..., -1.1640,  1.3779,  2.3144],\n           [ 2.5236,  0.6469,  0.9036,  ...,  0.8241,  0.9479,  2.2702],\n           [-0.7370, -0.0970,  1.1399,  ...,  2.2421,  2.3350,  1.1599]],\n \n          [[ 0.2225,  0.2810,  0.9279,  ..., -0.3366,  0.0211,  0.7363],\n           [-0.2547,  1.6720,  1.2914,  ...,  0.8674,  2.0747,  2.4719],\n           [-1.7428,  1.6988, -0.1866,  ..., -0.2751, -0.8661,  0.3446],\n           ...,\n           [ 0.1074, -0.1782, -0.3512,  ...,  1.2833,  1.4170,  0.8768],\n           [ 0.4826, -1.9764,  0.2304,  ...,  1.5707,  0.3856, -0.8444],\n           [ 0.1027,  1.3538, -1.1445,  ...,  1.8102,  2.0095, -0.0988]]]],\n        device='mps:0'),\n tensor([[[[-9.4108e-01,  2.1004e+00,  9.6578e-01,  ...,  1.7408e-02,\n            -1.0548e+00,  3.5163e-01],\n           [ 1.4158e+00,  6.6807e-01,  3.8081e-02,  ...,  6.7267e-01,\n             1.4069e+00,  7.8237e-01],\n           [ 1.1671e+00,  2.8535e-02,  7.4084e-01,  ...,  7.5562e-01,\n            -4.0092e-01,  1.2062e+00],\n           ...,\n           [-9.8369e-01,  3.0590e-01,  1.1164e+00,  ...,  1.2470e+00,\n            -6.4731e-01, -5.4257e-01],\n           [ 1.1434e+00,  9.2231e-01, -2.9283e-01,  ...,  3.4828e-01,\n            -5.4100e-01,  1.3786e-01],\n           [-9.1772e-02,  4.0616e-01,  1.0747e+00,  ...,  2.5021e+00,\n             1.3048e+00,  4.6489e-01]],\n \n          [[-4.9733e-01, -3.7633e-01, -1.6679e+00,  ..., -7.8919e-01,\n            -2.2402e+00,  4.8854e-01],\n           [-1.1220e+00, -4.2855e-01, -1.8555e+00,  ...,  5.0767e-01,\n            -9.7969e-02, -5.2650e-01],\n           [-9.7894e-01,  6.7683e-01,  1.9262e-01,  ..., -1.2867e-01,\n             6.9701e-01, -1.4039e+00],\n           ...,\n           [ 8.8427e-02,  1.1805e+00, -8.7336e-01,  ..., -1.8820e+00,\n            -3.0600e-01, -7.1207e-02],\n           [ 1.5372e+00, -7.5065e-02, -5.2858e-01,  ..., -1.1505e+00,\n            -7.9149e-01, -1.0011e+00],\n           [-3.3271e-01,  9.6810e-01,  3.3774e-01,  ..., -1.3395e+00,\n            -2.4715e+00, -1.2224e+00]],\n \n          [[ 1.6799e+00, -3.8873e-01, -4.3093e-01,  ...,  5.7670e-01,\n             4.0637e-01, -3.8919e-01],\n           [-1.7163e+00, -1.1445e+00, -6.1421e-01,  ..., -8.4571e-01,\n            -9.8217e-01, -1.2555e+00],\n           [-1.1612e+00,  3.0012e-02,  1.6285e+00,  ...,  1.7197e-01,\n             1.1391e+00,  1.7819e+00],\n           ...,\n           [ 4.4320e-01, -6.1339e-01,  1.3791e+00,  ..., -9.7740e-01,\n             1.3040e+00,  2.1439e+00],\n           [ 2.3067e+00,  6.4071e-01,  8.5900e-01,  ...,  8.0566e-01,\n             9.1488e-01,  2.1082e+00],\n           [-6.3157e-01, -3.9702e-03,  1.0872e+00,  ...,  2.0909e+00,\n             2.1673e+00,  1.1308e+00]],\n \n          [[ 1.8185e-01,  3.0007e-01,  8.4262e-01,  ..., -2.9147e-01,\n             1.3900e-03,  7.2682e-01],\n           [-2.3139e-01,  1.6112e+00,  1.1489e+00,  ...,  8.6573e-01,\n             1.9429e+00,  2.2842e+00],\n           [-1.6069e+00,  1.5977e+00, -1.5295e-01,  ..., -1.5751e-01,\n            -8.0239e-01,  3.8080e-01],\n           ...,\n           [ 5.5953e-02, -1.8334e-01, -3.4067e-01,  ...,  1.1678e+00,\n             1.2792e+00,  7.8934e-01],\n           [ 4.0396e-01, -1.8036e+00,  1.6472e-01,  ...,  1.4125e+00,\n             3.3886e-01, -7.5607e-01],\n           [ 1.8758e-02,  1.1905e+00, -1.0594e+00,  ...,  1.6531e+00,\n             1.7955e+00, -4.9383e-02]]]], device='mps:0'),\n tensor([[[[-8.6027e-01,  1.9894e+00,  9.7101e-01,  ..., -2.2683e-03,\n            -9.4097e-01,  3.7229e-01],\n           [ 1.3379e+00,  7.4482e-01,  5.8819e-02,  ...,  7.0682e-01,\n             1.3614e+00,  7.5399e-01],\n           [ 1.0715e+00,  1.1369e-01,  6.6867e-01,  ...,  7.4692e-01,\n            -3.9068e-01,  1.1711e+00],\n           ...,\n           [-7.9960e-01,  3.4939e-01,  1.0819e+00,  ...,  1.1514e+00,\n            -5.3124e-01, -4.6595e-01],\n           [ 1.0983e+00,  8.9064e-01, -1.7939e-01,  ...,  3.4102e-01,\n            -4.6309e-01,  1.0699e-01],\n           [-3.3371e-02,  4.4035e-01,  1.0278e+00,  ...,  2.2823e+00,\n             1.1943e+00,  4.5434e-01]],\n \n          [[-4.8484e-01, -3.5516e-01, -1.5318e+00,  ..., -7.2844e-01,\n            -2.1210e+00,  4.3715e-01],\n           [-1.0883e+00, -3.8584e-01, -1.7354e+00,  ...,  4.4135e-01,\n            -1.0353e-01, -5.0918e-01],\n           [-9.6032e-01,  6.4907e-01,  1.9491e-01,  ..., -1.0978e-01,\n             6.0588e-01, -1.3052e+00],\n           ...,\n           [ 1.0465e-01,  1.0830e+00, -7.6646e-01,  ..., -1.7402e+00,\n            -3.5078e-01, -1.1769e-01],\n           [ 1.4014e+00, -5.4965e-02, -4.5472e-01,  ..., -1.1016e+00,\n            -7.9250e-01, -9.4497e-01],\n           [-2.8082e-01,  8.8911e-01,  3.1282e-01,  ..., -1.2792e+00,\n            -2.3004e+00, -1.1380e+00]],\n \n          [[ 1.5625e+00, -3.5443e-01, -4.3789e-01,  ...,  5.1550e-01,\n             3.7247e-01, -3.2929e-01],\n           [-1.6489e+00, -1.0755e+00, -6.6085e-01,  ..., -7.8321e-01,\n            -9.6042e-01, -1.2126e+00],\n           [-1.1315e+00,  4.1844e-02,  1.4920e+00,  ...,  1.9051e-01,\n             1.0342e+00,  1.6825e+00],\n           ...,\n           [ 4.2976e-01, -4.8859e-01,  1.3002e+00,  ..., -8.0332e-01,\n             1.2406e+00,  1.9901e+00],\n           [ 2.1146e+00,  6.3293e-01,  8.2140e-01,  ...,  7.9082e-01,\n             8.8500e-01,  1.9621e+00],\n           [-5.3247e-01,  7.4626e-02,  1.0426e+00,  ...,  1.9494e+00,\n             2.0161e+00,  1.0992e+00]],\n \n          [[ 1.4672e-01,  3.2121e-01,  7.6433e-01,  ..., -2.5540e-01,\n            -2.1292e-02,  7.1554e-01],\n           [-2.1093e-01,  1.5570e+00,  1.0146e+00,  ...,  8.6913e-01,\n             1.8409e+00,  2.1047e+00],\n           [-1.4839e+00,  1.5102e+00, -1.1958e-01,  ..., -4.4237e-02,\n            -7.5025e-01,  4.0798e-01],\n           ...,\n           [ 3.8100e-03, -1.9052e-01, -3.2949e-01,  ...,  1.0627e+00,\n             1.1557e+00,  7.0749e-01],\n           [ 3.4300e-01, -1.6402e+00,  1.1245e-01,  ...,  1.2705e+00,\n             2.9942e-01, -6.7121e-01],\n           [-5.3784e-02,  1.0292e+00, -9.7915e-01,  ...,  1.5123e+00,\n             1.6113e+00, -7.7746e-03]]]], device='mps:0'),\n tensor([[[[-0.7891,  1.8914,  0.9797,  ..., -0.0270, -0.8310,  0.3952],\n           [ 1.2677,  0.8193,  0.0795,  ...,  0.7390,  1.3232,  0.7189],\n           [ 0.9847,  0.1928,  0.6040,  ...,  0.7404, -0.3886,  1.1317],\n           ...,\n           [-0.6308,  0.3902,  1.0489,  ...,  1.0621, -0.4257, -0.3983],\n           [ 1.0588,  0.8616, -0.0700,  ...,  0.3288, -0.3997,  0.0676],\n           [ 0.0197,  0.4633,  0.9748,  ...,  2.0854,  1.0988,  0.4574]],\n \n          [[-0.4755, -0.3419, -1.4076,  ..., -0.6702, -2.0113,  0.3879],\n           [-1.0595, -0.3483, -1.6294,  ...,  0.3802, -0.1054, -0.4909],\n           [-0.9444,  0.6278,  0.1985,  ..., -0.0957,  0.5276, -1.2142],\n           ...,\n           [ 0.1223,  0.9920, -0.6673,  ..., -1.6112, -0.3941, -0.1644],\n           [ 1.2825, -0.0324, -0.3866,  ..., -1.0586, -0.7965, -0.8977],\n           [-0.2347,  0.8154,  0.2941,  ..., -1.2350, -2.1550, -1.0662]],\n \n          [[ 1.4502, -0.3249, -0.4488,  ...,  0.4571,  0.3438, -0.2636],\n           [-1.5905, -1.0096, -0.7099,  ..., -0.7287, -0.9422, -1.1789],\n           [-1.1063,  0.0471,  1.3681,  ...,  0.2127,  0.9419,  1.5910],\n           ...,\n           [ 0.4154, -0.3750,  1.2264,  ..., -0.6440,  1.1804,  1.8495],\n           [ 1.9433,  0.6190,  0.7904,  ...,  0.7803,  0.8604,  1.8311],\n           [-0.4430,  0.1383,  1.0026,  ...,  1.8239,  1.8783,  1.0714]],\n \n          [[ 0.1096,  0.3456,  0.6874,  ..., -0.2239, -0.0387,  0.7096],\n           [-0.1938,  1.5072,  0.8878,  ...,  0.8745,  1.7545,  1.9360],\n           [-1.3721,  1.4289, -0.0888,  ...,  0.0693, -0.7013,  0.4294],\n           ...,\n           [-0.0433, -0.1964, -0.3211,  ...,  0.9684,  1.0450,  0.6360],\n           [ 0.2959, -1.4858,  0.0661,  ...,  1.1401,  0.2630, -0.5921],\n           [-0.1185,  0.8761, -0.9078,  ...,  1.3843,  1.4432,  0.0399]]]],\n        device='mps:0'),\n tensor([[[[-0.7252,  1.7989,  0.9908,  ..., -0.0588, -0.7243,  0.4174],\n           [ 1.2015,  0.8909,  0.1027,  ...,  0.7647,  1.2937,  0.6804],\n           [ 0.9021,  0.2662,  0.5399,  ...,  0.7333, -0.3933,  1.0957],\n           ...,\n           [-0.4717,  0.4288,  1.0189,  ...,  0.9753, -0.3311, -0.3382],\n           [ 1.0236,  0.8335,  0.0326,  ...,  0.3061, -0.3565,  0.0180],\n           [ 0.0711,  0.4877,  0.9258,  ...,  1.9309,  1.0389,  0.4774]],\n \n          [[-0.4663, -0.3298, -1.2935,  ..., -0.6187, -1.9167,  0.3438],\n           [-1.0335, -0.3150, -1.5350,  ...,  0.3249, -0.1048, -0.4734],\n           [-0.9301,  0.6136,  0.2016,  ..., -0.0864,  0.4560, -1.1305],\n           ...,\n           [ 0.1400,  0.9110, -0.5765,  ..., -1.4900, -0.4334, -0.2071],\n           [ 1.1742, -0.0095, -0.3216,  ..., -1.0200, -0.8024, -0.8530],\n           [-0.1892,  0.7519,  0.2785,  ..., -1.2104, -2.0319, -0.9991]],\n \n          [[ 1.3469, -0.2982, -0.4630,  ...,  0.3964,  0.3246, -0.1913],\n           [-1.5365, -0.9489, -0.7570,  ..., -0.6769, -0.9224, -1.1549],\n           [-1.0819,  0.0527,  1.2557,  ...,  0.2270,  0.8524,  1.5070],\n           ...,\n           [ 0.4018, -0.2662,  1.1605,  ..., -0.4935,  1.1301,  1.7235],\n           [ 1.7848,  0.6076,  0.7629,  ...,  0.7783,  0.8438,  1.7160],\n           [-0.3608,  0.2033,  0.9699,  ...,  1.7133,  1.7572,  1.0449]],\n \n          [[ 0.0722,  0.3664,  0.6140,  ..., -0.1994, -0.0485,  0.7071],\n           [-0.1787,  1.4591,  0.7699,  ...,  0.8717,  1.6757,  1.7708],\n           [-1.2709,  1.3541, -0.0649,  ...,  0.1717, -0.6590,  0.4473],\n           ...,\n           [-0.0904, -0.2033, -0.3137,  ...,  0.8814,  0.9418,  0.5696],\n           [ 0.2532, -1.3443,  0.0222,  ...,  1.0172,  0.2242, -0.5213],\n           [-0.1805,  0.7338, -0.8416,  ...,  1.2734,  1.2982,  0.0953]]]],\n        device='mps:0'),\n tensor([[[[-0.6694,  1.7110,  1.0038,  ..., -0.0913, -0.6216,  0.4385],\n           [ 1.1422,  0.9581,  0.1298,  ...,  0.7912,  1.2646,  0.6333],\n           [ 0.8218,  0.3391,  0.4756,  ...,  0.7260, -0.3958,  1.0579],\n           ...,\n           [-0.3235,  0.4639,  0.9897,  ...,  0.8933, -0.2419, -0.2793],\n           [ 0.9913,  0.8064,  0.1312,  ...,  0.2827, -0.3233, -0.0375],\n           [ 0.1184,  0.5079,  0.8767,  ...,  1.7958,  0.9908,  0.5071]],\n \n          [[-0.4609, -0.3204, -1.1905,  ..., -0.5760, -1.8340,  0.3040],\n           [-1.0057, -0.2836, -1.4507,  ...,  0.2732, -0.0968, -0.4552],\n           [-0.9236,  0.6024,  0.2062,  ..., -0.0856,  0.3965, -1.0525],\n           ...,\n           [ 0.1588,  0.8376, -0.4918,  ..., -1.3734, -0.4684, -0.2492],\n           [ 1.0765,  0.0137, -0.2589,  ..., -0.9836, -0.8084, -0.8142],\n           [-0.1472,  0.6938,  0.2658,  ..., -1.1967, -1.9229, -0.9431]],\n \n          [[ 1.2542, -0.2760, -0.4792,  ...,  0.3452,  0.3108, -0.1261],\n           [-1.4897, -0.8902, -0.8066,  ..., -0.6355, -0.9010, -1.1398],\n           [-1.0567,  0.0609,  1.1476,  ...,  0.2394,  0.7752,  1.4304],\n           ...,\n           [ 0.3865, -0.1633,  1.1000,  ..., -0.3527,  1.0846,  1.6039],\n           [ 1.6379,  0.5946,  0.7379,  ...,  0.7782,  0.8278,  1.6065],\n           [-0.2849,  0.2627,  0.9407,  ...,  1.6161,  1.6468,  1.0218]],\n \n          [[ 0.0341,  0.3836,  0.5418,  ..., -0.1840, -0.0591,  0.6994],\n           [-0.1647,  1.4117,  0.6562,  ...,  0.8679,  1.6085,  1.6077],\n           [-1.1742,  1.2837, -0.0459,  ...,  0.2664, -0.6145,  0.4588],\n           ...,\n           [-0.1341, -0.2060, -0.3041,  ...,  0.8024,  0.8501,  0.5122],\n           [ 0.2202, -1.2065, -0.0154,  ...,  0.9045,  0.1871, -0.4537],\n           [-0.2374,  0.5994, -0.7764,  ...,  1.1769,  1.1660,  0.1597]]]],\n        device='mps:0'),\n tensor([[[[-0.6194,  1.6270,  1.0220,  ..., -0.1237, -0.5226,  0.4542],\n           [ 1.0909,  1.0175,  0.1602,  ...,  0.8212,  1.2364,  0.5820],\n           [ 0.7432,  0.4075,  0.4112,  ...,  0.7182, -0.4029,  1.0218],\n           ...,\n           [-0.1815,  0.4961,  0.9629,  ...,  0.8137, -0.1562, -0.2216],\n           [ 0.9614,  0.7785,  0.2244,  ...,  0.2626, -0.2855, -0.0896],\n           [ 0.1650,  0.5294,  0.8278,  ...,  1.6593,  0.9411,  0.5296]],\n \n          [[-0.4563, -0.3133, -1.0963,  ..., -0.5458, -1.7566,  0.2691],\n           [-0.9767, -0.2557, -1.3727,  ...,  0.2257, -0.0864, -0.4386],\n           [-0.9223,  0.6010,  0.2112,  ..., -0.0808,  0.3421, -0.9750],\n           ...,\n           [ 0.1785,  0.7723, -0.4119,  ..., -1.2639, -0.5012, -0.2896],\n           [ 0.9878,  0.0364, -0.1976,  ..., -0.9501, -0.8126, -0.7753],\n           [-0.1048,  0.6436,  0.2555,  ..., -1.1849, -1.8156, -0.8917]],\n \n          [[ 1.1665, -0.2543, -0.4964,  ...,  0.3007,  0.3000, -0.0615],\n           [-1.4486, -0.8366, -0.8566,  ..., -0.5960, -0.8795, -1.1318],\n           [-1.0321,  0.0704,  1.0478,  ...,  0.2562,  0.7024,  1.3630],\n           ...,\n           [ 0.3707, -0.0637,  1.0459,  ..., -0.2169,  1.0481,  1.4912],\n           [ 1.5003,  0.5827,  0.7143,  ...,  0.7792,  0.8139,  1.5021],\n           [-0.2151,  0.3211,  0.9158,  ...,  1.5191,  1.5383,  1.0018]],\n \n          [[-0.0074,  0.3941,  0.4708,  ..., -0.1756, -0.0713,  0.6867],\n           [-0.1522,  1.3627,  0.5452,  ...,  0.8615,  1.5452,  1.4445],\n           [-1.0813,  1.2142, -0.0279,  ...,  0.3650, -0.5709,  0.4711],\n           ...,\n           [-0.1791, -0.2081, -0.2944,  ...,  0.7234,  0.7636,  0.4569],\n           [ 0.1907, -1.0770, -0.0517,  ...,  0.7959,  0.1538, -0.3888],\n           [-0.2960,  0.4712, -0.7157,  ...,  1.0847,  1.0450,  0.2233]]]],\n        device='mps:0'),\n tensor([[[[-5.7200e-01,  1.5430e+00,  1.0497e+00,  ..., -1.5371e-01,\n            -4.2238e-01,  4.6855e-01],\n           [ 1.0450e+00,  1.0706e+00,  1.9440e-01,  ...,  8.4972e-01,\n             1.2151e+00,  5.2899e-01],\n           [ 6.6653e-01,  4.7170e-01,  3.4677e-01,  ...,  7.0818e-01,\n            -4.1687e-01,  9.8290e-01],\n           ...,\n           [-4.1428e-02,  5.2852e-01,  9.3918e-01,  ...,  7.3618e-01,\n            -7.0389e-02, -1.6301e-01],\n           [ 9.3257e-01,  7.5152e-01,  3.1851e-01,  ...,  2.4048e-01,\n            -2.5771e-01, -1.4922e-01],\n           [ 2.1211e-01,  5.5235e-01,  7.7597e-01,  ...,  1.5388e+00,\n             9.0206e-01,  5.6347e-01]],\n \n          [[-4.5661e-01, -3.1426e-01, -1.0027e+00,  ..., -5.2490e-01,\n            -1.6855e+00,  2.3784e-01],\n           [-9.4494e-01, -2.3075e-01, -1.2924e+00,  ...,  1.8244e-01,\n            -7.3372e-02, -4.1930e-01],\n           [-9.2266e-01,  6.0736e-01,  2.1702e-01,  ..., -7.6971e-02,\n             2.9973e-01, -8.9670e-01],\n           ...,\n           [ 1.9859e-01,  7.1264e-01, -3.3644e-01,  ..., -1.1563e+00,\n            -5.3093e-01, -3.2805e-01],\n           [ 9.0597e-01,  5.9214e-02, -1.3632e-01,  ..., -9.1630e-01,\n            -8.1803e-01, -7.3705e-01],\n           [-6.4500e-02,  5.9709e-01,  2.4666e-01,  ..., -1.1818e+00,\n            -1.7192e+00, -8.4430e-01]],\n \n          [[ 1.0765e+00, -2.3506e-01, -5.1701e-01,  ...,  2.6286e-01,\n             2.9225e-01,  7.9345e-04],\n           [-1.4127e+00, -7.8663e-01, -9.1259e-01,  ..., -5.5942e-01,\n            -8.5341e-01, -1.1259e+00],\n           [-1.0103e+00,  7.5576e-02,  9.6475e-01,  ...,  2.7999e-01,\n             6.3695e-01,  1.3030e+00],\n           ...,\n           [ 3.5147e-01,  3.5087e-02,  9.9673e-01,  ..., -8.2290e-02,\n             1.0205e+00,  1.3845e+00],\n           [ 1.3694e+00,  5.6903e-01,  6.9133e-01,  ...,  7.8841e-01,\n             8.0541e-01,  1.4064e+00],\n           [-1.5053e-01,  3.7553e-01,  8.9395e-01,  ...,  1.4303e+00,\n             1.4383e+00,  9.8578e-01]],\n \n          [[-4.7986e-02,  4.0157e-01,  4.0226e-01,  ..., -1.7434e-01,\n            -8.3702e-02,  6.7015e-01],\n           [-1.3797e-01,  1.3111e+00,  4.3852e-01,  ...,  8.5546e-01,\n             1.4890e+00,  1.2864e+00],\n           [-9.9048e-01,  1.1477e+00, -1.3956e-02,  ...,  4.6568e-01,\n            -5.2625e-01,  4.8309e-01],\n           ...,\n           [-2.2657e-01, -2.0819e-01, -2.8418e-01,  ...,  6.4559e-01,\n             6.8310e-01,  4.0456e-01],\n           [ 1.6553e-01, -9.5042e-01, -8.6049e-02,  ...,  6.9233e-01,\n             1.1679e-01, -3.2605e-01],\n           [-3.5765e-01,  3.4405e-01, -6.5706e-01,  ...,  1.0031e+00,\n             9.3008e-01,  2.9704e-01]]]], device='mps:0'),\n tensor([[[[-5.3101e-01,  1.4597e+00,  1.0849e+00,  ..., -1.8937e-01,\n            -3.2787e-01,  4.8179e-01],\n           [ 1.0027e+00,  1.1206e+00,  2.3620e-01,  ...,  8.8087e-01,\n             1.2002e+00,  4.7405e-01],\n           [ 5.9120e-01,  5.4032e-01,  2.8668e-01,  ...,  6.9305e-01,\n            -4.3383e-01,  9.4277e-01],\n           ...,\n           [ 9.9248e-02,  5.6189e-01,  9.1595e-01,  ...,  6.5802e-01,\n             1.4550e-02, -1.0023e-01],\n           [ 9.0293e-01,  7.2422e-01,  4.1470e-01,  ...,  2.1960e-01,\n            -2.2910e-01, -2.1501e-01],\n           [ 2.6232e-01,  5.7960e-01,  7.2126e-01,  ...,  1.4145e+00,\n             8.6175e-01,  5.9705e-01]],\n \n          [[-4.5820e-01, -3.2505e-01, -9.1087e-01,  ..., -5.0905e-01,\n            -1.6191e+00,  2.0386e-01],\n           [-9.0747e-01, -2.0980e-01, -1.2090e+00,  ...,  1.4881e-01,\n            -6.1177e-02, -4.0390e-01],\n           [-9.2487e-01,  6.1861e-01,  2.3251e-01,  ..., -7.6746e-02,\n             2.6459e-01, -8.2195e-01],\n           ...,\n           [ 2.1855e-01,  6.6045e-01, -2.6474e-01,  ..., -1.0489e+00,\n            -5.5449e-01, -3.6509e-01],\n           [ 8.2756e-01,  8.2480e-02, -7.3165e-02,  ..., -8.8010e-01,\n            -8.1931e-01, -6.9632e-01],\n           [-2.3865e-02,  5.5516e-01,  2.3817e-01,  ..., -1.1793e+00,\n            -1.6201e+00, -8.0241e-01]],\n \n          [[ 9.8588e-01, -2.2393e-01, -5.4517e-01,  ...,  2.2404e-01,\n             2.9137e-01,  7.2102e-02],\n           [-1.3736e+00, -7.3663e-01, -9.7291e-01,  ..., -5.2020e-01,\n            -8.2635e-01, -1.1242e+00],\n           [-9.8746e-01,  7.3199e-02,  9.0217e-01,  ...,  3.0931e-01,\n             5.8169e-01,  1.2532e+00],\n           ...,\n           [ 3.2819e-01,  1.3398e-01,  9.4982e-01,  ...,  5.0852e-02,\n             1.0018e+00,  1.2740e+00],\n           [ 1.2384e+00,  5.5358e-01,  6.6372e-01,  ...,  8.0149e-01,\n             7.9356e-01,  1.3079e+00],\n           [-8.9733e-02,  4.2936e-01,  8.7453e-01,  ...,  1.3435e+00,\n             1.3376e+00,  9.7277e-01]],\n \n          [[-8.9932e-02,  4.0682e-01,  3.3363e-01,  ..., -1.7953e-01,\n            -9.2030e-02,  6.5363e-01],\n           [-1.1536e-01,  1.2566e+00,  3.3534e-01,  ...,  8.5502e-01,\n             1.4394e+00,  1.1350e+00],\n           [-8.9324e-01,  1.0843e+00,  6.7312e-04,  ...,  5.6704e-01,\n            -4.7744e-01,  4.9618e-01],\n           ...,\n           [-2.7689e-01, -2.0396e-01, -2.7362e-01,  ...,  5.6503e-01,\n             6.0726e-01,  3.5430e-01],\n           [ 1.4321e-01, -8.2117e-01, -1.1850e-01,  ...,  5.9259e-01,\n             8.1263e-02, -2.6279e-01],\n           [-4.2291e-01,  2.1568e-01, -5.9857e-01,  ...,  9.2660e-01,\n             8.1857e-01,  3.7436e-01]]]], device='mps:0'),\n tensor([[[[-0.4961,  1.3747,  1.1250,  ..., -0.2269, -0.2382,  0.4965],\n           [ 0.9563,  1.1789,  0.2874,  ...,  0.9096,  1.1972,  0.4202],\n           [ 0.5160,  0.6089,  0.2285,  ...,  0.6732, -0.4491,  0.8999],\n           ...,\n           [ 0.2469,  0.6026,  0.8883,  ...,  0.5748,  0.1126, -0.0269],\n           [ 0.8715,  0.6929,  0.5255,  ...,  0.1937, -0.1984, -0.2951],\n           [ 0.3123,  0.6171,  0.6573,  ...,  1.2901,  0.8270,  0.6358]],\n \n          [[-0.4614, -0.3424, -0.8230,  ..., -0.4962, -1.5664,  0.1716],\n           [-0.8702, -0.2004, -1.1225,  ...,  0.1262, -0.0449, -0.3804],\n           [-0.9319,  0.6336,  0.2594,  ..., -0.0771,  0.2385, -0.7438],\n           ...,\n           [ 0.2437,  0.6129, -0.1953,  ..., -0.9430, -0.5727, -0.3930],\n           [ 0.7526,  0.1055, -0.0079,  ..., -0.8413, -0.8137, -0.6475],\n           [ 0.0139,  0.5125,  0.2291,  ..., -1.1772, -1.5131, -0.7553]],\n \n          [[ 0.8944, -0.2116, -0.5652,  ...,  0.1892,  0.2971,  0.1462],\n           [-1.3356, -0.6864, -1.0437,  ..., -0.4824, -0.7962, -1.1238],\n           [-0.9609,  0.0704,  0.8362,  ...,  0.3419,  0.5325,  1.2056],\n           ...,\n           [ 0.3018,  0.2391,  0.9064,  ...,  0.1919,  1.0013,  1.1558],\n           [ 1.1049,  0.5286,  0.6278,  ...,  0.8334,  0.7843,  1.2067],\n           [-0.0267,  0.4845,  0.8615,  ...,  1.2561,  1.2387,  0.9628]],\n \n          [[-0.1278,  0.4134,  0.2680,  ..., -0.1905, -0.1029,  0.6323],\n           [-0.0822,  1.2040,  0.2312,  ...,  0.8585,  1.3889,  0.9794],\n           [-0.7835,  1.0086,  0.0222,  ...,  0.6680, -0.4235,  0.5068],\n           ...,\n           [-0.3389, -0.1902, -0.2685,  ...,  0.4679,  0.5315,  0.2976],\n           [ 0.1264, -0.6804, -0.1559,  ...,  0.4919,  0.0438, -0.2016],\n           [-0.5029,  0.0836, -0.5352,  ...,  0.8605,  0.7064,  0.4600]]]],\n        device='mps:0'),\n tensor([[[[-0.4768,  1.2656,  1.1770,  ..., -0.2720, -0.1418,  0.5023],\n           [ 0.8909,  1.2233,  0.3623,  ...,  0.9423,  1.2120,  0.3533],\n           [ 0.4194,  0.6685,  0.1495,  ...,  0.6298, -0.4699,  0.8294],\n           ...,\n           [ 0.4459,  0.6751,  0.8361,  ...,  0.4892,  0.2405,  0.0982],\n           [ 0.7856,  0.6024,  0.6936,  ...,  0.1564, -0.1403, -0.4825],\n           [ 0.3960,  0.6990,  0.5088,  ...,  1.1397,  0.8067,  0.7186]],\n \n          [[-0.4617, -0.3378, -0.6968,  ..., -0.4902, -1.5312,  0.1443],\n           [-0.8124, -0.2118, -1.0067,  ...,  0.1121, -0.0425, -0.3149],\n           [-0.9411,  0.6369,  0.2917,  ..., -0.1003,  0.2266, -0.6426],\n           ...,\n           [ 0.2974,  0.5715, -0.1021,  ..., -0.8231, -0.5486, -0.3671],\n           [ 0.6987,  0.1413,  0.0599,  ..., -0.7809, -0.7605, -0.5860],\n           [ 0.0052,  0.4473,  0.2219,  ..., -1.1899, -1.3511, -0.6881]],\n \n          [[ 0.8166, -0.2340, -0.5569,  ...,  0.1887,  0.2895,  0.2177],\n           [-1.2997, -0.6252, -1.1339,  ..., -0.4886, -0.7597, -1.1321],\n           [-0.9487,  0.1009,  0.7818,  ...,  0.3501,  0.4955,  1.1704],\n           ...,\n           [ 0.2944,  0.4377,  0.8584,  ...,  0.3715,  1.0714,  1.0166],\n           [ 0.9692,  0.4232,  0.5649,  ...,  0.9226,  0.7988,  1.0707],\n           [ 0.0407,  0.5053,  0.8963,  ...,  1.1398,  1.1104,  0.9750]],\n \n          [[-0.1296,  0.4381,  0.1608,  ..., -0.1927, -0.1350,  0.5789],\n           [-0.0320,  1.1365,  0.0937,  ...,  0.8757,  1.3188,  0.8449],\n           [-0.6455,  0.9235,  0.0534,  ...,  0.7877, -0.3574,  0.5104],\n           ...,\n           [-0.4370, -0.1572, -0.2853,  ...,  0.2709,  0.4455,  0.2143],\n           [ 0.1454, -0.4759, -0.2500,  ...,  0.3502,  0.0100, -0.1236],\n           [-0.6766, -0.1301, -0.4475,  ...,  0.7747,  0.5166,  0.6001]]]],\n        device='mps:0'),\n tensor([[[[-0.4764,  1.2593,  1.1797,  ..., -0.2730, -0.1395,  0.5030],\n           [ 0.8858,  1.2245,  0.3650,  ...,  0.9447,  1.2112,  0.3505],\n           [ 0.4162,  0.6718,  0.1468,  ...,  0.6312, -0.4695,  0.8270],\n           ...,\n           [ 0.4639,  0.6733,  0.8252,  ...,  0.4929,  0.2438,  0.1054],\n           [ 0.7798,  0.5978,  0.7053,  ...,  0.1551, -0.1313, -0.4962],\n           [ 0.3988,  0.7034,  0.5051,  ...,  1.1279,  0.8073,  0.7240]],\n \n          [[-0.4618, -0.3399, -0.6955,  ..., -0.4898, -1.5283,  0.1415],\n           [-0.8128, -0.2132, -1.0031,  ...,  0.1101, -0.0426, -0.3138],\n           [-0.9419,  0.6344,  0.2953,  ..., -0.0998,  0.2257, -0.6394],\n           ...,\n           [ 0.3059,  0.5686, -0.1098,  ..., -0.8137, -0.5469, -0.3621],\n           [ 0.6975,  0.1354,  0.0612,  ..., -0.7775, -0.7607, -0.5875],\n           [ 0.0020,  0.4419,  0.2205,  ..., -1.1940, -1.3369, -0.6812]],\n \n          [[ 0.8105, -0.2355, -0.5585,  ...,  0.1873,  0.2883,  0.2198],\n           [-1.2982, -0.6234, -1.1387,  ..., -0.4866, -0.7582, -1.1325],\n           [-0.9515,  0.0998,  0.7788,  ...,  0.3519,  0.4919,  1.1719],\n           ...,\n           [ 0.2954,  0.4487,  0.8609,  ...,  0.3737,  1.0783,  1.0091],\n           [ 0.9662,  0.4205,  0.5661,  ...,  0.9287,  0.8032,  1.0642],\n           [ 0.0458,  0.5115,  0.8978,  ...,  1.1325,  1.1037,  0.9735]],\n \n          [[-0.1278,  0.4397,  0.1548,  ..., -0.1944, -0.1367,  0.5766],\n           [-0.0266,  1.1366,  0.0865,  ...,  0.8782,  1.3178,  0.8367],\n           [-0.6423,  0.9202,  0.0542,  ...,  0.7931, -0.3547,  0.5101],\n           ...,\n           [-0.4544, -0.1617, -0.2817,  ...,  0.2596,  0.4378,  0.2111],\n           [ 0.1464, -0.4761, -0.2674,  ...,  0.3407,  0.0082, -0.1146],\n           [-0.6853, -0.1455, -0.4478,  ...,  0.7792,  0.5050,  0.6157]]]],\n        device='mps:0')]"
  },
  {
    "objectID": "posts/2022-09-26/index.html",
    "href": "posts/2022-09-26/index.html",
    "title": "Why I am writing a cookbook… and so should you",
    "section": "",
    "text": "The Practical Deep Learning for Coders course has a top-down approach to teaching. Let’s not start from the basics, but from our goals. In the first lesson, you learn how to build a classifier that would be science fiction in 2015. Neat! Equally important, thought, is what is implicit: a requirement for a top-down approach to learning:"
  },
  {
    "objectID": "posts/2022-09-26/index.html#what-is-a-fast.ai-cookbook",
    "href": "posts/2022-09-26/index.html#what-is-a-fast.ai-cookbook",
    "title": "Why I am writing a cookbook… and so should you",
    "section": "What is a fast.ai cookbook?",
    "text": "What is a fast.ai cookbook?\nA cookbook is a collection of recipes. A fastai recipe is a jupyter notebook where you compile steps that you can follow to reproduce a result using the library. But you do that in a literate programming style, ie you add explanations and annotations to the code.\n\n\n\n\n\n\n\nplayground\n\n\n\nMany fastai cookbooks use the name playground.\n\nThere are a list of some interesting fast.ai cookbooks in Github:\n\nZachary Mueller’s cookbook\nSylvain’s cookbook\nFred’s old cookbook\nSunhwan’s cookbook\nKechan’s cookbook\nSeverus Snape’s cookbook\n\n\n\n\nFigure 1: An annotated copy of Advanced Potion-Making book belonged to Severus Snape while a Hogwarts student.\n\n\n\nWhy build a cookbook?\nA cookbook enables reproducibility, but as important… it allows you to come back to your work in a few months and understand what you did. It is a gift for your future self.\n\n\n\n\nflowchart LR\n  A(Watch lecture) --> B(Run lesson notebook)\n  B --> C(Reproduce)\n  C --Different dataset--> C\n  C --> D(Share learnings)\n  D:::someclass\n  classDef someclass fill:#f96;\n\n\n\n\n\n\n\n\n\nBesides, when you share your learnings you need to structure your thought and that makes you check for gaps in your understanding and solidifies what you have learned.\n\n\nHow to build a cookbook?\nThe best way to document your fastai learnings is using nbdev, which is the development tool used to build and document fast.ai lib itself. That is how this Fred's fast.ai cookbook was made. You can just fork my repo or follow the nbdev tutorial (that is what I did)."
  },
  {
    "objectID": "posts/2022-10-25/index.html",
    "href": "posts/2022-10-25/index.html",
    "title": "ffc",
    "section": "",
    "text": "no much logistics\nshout outs:\n\nexplaining @sebderhy ideas on guidance (normalization)\nbugs can be beneficial :-D\nstudents rebounding ideas from each other — great!\n@jeremeyhoward with difficulty to keep up\n@strickvl study method\n@_akhaliq at Twitter finds interesting papers\n\nDiffEdit: reading paper together\n\npeer review in Twitter\nworkflow\n\nadd paper to zotero\nread in zotero\n\nsee outline\nread abstract – new task, looks amazing\ninpainting by copy-pasting pixel values outside a user-given mask\nbackground is often the scariest part\n\noften written last and made to impress reviewers\n\nmathpix\ndownload latex of the paper in arxiv\n\n\n\ncourse22p2/nbs/01_matmul.ipynb\n\nFor the week: - read @strickvl method, try to follow this week - revisit lesson 10 - try to implement DiffEdit"
  },
  {
    "objectID": "posts/2022-10-11/index.html",
    "href": "posts/2022-10-11/index.html",
    "title": "Practical Deep Learning For Coders 2022, Lesson 9",
    "section": "",
    "text": "Warning\n\n\n\nRemember not to share links to course recordings or materials.\n\n\n\nYou can share your notes/learnings, but please don’t share links to course recordings or materials.\nResources Needed: – Colab pricing has gone crazy, now is not a bad time to buy a GPU. – Lambda is offering $150 in GPU time. The challenge is that you cannot pause a lambda instance. – For part 2, you may need 16Gb to 24Gb of GPU VRAM for training and 8Gb for inference.\nCheck you are tracking Lesson 9 official topic (end of the page). The “chat” is the official source of information for the lesson.\nThere is a Google calendar for lessons\nLesson 9 is divided into 3 parts (links in Lesson 9 official topic):\n\nThis stream\nLesson 9A from Jonathan Whitaker (@johnowhitaker): A deeper dive on the subject\nLesson 9B from Tanishq (@ilovescience) and Wasim (@seem): About the math. To be released.\n\ndiffusion-nbs repo: things to start to play with stable diffusion\nIt is recommended to take a look at the background links\n\nanother suggestion was Radek’s Meta Learning book\n\nThere will always be a little bit of logistics talk before the recording of each lesson. The official recording starts when slide with title appears.\nStudy groups make learning more fun\n\nThe SF Study Group is the one running longer"
  },
  {
    "objectID": "posts/2022-10-11/index.html#introduction-0000",
    "href": "posts/2022-10-11/index.html#introduction-0000",
    "title": "Practical Deep Learning For Coders 2022, Lesson 9",
    "section": "Introduction [00:00]",
    "text": "Introduction [00:00]\n\n\n\n\nStable Diffusion example with Jeremy Howard as a dwarf (tweet) via Astraea/strmr\n\n\nFirst lesson of part 2: “Deep learning Foundations to Stable Diffusion.”\n(Im)practical: we will learn a lot of details that are not necessary for use but will be essential for research.\nWe will do a quick run on how to use Stable Diffusion\nIf you haven’t done DL before, it will be hard. Strongly suggest doing Part 1 before this part.\nStable diffusion is moving quickly.\n\nEven as of recording, the Lesson Notebook is a little bit outdated.\nBut don’t worry, the foundations don’t change so much.\n\n\n\nWhat has changed from previous courses\n\nNo longer all centred in Jeremy\ninfluenced by Fast.ai alumni\n\n[Jonathan Whitaker (@johnowhitaker)]: first to create educational material\nWasim Lorgat (@seem): extraordinary fastai contributor\nPedro Cuenca (@pcuenq): came to SF last course and it is now at HuggingFace\nTanishq (@ilovescience): now at stability.ai, expertise on medical applications\n\n\n\n\nCompute\nPart 2 requires more computing. Check options in course.fast.ai:\n\nColab is still good but is getting pricier\nPaperspace Gradient\nJarvis Labs: made by fastai alumni and loved by many students\nLambda Labs is the most recent provider. They are the cheapest (at the moment)\nGPU prices are going down"
  },
  {
    "objectID": "posts/2022-10-11/index.html#play-with-stable-diffusion-1630",
    "href": "posts/2022-10-11/index.html#play-with-stable-diffusion-1630",
    "title": "Practical Deep Learning For Coders 2022, Lesson 9",
    "section": "Play with Stable Diffusion! [16:30]",
    "text": "Play with Stable Diffusion! [16:30]\n\nfastai/diffusion-nbs\nreferences to tools and cool stuff\nPlay a lot! It is important to play and learn the capabilities and limitations\nthe community has moved towards keeping code available as colab `notebooks\n\nexample: Deforum\n\nThe best way to learn about prompts is (Lexica.art)[lexica.art]\nBy the end of this course, we will understand how prompts work and go beyond with new data types"
  },
  {
    "objectID": "posts/2022-10-11/index.html#how-to-get-started-with-stable-diffusion-2100",
    "href": "posts/2022-10-11/index.html#how-to-get-started-with-stable-diffusion-2100",
    "title": "Practical Deep Learning For Coders 2022, Lesson 9",
    "section": "How to get started with Stable Diffusion [21:00]",
    "text": "How to get started with Stable Diffusion [21:00]\n\nUsing 🤗 Huggingface Diffusers\n\n\nNotebook\nDiffusers is HuggingFace library for Stable Diffusion\n\nat the moment, the recommended library\nHF 🤗 has done a great good job of being pioneers\nHF pipeline is similar to fastai learn\n\ntorch.manual_seed(1024)\nnum_rows,num_cols = 5,5\npipe.safety_checker = lambda images, clip_input: (images, False)\nimages = concat(pipe(prompt=\"a photograph of an astronaut riding a horse\", num_inference_steps=s, guidance_scale=7).images for s in  list(range(2,100,4)))\n\n\n\nFigure 1: Image output of the inference with a different number of steps (from 2 to 98 steps, increasing 4 steps per image).\n\n\ninference is quite different to what we have been used to in fastai\n\nusage of prompts, guidance scale, etc\nthese models require many steps\nresearch is reducing the number of steps, but good results still require many\n\n\n\n\n\n\n\n\nFred’s observations\n\n\n\nMany steps can be detrimental to the final quality, but this hunch needs more experimentation.\n\n\n\nguidance scale says to what degree we should be focusing on the caption (prompt)\nJH has the feeling that there is something to be done in this function\n\n\n\n\nFigure 2: Image output of the inference with different number degrees in the guidance scale for each row.\n\n\n\n[33:20] negative_prompt: will take the prompt and create a second image that responds to the negative_prompt and subtract from the first one\n\n\n🤗 Diffuser Img2Img Pipeline [34:30]\n\nyou can create something with the composition you are looking for\nyou can use the output of a previous result as input\n\n\n\n\n\n\n\n\n(a) sketch\n\n\n\n\n\n\n\n(b) photo\n\n\n\n\nFigure 3: From Figure 3 (a) to Figure 3 (b) using the img2img pipeline.\n\n\n\n\nFine tunning 🤗 Diffuser model\n\ntextual inversion: you fine tune a single embedding.\n\ngive the concept a name (token)\ngive example pictures of this token and add them to the model\n\nDreambooth [41:15]\n\ntakes a not-so-used token and finetunes just this token"
  },
  {
    "objectID": "posts/2022-10-11/index.html#how-stable-diffusion-works-4455",
    "href": "posts/2022-10-11/index.html#how-stable-diffusion-works-4455",
    "title": "Practical Deep Learning For Coders 2022, Lesson 9",
    "section": "How Stable Diffusion works [44:55]",
    "text": "How Stable Diffusion works [44:55]\n\nWe will use a different explanation to what is commonly explained\n\nit is equally mathematically valid\n\nStart by imagining we want Stable Diffusion to generate something simpler, like handwritten digits\n\n\nAssume there is a black box that takes an image of a handwritten digit and returns the probability that this image is a handwritten digit\n\n\n\n\nFigure 4: Our black box f\n\n\n\nWe can use this black box to generate a new image (of a handwritten digit)\n\n\nwe start with one of the input pixels. Let’s say it is a 28x28 image, 784 pixels.\nwe take one pixel of the image, change it (make it darker or lighter) and see what happens to the probability of the image being a handwritten digit\n\n\n\n\nFigure 5: Changing one pixel and analysing the change to the probability\n\n\n\nwe could do this by each pixel…. but\n\n\nTake \\(\\frac{\\nabla p(X_3)}{\\nabla X_3} \\leftarrow 784 \\text{ values}\\) : the gradient of the probability of the image being a handwritten digit with respect to the pixels of \\(X_3\\)\n\n\nthe values show us how we can change \\(X_3\\) to increase the probability\nwe will do something similar to what we did with the weights of a model, but with the input pixels\n\n\n\n\nFigure 6: The gradient points to the changes needed to the input to make it closer to a handwritten digit\n\n\n\nwe will then apply some constant value (like a learning rate) to the gradient and add it to the image\n\nwe repeat this process many times\n\n\nAssume that f has a f.backward() which gives us the gradient directly:\n\n\nwe don’t particularly need the calculation of the probabilities\n\n\nNow, how to create f? Use a neural network for that:\n\n\nwe can use a dataset of handwritten digits and input random noise on them (to any amount wanted)\nwe want the neural net to predict the noise that was added to the handwritten image\nwe are going to think about neural nets as just a black box of inputs, outputs and a loss function\n\nthe inputs and outputs applied to the loss function changes the weights of the model\n\nwe are building a neural network that predicts the noise\n\n\nWe already know how to do it (Part 1 of the course)\n\n\nwe are done…. because\n\n\nWith such neural net f, we can input a random noise and get the gradient that tells us how to change it to make it more likely to be a handwritten digit\n\n\nfor this nnet, we use a Unet\nthe input is a somewhat noisy image\nthe output is the noise added to the image\n\n\n\n\nFigure 7: Using a model to identify noise in input images.\n\n\n\n\n\n\nInput\nOutput\n\n\n\n\nUnet\nsomewhat noisy images\nthe noise\n\n\n\n\nThe problem we have is that (besides handwritten digits generation) we want to generate 512 x 512 x 3 images which are too big (786,432 pixels)\n\n\ntraining this model by changing images pixel-by-pixel too slow\nhow to do it more efficiently? We know there is a way to compress images (like JPEG)\na way to do it is to use a neural network to compress the image\nwe can then train our Unet with the compressed version of our images\n\n\n\n\nFigure 8: Auto encoder.\n\n\n\n\n\n\nInput\nOutput\n\n\n\n\nUnet\nsomewhat noisy latents\nthe noise\n\n\nVAE’s decoder\nsmall latents tensor\nlarge image\n\n\n\n\nWe use our Unet with somewhat noisy latents and output the noise that was added to the latent. Then we use the decoder to get the resulting image\nBut that was not what we were doing in the beginning; we used prompts to tell what we wanted to generate\n\n\nwhat if we add to the noisy input of the unet the number we want to generate? Add a one-hot-encoding of the possible digits.\nnow our unet will output what are the pixels need to change to create the specific handwritten digit we want to build\nthe digit we want to produce works like guidance to the model\n\n\n\n\n\n\nFigure 9: Guidance\n\n\n\n\n\n\n\n\nFred’s observation\n\n\n\nwhy not add inputs that are not only the description of the image we want but also some classification of style (as an embedding) or action to images (composition, etc)?\n\n\n\nBack to the original problem, how can we generate an image from a prompt like “a cute teddy”?\n\n\nwe cannot one-hot-encode all possible descriptions\nwe need an encoding that represent the image we want (the prompt)\nfor that, we get millions of images from the internet with their alt text descriptions\nwe can then create a model that is a textencoder\nwe pair the output of the text encoder with the output of the image encoder (using the dot product)\nwe build a model that correlates those two encodings\nwe have built a multimodal model for generating encodings\nthat is what we will use instead of one-hot-encodings\nthe model that is used here is named CLIP\nwhere similar text descriptions give us similar embeddings\n\n\n\n\nFigure 10: TextEncoding x ImageEncoding\n\n\n\n\n\n\nInput\nOutput\n\n\n\n\nUnet\nsomewhat noisy latents\nthe noise\n\n\nVAE’s decoder\nsmall latents tensor\nlarge image\n\n\nCLIP\ntext description\nembedding\n\n\n\n\nThe last we need is how to inference\n\n\nwe will avoid use the term “time steps”"
  },
  {
    "objectID": "posts/2022-10-11/index.html#next-lesson",
    "href": "posts/2022-10-11/index.html#next-lesson",
    "title": "Practical Deep Learning For Coders 2022, Lesson 9",
    "section": "Next lesson",
    "text": "Next lesson\n\nlooking inside the pipeline\nthen a huge rewind through the foundations"
  },
  {
    "objectID": "dissertation/ai.html",
    "href": "dissertation/ai.html",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "This chapter defines artificial intelligence, presents the epistemological differences of intelligent agents in history, and discusses their consequences to machine learning theory."
  },
  {
    "objectID": "dissertation/ai.html#artificial-intelligence",
    "href": "dissertation/ai.html#artificial-intelligence",
    "title": "Artificial Intelligence",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\n\n AI is the branch of Computer Science that studies general principles of intelligent agents and how to construct them (Russell, Norvig, and Davis 2010).\n\nThis definition uses the terms intelligence and intelligent agents, so let us start from them.\n\nWhat is intelligence?\nDespite a long history of research, there is still no consensual definition of intelligence.1 Whatever it is, though, humans are particularly proud of it. We even call our species homo sapiens, as intelligence was an intrinsic human characteristic.1 For a list with 70 definitions of intelligence, see .\nIn this dissertation:\n\nIntelligence is the ability to predict a course of action to achieve success in specific goals.\n\n\n\nIntelligent Agents\nUnder our generous definition, intelligence is not limited to humans. It applies to any agent2: animal or machine. For example, a bacteria can perceive its environment through chemical signals, process them, and then produce chemicals to signal other bacteria. An air-conditioning can observe temperature changes, know its state, and adapt its functioning, turning off if it is cold or on if it is hot — intelligence exempts understanding. The air-conditioning does not comprehend what it is doing. The same way a calculator does not know arithmetics.2 An agent is anything that perceives its environment and acts on it.\n\n\nA strange inversion of reasoning\nThis competence without comprehension is what the philosopher Daniel Dennett calls Turing’s strange inversion of reasoning3. The idea of a strange inversion comes from one of Darwin’s 19th-century critics (MacKenzie (1868) as cited by Dennett (2009)):3 In his work, Turing discusses if computers can “think”, meaning to examine if they can perform indistinguishably from the way thinkers do.\n\nIn the theory with which we have to deal, Absolute Ignorance is the artificer; so that we may enunciate as the fundamental principle of the whole system, that, in order to make a perfect and beautiful machine, it is not requisite to know how to make it. This proposition will be found, on careful examination, to express, in condensed form, the essential purport of the [Evolution] Theory, and to express in a few words all Mr Darwin’s meaning; who, by a strange inversion of reasoning, seems to think Absolute Ignorance fully qualified to take the place of Absolute Wisdom in all of the achievements of creative skill. — Robert MacKenzie\n\n\nCounterintuitively to MacKenzie (1868) and many others to this date, intelligence can emerge from absolute ignorance. Turing’s strange inversion of reasoning comes from the realisation that his automata can perform calculations by symbol manipulation, proving that it is possible to build agents that behave intelligently, even if they are entirely ignorant of the meaning of what they are doing (Turing 2007)."
  },
  {
    "objectID": "dissertation/ai.html#dreaming-of-robots",
    "href": "dissertation/ai.html#dreaming-of-robots",
    "title": "Artificial Intelligence",
    "section": "Dreaming of robots",
    "text": "Dreaming of robots\n\nFrom mythology to Logic\nThe idea of creating an intelligent agent is perhaps as old as humans. There are accounts of artificial intelligence in almost any ancient mythology: Greek, Etruscan, Egyptian, Hindu, Chinese (Mayor 2018). For example, in Greek mythology, the story of the bronze automaton of Talos built by Hephaestus, the god of invention and blacksmithing, first mentioned around 700 BC.\nThis interest may explain why, since ancient times, philosophers have looked for mechanical methods of reasoning. Chinese, Indian and Greek philosophers all developed formal deduction in the first millennium BC.In particular, Aristotelian syllogism, laws of thought, provided patterns for argument structures to yield irrefutable conclusions, given correct premises. These ancient developments were the beginning of the field we now call Logic.\n\n\nRationalism: The Cartesian view of Nature\n\n\n\n\nFigure 1: Example of one of Lull’s Ars Magna’s paper discs.\n\nIn the 13th century, the Catalan philosopher Ramon Lull wanted to produce all statements the human mind can think. For this task, he developed logic paper machines, discs of paper filled with esoteric coloured diagrams that connected symbols representing statements. Unfortunately, according to Gardner (1959), in a modern reassessment of his work, “it is impossible, perhaps, to avoid a strong sense of anticlimax” (Gardner 1959). With megalomaniac self-esteem that suggests psychosis, his delusional sense of importance is more characteristic of cult founders. On the bright side, his ideas and books exerted some magic appeal that helped them be rapidly disseminated through all Europe (Gardner 1959).\nLull’s work greatly influenced Leibniz and Descartes, who, in the 17thcentury, believed that all rational thought could be mechanised. This belief was the basis of rationalism, the epistemic view of the Enlightenment that regarded reason as the sole source of knowledge. In other words, they believed that reality has a logical structure and that certain truths are self-evident, and all truths can be derived from them.\nThere was considerable interest in developing artificial languages during this period. Nowadays, they are called formal languages.\n\nIf controversies were to arise, there would be no more need for disputation between two philosophers than between two accountants. For it would suffice to take their pencils in their hands, to sit down to their slates, and to say to each other: Let us calculate. — Gottfried Leibniz\n\nThe rationalist view of the world has had an enduring impact on society until today. In the 19thcentury, George Boole and others developed a precise notation for statements about all kinds of objects in Nature and their relations. Before them, Logic was philosophical rather than mathematical. The name of Boole’s masterpiece, “The Laws of Thought”, is an excellent indicator of his Cartesian worldview.\nAt the beginning of the 20th century, some of the most famous mathematicians, David Hilbert, Bertrand Russel, Alfred Whitehead, were still interested in formalism: they wanted mathematics to be formulated on a solid and complete logical foundation. In particular, Hilbert’s Entscheidungs Problem (decision problem) asked if there were limits to mechanical Logic proofs (Chaitin 2006).\nKurt Gödel’s incompleteness theorem (1931) proved that any language expressive enough to describe arithmetics of the natural numbers is either incomplete or inconsistent. This theorem imposes a limit on logic systems. There will always be truths that will not be provable from within such languages: there are “true” statements that are undecidable.\nAlan Turing brought a new perspective to the Entscheidungs Problem: a function on natural numbers that an algorithm in a formal language cannot represent cannot be computable (Chaitin 2006). Gödel’s limit appears in this context as functions that are not computable,  no algorithm can decide whether another algorithm will stop or not (the halting problem). To prove that, Turing developed a whole new general theory of computation: what is computable and how to compute it, laying out a blueprint to build computers, and making possible Artificial Intelligence research as we know it. An area in which Turing himself was very much invested.\n\n\nEmpiricism: The sceptical view of Nature\n\n\n\n\nFigure 2: David Hume, Scottish Enlightenment philosopher, historian, economist, librarian and essayist.\n\nThe response to rationalism was empiricism, the epistemological view that knowledge comes from sensory experience, our perceptions of the world. Locke explains this with the peripatetic axiom4: “there is nothing in the intellect that was not previously in the senses” (Uzgalis 2020). Bacon, Locke and Hume were great exponents of this movement, which established the grounds of the scientific method.4 This citation is the principle from the Peripatetic school of Greek philosophy and is found in Thomas Aquinas’ work cited by Locke.\nDavid Hume, in particular, presented in the 18th century a radical empiricist view: reason only does not lead to knowledge. In (Hume 2009), Hume distinguishes relations of ideas, propositions that derive from deduction and matters of facts, which rely on the connection of cause and effect through experience (induction). Hume’s critiques, known as the Problem of Induction, added a new slant on the debate of the emerging scientific method.\nFrom Hume’s own words:\n\nThe bread, which I formerly eat, nourished me; that is, a body of such sensible qualities was, at that time, endued with such secret powers: but does it follow, that other bread must also nourish me at another time, and that like sensible qualities must always be attended with like secret powers? The consequence seems nowise necessary. — David Hume\n\nThere is no logic to deduce that the future will resemble the past. Still, we expect uniformity in Nature. As we see more examples of something happening, it is wise to expect that it will happen in the future just as it did in the past. There is, however, no rationality5 in this expectation.5 In the philosophical sense.\nHume explains that we see conjunction repeatedly, “bread” and “nourish”, and we expect uniformity in Nature; we hope that “nourish” will always follow “eating bread”; When we fulfil this expectancy, we misinterpret it as causation. In other words, we project causation into phenomena. Hume explained that this connection does not exist in Nature. We do not “see causation”; we create it.\nThis projection is Hume’s strange inversion of reasoning (Huebner 2017): We do not like sugar because it is sweet; sweetness exists because we like (or need) it. There is no sweetness in honey. We wire our brain so that glucose triggers a labelled desire we call sweetness. As we will see later, sweetness is information. This insight shows the pattern matching nature of humans. Musicians have relied on this for centuries. Music is a sequence of sounds in which we expect a pattern. The expectancy is the tension we feel while the chords progress. When the progression finally resolves, forming a pattern, we release the tension. We feel pattern matching in our core. It is very human, it can be beneficial and wise, but it is, stricto sensu, irrational.\nThe epistemology of the sceptical view of Nature is science: to weigh one’s beliefs to the evidence. Knowledge is not absolute truth but justified belief. It is a Babylonian epistemology.\nIn rationalism, Logic connects knowledge and good actions. In empiricism, the connection between knowledge and justifiable actions is determined by probability. More specifically, Bayes’ theorem. As Jaynes puts it, probability theory is the “Logic of Science” . 66 The Bayes’ theorem is attributed to the Reverend Thomas Bayes after the posthumous publication of his work. By the publication time, it was an already known theorem, derived by Laplace.\n\n\nThe birth of AI as a research field\n\n\n\n\nFigure 3: Claude Shannon, father of “information theory”.\n\nIn 1943, McCulloch and Pitts, a neurophysiologist and a logician, demonstrated that neuron-like electronic units could be wired together, act and interact by physiologically plausible principles and perform complex logical calculations (Russell, Norvig, and Davis 2010). Moreover, they showed that any computable function could be computed by some network of connected neurons (McCulloch and Pitts 1943). Their work marks the birth of ANNs, even before the field of AI had this name. It was also the birth of Connectionism, using artificial neural networks, loosely inspired by biology, to explain mental phenomena and imitate intelligence.\nTheir work inspired John von Neumann’s demonstration of how to create a universal Turing machine out of electronic components, which lead to the advent of computers and programming languages. Ironically, these advents hastened the ascent of the formal logicist approach called Symbolism, disregarding Connectionism.\nIn 1956, John McCarthy, Claude Shannon, Marvin Minsky and Nathaniel Rochester organised a 2-month summer workshop in Dartmouth College to bring researchers of different fields concerned with “thinking machines” (cybernetics, information theory, automata theory). The workshop attendees became a community of researchers and chose the term “artificial intelligence” for the field.\n\n\n\n\nThe Blind Men and the Elephant.\n\n\n\n\nIt was six men of Indostan\nTo learning much inclined,\nWho went to see the Elephant\n(Though all of them were blind),\nThat each by observation\nMight satisfy his mind\n—John Godfrey Saxe,\n \nThe Blind Men and the Elephant"
  },
  {
    "objectID": "dissertation/ai.html#building-intelligent-agents",
    "href": "dissertation/ai.html#building-intelligent-agents",
    "title": "Artificial Intelligence",
    "section": "Building Intelligent Agents",
    "text": "Building Intelligent Agents\n\nAnatomy of intelligent agents\nLike the blind men in the parable, an intelligent agent shall model her understanding of Nature from limited sensory data.\n\n\n\nFigure 4: Anatomy of an Intelligent Agent. Inspired by art in (Russell, Norvig, and Davis (2010))\n\n\nThus, an agent perceives her environment with sensors, treat sensory data as facts and use these facts to possibly update her model of Nature, use the model to decide her actions, and acts via her actuators. In a way, agents continually communicate with Nature in a perception/action conversation ([fig-anatomy]).\nThe expected result of this conversation is a change in the agent’s KB, therefore in her model and, more importantly, her future decisions. The model is an abstraction of how the agent “thinks” the world is (her “mental picture” of the environment). Therefore, it should be consistent with it: if something is true in Nature, it is equally valid, mutatis mutandis, in the model. A Model should also be as simple as possible so that the agent can make decisions that maximise a chosen performance measure, but not simpler. As the agent knows more about Nature, less it gets surprised by it.\nThis rudimentary anatomy is flexible enough to entail different epistemic views, like the rationalist (mathematical) and the empiricist (scientific); different approaches to how to implement the knowledge base (it can be learned, therefore updatable, or it can be set in stone from an expert prior knowledge); and also from how to implement it (a robot or software).\nNoteworthy, though, is that the model that transforms input data into decisions should be the target of our focus.\n\n\nSymbolism\nSymbolism is the pinnacle of rationalism. In the words of Thomas Hobbes, one of the forerunners of rationalism, “thinking is the manipulation of symbols and reasoning is computation”. Symbolism is the approach to building intelligent agents that does just that. It attempts to represent knowledge with a formal language and explicitly connects the knowledge with actions. It is competence from comprehension. In other words, it is programmed.\nEven though McCulloch and Pitts work on artificial neural networks predates Von Neumann’s computers, Symbolism dominated AI until the \\(1980\\)s. It was so ubiquitous that symbolic AI is even called “good old fashioned AI” (Russell, Norvig, and Davis 2010).\nThe symbolic approach can be traced back to Nichomachean Ethics (Aristotle 2000):\n\nWe deliberate not about ends but means. For a doctor does not deliberate whether he shall heal, nor an orator whether he shall persuade, nor a statesman whether he shall produce law and order, nor does anyone else deliberate about his end. They assume the end and consider how and by what means it is to be attained; and if it seems to be produced by several means, they consider by which it is most easily and best produced, while if it is achieved by one only they consider how it will be achieved by this and by what means this will be achieved, till they come to the first cause, which in the order of discovery is last.\n— Aristotle \n\nThis perspective is so entrenched that Russell, Norvig, and Davis (2010, 7) still says: “(\\(\\ldots\\)) Only by understanding how actions can be justified can we understand how to build an agent whose actions are justifiable”; even though, in the same book, they cover machine learning (which we will address later in this chapter) without noticing it is proof that there are other ways to build intelligent agents. Moreover, it is also a negation of competence without comprehension. It seems that even for AI researchers, the strange inversion of reasoning is uncomfortable ([ch:introduction]).\nAll humans, even those in prisons and under mental health care, think their actions are justifiable. Is that not an indication that we rationalise our actions ex post facto? We humans tend to think our rational assessments lead to actions, but it is also likely possible that we act and then rationalise afterwards to justify what we have done, fullheartedly believing that the rationalisation came first.\n\nClaude Shannon’s Theseus\nAfter writing what is probably the most important master’s dissertation of the 20th century and “inventing” IT, what made possible the Information Age we live in today, Claude Shannon enjoyed the freedom to pursue any interest to which his curious mind led him (Soni and Goodman 2017). In the \\(1950\\)s, his interest shifted to building artificial intelligence. He was not a typical academic, in any case. A lifelong tinkerer, he liked to “think” with his hand as much as with his mind. Besides developing an algorithm to play chess (when he even did not have a computer to run it), one of his most outstanding achievements in AI was Theseus, a robotic maze-solving mouse.77 Many AI students will recognise in Theseus the inspiration to Russel and Norvig’s Wumpus World .\nTo be more accurate, Theseus was just a bar magnet covered with a sculpted wooden mouse with copper whiskers; the maze was the “brain” that solved itself (Klein 2018).\n\n“Under the maze, an electromagnet mounted on a motor-­powered carriage can move north, south, east, and west; as it moves, so does Theseus. Each time its copper whiskers touch one of the metal walls and complete the electric circuit, two things happen. First, the corresponding relay circuit’s switch flips from”on” to “off,” recording that space as having a wall on that side. Then Theseus rotates \\(90^{\\circ}\\) clockwise and moves forward. In this way, it systematically moves through the maze until it reaches the target, recording the exits and walls for each square it passes through.” — Klein (2018).\n\n\n\nSymbolic AI problems\nSeveral symbolic AI projects sought to hard-code knowledge about domains in formal languages, but it has always been a costly, slow process that could not scale.\nAnyhow, by \\(1965\\), there were already programs that could solve any solvable problem described in logical notation (Russell, Norvig, and Davis 2010, 4). However, hubris and lack of philosophical perspective made computer scientists believe that “intelligence was a problem about to be solved8.”8 Marvin Minsky, head of the artificial intelligence laboratory at MIT (\\(1967\\))\nThose inflated expectations lead to disillusionment and funding cuts9 (Russell, Norvig, and Davis 2010). They failed to estimate the inherent difficulty in slating informal knowledge in formal terms: the world has many shades of grey. Besides, complexity theory had yet to be developed: they did not count on the exponential explosion of their problems.9 Sometimes called winters.\n\n\n\nConnectionism: a different approach\nThe fundamental idea in Connectionism is that intelligent behaviour emerges from a large number of simple computational units when networked together (Goodfellow, Bengio, and Courville 2016).\nIt was pioneered by McCulloch and Pitts in 1943 (McCulloch and Pitts 1943). One of Connectionism’s first wave developments was Frank Rosenblatt’s Perceptron, an algorithm for learning binary classifiers, or more specifically threshold functions: \\[\\begin{aligned}\n    y=\n    \\begin{cases}\n        1 \\text{ if } \\mW\\vx + \\vb > 0\\\\\n        0 \\text{ otherwise }\n    \\end{cases}\n\\end{aligned}\\] where \\(\\mW\\) is the vector of weights, \\(\\vx\\) is the input vector, \\(\\vb\\) is a bias, and \\(\\vy\\) is the classification. In neural networks, a perceptron is an artificial neuron using a step function as the activation function.\n\n\n\n\n\n\nFigure 5: Building in Harare, Zimbabwe, modelled after termite mounds. Photo by Mike Pearce.\n\n\n\n\n\n\n\nFigure 6: Cathedral termite mound, Australia. Photo by Awoisoak Kaosiowa, 2008.\n\n\n\n\n\n\nBiomimicry of termite technique achieves superior energy efficiency in buildings.\n\n\n\nSee [fig-termite_cathedral], termites self-cooling mounds keep the temperature inside at exactly \\(31^{\\circ} C\\), ideal for their fungus-farming; while the temperatures outside range from 2 to \\(40^{\\circ} C\\) throughout the day. Such building techniques inspired architect Mike Pearce to design a shopping mall that uses a tenth of the energy used by a conventional building of the same size.\nFrom where does termites intelligence come?\n\nIndividual termites react rather than think, but at a group level, they exhibit a kind of cognition and awareness of their surroundings. Similarly, in the brain, individual neurons do not think, but thinking arises in their connections. — Radhika Nagpal, Harvard University (Margonelli 2016).\n\nSuch collective intelligence happens in groups of just a couple of million termites. There are around 80 to 90 billion neurons in the human brain, each less capable than a termite, but collectively they show incomparable intelligence capabilities.\n\n\n\nFigure 7: A brief history of connectionism. Adapted from (Tishby (2020)).\n\n\nIn contrast with the symbolic approach, in neural networks, the knowledge is not explicit in symbols but implicit in the strength of the connections between the neurons. Besides, it is a very general and flexible approach since these connections can be updated algorithmically: they are algorithms that learn: the connectionist approach is an example of what we now call Machine Learning.\n\n\nMachine Learning\n\n\n\n\nFigure 8: Is this a cat?\n\nLook at [fig-lulu]. Is this a picture of a cat? How to write a program to do such a simple classification task (cat/no cat)? One could develop clever ways to use features from the input picture and process them to guess. Though, it is not an easy program to design. Worse, even if one manages to program such a task, how much would it worth to accomplish a related task, to recognise a dog, for example? For long, this was the problem of researchers in many areas of interest of AI:CV, NLP, Speech Recognition SR; much mental effort was put, with inferior results, in problems that we humans solve with apparent ease.\nThe solution is an entirely different approach for building artificial intelligence: instead of making the program do the task, build the program that outputs the program that does the task. In other words, learning algorithms use “training data” to infer the transformations to the input that generates the desired output.\n\nTypes of learning\nMachine Learning can happen in different scenarios, which differ in the availability of training data, how training data is received, and how the test data is used to evaluate the learning. Here, we describe the most typical of them (Mohri, Rostamizadeh, and Talwalkar 2012):\n\nSupervised learning: The most successful scenario. The learner receives a set of labelled examples as training data and makes predictions for unseen data.\nUnsupervised learning: The learner receives unlabelled training data and makes predictions for unseen instances.\nSemi-supervised learning: The learner receives a training sample consisting of labelled and unlabelled data and makes predictions for unseen examples. Semi-supervised learning is usual in settings where unlabelled data is easily accessible, but labelling is too costly.\nReinforcement learning: The learner actively interacts with the environment and receives an immediate reward for her actions. The training and testing phases are intermixed.\n\n\n\n\nDeep Learning\nThe \\(2010\\)s have been an AI Renaissance not only in academia but also in the industry. Such successes are mostly due to DL, in particular, supervised deep learning with vast amounts of data trained in GPUs. It was the decade of DL.\n\n“Deep learning algorithms seek to exploit the unknown structure in the input distribution to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features.” — Joshua Bengio (Bengio 2012)\n\nThe name is explained by  Goodfellow, Bengio, and Courville (2016): “A graph showing the concepts being built on top of each other is a deep graph. Therefore the name, deep learning” (Goodfellow, Bengio, and Courville 2016). Although it is a direct descendant of the connectionist movement, it goes beyond the neuroscientific perspective in its modern form. It is more a general principle of learning multiple levels of compositions.\nThe quintessential example of a deep learning model is the deep feedforward network or MLP (Russell, Norvig, and Davis 2010).\n\nLet,\n\nbe the input vector \\(\\{\\vx_1, \\ldots, \\vx_m\\}\\)\nbe the layer index, such that \\(k \\in [1,l]\\),\nbe the matrix of weights in the \\(k\\)-th layer, where \\(i \\in [0,d_{k-1}], j \\in [1, d_k] \\text{ and }\\mW^{(k)}_{0,:}\\) are the biases\nbe a nonlinear function,\n\na MLPs is a neural network where the input is defined by: $$\n\\[\\begin{aligned}\n        h^{(0)}= 1^\\frown \\vx,\n    \n\\end{aligned}\\]\n\\[ a hidden layer is defined by: \\]\n\\[\\begin{aligned}\n        h^{(k)}&=\\sigma^{(k)}(\\mW^{(k)~\\top} h^{(k-1)}).\n    \n\\end{aligned}\\]\n\\[ The output is defined by: \\]\n\\[\\begin{aligned}\n        \\hat{y}&=h^{(l)}.\n    \n\\end{aligned}\\]\n$$\n\nDeep Learning is usually associated with DNNs, but the network architecture is only one of its components:\n\nDNN architecture\nSGD — the optimiser\nDataset\nLoss function\n\nThe architecture is not the sole component essential to current Deep Learning success. The SGD plays a crucial role, and so does the usage of large datasets.\nA known problem, though, is that DNNs are prone to overfitting ([sec-bias-variance]).  Zhang et al. (2016) show state-of-the-art convolutional deep neural networks can easily fit a random labelling of training data (Zhang et al. 2016)."
  },
  {
    "objectID": "dissertation/ai.html#concluding-remarks",
    "href": "dissertation/ai.html#concluding-remarks",
    "title": "Artificial Intelligence",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThis chapter derived the need for a language from the definitions of intelligence and intelligent agents. An intelligent agent needs language to store her knowledge (what she has learned) and with that to communicate/share this knowledge with its future self and with other agents.\nWe claim (without proving) that a language can be derived from a definition of knowledge: an epistemic choice. We claim that mathematics and science can be seen as languages that differ in consequence of different views on what knowledge is and gave historical background on two epistemic views, Rationalism and Empiricism ([sec-rationalism,sec-empiricism]).\nWe gave historical background on AI and showed that different epistemic views relate to AI movements: Symbolism and Connectionism. We gave some background on basic AI concepts: intelligent agents, machine learning, types of learning, neural networks and deep learning, showing that DL relates to Connectionism and, hence, to science and an empiricist epistemology. Previously ([sec-bringing_science]), we have discussed that Computer Science generally relates to the rationalist epistemology. We hope this can help us better understand our research community.\n\nAssumptions\n\nA definition of intelligence ([def-intelligence])\nAn epistemic choice on the definition of Knowledge ([sec-rationalism,sec-empiricism])"
  },
  {
    "objectID": "dissertation/intro.html",
    "href": "dissertation/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In his acceptance speech for the Test-of-Time award in NeurIPS 2017,1 Ali Rahimi2 started a controversy by frankly declaring (Rahimi 2018). His concerns on the lack of theoretical understanding of machine learning for critical decision-making are rightful:1 Conference on Neural Information Processing.2 Research Scientist, Google.\nBoth researchers, at least, agree upon one thing: the practice of machine learning has outpaced its theoretical development. That is certainly a research opportunity."
  },
  {
    "objectID": "dissertation/intro.html#problem",
    "href": "dissertation/intro.html#problem",
    "title": "Introduction",
    "section": "Problem",
    "text": "Problem\n\n\n\n\n\nFigure 2: ?(caption)\n\n\nSource: https://xkcd.com/1838/. Reprinted with permission.\nIn the last decade, we have witnessed a myriad of astonishing successes in Deep Learning. Despite those many successes in research and industry applications, we may again be climbing a peak of inflated expectations. If in the past, the false solution was to “add computation power” on problems, today we try to solve them by “piling data”([[fig-machine_learning_2x]][2]). Such behaviour has triggered a winner-takes-all competition for who collects more data (our data) amidst a handful of large corporations, raising ethical concerns about privacy and concentration of power (O’Neil 2016).\nNevertheless, we know that learning from way fewer samples is possible: humans show a much better generalisation ability than our current state-of-the-art artificial intelligence. To achieve such needed generalisation power, we may need to understand better how learning happens in deep learning. Rethinking generalisation might reshape the foundations of machine learning theory (Zhang et al. 2016).\n\nPossible new explanation in the horizon\nIn \\(2015\\),  Tishby and Zaslavsky (2015) proposed a theory of deep learning  (Tishby and Zaslavsky 2015) based on the information-theoretical concept of the bottleneck principle, of which Tishby is one of the authors. Later, in 2017,  Shwartz-Ziv and Tishby (2017) followed up on the IBT with the paper  , which was presented in a well-attended workshop8, with appealing visuals that clearly showed a “phase transition” happening during training. The video posted on Youtube (Tishby 2017) became a “sensation”9, and received a wealth of publicity when well-known researchers like Geoffrey Hinton10, Samy Bengio (Apple) and Alex Alemi (Google Research) have expressed interest in Tishby’s ideas (Wolchover 2017). they are called formal languages.8 Deep Learning: Theory, Algorithms, and Applications. Berlin, June 2017 http://doc.ml.tu-berlin.de/dlworkshop20179 By the time of this writing, this video as more than \\(84,000\\) views, which is remarkable for an hour-long workshop presentation in an academic niche. https://youtu.be/bLqJHjXihK810 Another Deep Learning Pioneer and Turing award winner (2018).\n\nI believe that the information bottleneck idea could be very important in future deep neural network research. — Alex Alemi\n\nAndrew Saxe (Harvard University) rebutted  Shwartz-Ziv and Tishby (2017) claims in   and was followed by other critics. According to Saxe, it was impossible to reproduce  (Shwartz-Ziv and Tishby 2017)’s experiments with different parameters.\nHas the initial enthusiasm on the IBT been unfounded? Have we let us “fool ourselves” by beautiful charts and a good story?\n\n\nProblem statement\nThe practice of modern machine learning has outpaced its theoretical development. In particular, deep learning models present generalisation capabilities unpredicted by the current machine learning theory. There is yet no established new general theory of learning which handles this problem.\nIBT was proposed as a possible new theory with the potential of filling the theory-practice gap. Unfortunately, to the extent of our knowledge, there is still no comprehensive digest of IBT nor an analysis of how it relates to current MLT."
  },
  {
    "objectID": "dissertation/intro.html#objective",
    "href": "dissertation/intro.html#objective",
    "title": "Introduction",
    "section": "Objective",
    "text": "Objective\nThis dissertation aims to investigate to what extent can the emergent Information Bottleneck Theory help us better understand Deep Learning and its phenomena, especially generalisation, presenting its strengths, weaknesses and research opportunities.\n\nResearch Questions"
  },
  {
    "objectID": "dissertation/intro.html#methodology",
    "href": "dissertation/intro.html#methodology",
    "title": "Introduction",
    "section": "Methodology",
    "text": "Methodology\n\nGiven that IBT is yet not a well-established learning theory, there were two difficulties that the research had to address:\n\nThere is a growing interest in the subject, and new papers are published every day. It was essential to select literature and restrain the analysis.\nEarly on, the marks of an emergent theory in its infancy manifested in the form of missing assumptions, inconsistent notation, borrowed jargon, and seeming missing steps. Foremost, it was unclear what was missing from the theory and what was missing in our understanding.\n\nAn initial literature review on IBT was conducted to define the scope.11 We then chose to narrow the research to theoretical perspective on generalisation, where we considered that it could bring fundamental advances. We made the deliberate choice of going deeper in a limited area of IBT and not broad, leaving out a deeper experimental and application analysis, all the work on ITL12  (Principe 2010) and statistical-mechanics-based analysis of SGD  (P. Chaudhari and Soatto 2018; Pratik Chaudhari et al. 2019). From this set of constraints, we chose a list of pieces of IBT literature to go deeper ([[ch:literature]][3]).\nIn order to answer , we discuss the epistemology of AI to choose fundamental axioms (definition of intelligence and the definition of knowledge) with which we deduced from the ground up MLT, IT and IBT, revealing hidden assumptions, pointing out similarities and differences. By doing that, we built a “genealogy” of these research fields. This comparative study was essential for identifying missing gaps and research opportunities.\nIn order to answer , we first dissected the selected literature ([[ch:literature]][3]) and organised scattered topics in a comprehensive sequence of subjects.\nIn the process of the literature digest, we identified results, strengths, weaknesses and research opportunities.\n\n11 Not even the term IBT is universally adopted.12 ITL makes the opposite path we are taking, bringing concepts of machine learning to information theory problems."
  },
  {
    "objectID": "dissertation/intro.html#contributions",
    "href": "dissertation/intro.html#contributions",
    "title": "Introduction",
    "section": "Contributions",
    "text": "Contributions\nIn the research conducted, we produced three main results that, to the extent of our knowledge, are original:\n\nThe dissertation itself is the main expected result: a comprehensive digest of the IBT literature and a snapshot analysis of the field in its current form, focusing on its theoretical implications for generalisation.\nWe propose an Information-Theoretical learning problem different from MDL proposed by  (Hinton and Van Camp 1993) for which we derived bounds using Shannon’s . These results, however, are only indicative as they lack peer review to be validated.\nWe present a critique on Achille (2019)’s explanation(Achille 2019; Achille and Soatto 2018) for the role of layers in Deep Representation in the IBT perspective ([[sec-achille_proof_critique]][4]), pointing out a weakness in the argument that, as far as we know, has not yet been presented. We then propose a counter-intuitive hypothesis that layers reduce the model’s “effective” hypothesis space. This hypothesis is not formally proven in the present work, but we try to give the intuition behind it ([[sec-proposed_hypothesis]][5]). This result has not yet been validated as well."
  },
  {
    "objectID": "dissertation/intro.html#dissertation-preview-and-outline",
    "href": "dissertation/intro.html#dissertation-preview-and-outline",
    "title": "Introduction",
    "section": "Dissertation preview and outline",
    "text": "Dissertation preview and outline\n\n\n\n\n\nFigure 3: ?(caption)\n\n\nIBT “Genealogy” tree\nThe dissertation is divided into two main parts ([[pt:background]][6] and [[pt:emergence_of_theory]][7]), with a break in the middle ([[pt:intermezzo]][8]) Figure 3.\n\nBackground ([[pt:background]][6])\n\nChapter 2–Artificial Intelligence: The chapter defines what artificial intelligence is, presents the epistemological differences of intelligent agents in history, and discusses their consequences to machine learning theory.\nChapter 3 — Probability Theory: The chapter derives propositional calculus and probability theory from a list of desired characteristics for epistemic agents. It also presents basic Probability Theory concepts.\nChapter 4 — Machine Learning Theory: The chapter presents the theoretical framework of Machine Learning, the PAC model, theoretical guarantees for generalisation, and expose its weaknesses concerning Deep Learning phenomena.\nChapter 5 — Information Theory: The chapter derives Shannon Information from Probability Theory, explicates some implicit assumptions, and explains basic Information Theory concepts.\n\nIntermezzo ([[pt:intermezzo]][8])\n\nChapter 6 — Information-Theoretical Epistemology: This chapter closes the background part and opens the IBT part of the dissertation. It shows the connection of IT and MLT in the learning problem, proves that Shannon theorems can be used to prove PAC bounds and present the MDL Principle, an earlier example of this kind of connection.\n\nThe emergence of a theory ([[pt:emergence_of_theory]][7])\n\nChapter 7 — IB Principle: Explains the IB method and its tools: KL as a natural distortion (loss) measure, the IB Lagrangian and the Information Plane.\nChapter 8 — IB and Representation Learning: Presents the learning problem in the IBT perspective (not specific to DL). It shows how some usual choices of the practice of DL emerge naturally from a list of desired properties of representations. It also shows that the information in the weights bounds the information in the activations.\nChapter 9 — IB and Deep Learning: This chapter presents the IBT perspective specific to Deep Learning. It presents IBT analysis of Deep Learning training, some examples of applications of IBT to improve or create algorithms; and the IBT learning theory of Deep Learning. We also explain Deep Learning phenomena in the IBT perspective.\nChapter 10 — Conclusion: In this chapter, we present a summary of the findings, answer the research questions, and present suggestions for future work.\n\n\nWe found out that IBT does not invalidate MLT; it just interprets complexity not as a function of the data (number of parameters) but as a function of the information contained in the data. With this interpretation, there is no paradox in improving generalisation by adding layers.\nFurthermore, they both share more or less the same “genealogy” of assumptions. IBT can be seen as particular case of MLT. Nevertheless, IBT allows us to better understand the training process and provide a different narrative that helps us comprehend Deep Learning phenomena in a more general way."
  },
  {
    "objectID": "05-sd-exploration.html",
    "href": "05-sd-exploration.html",
    "title": "Exploring Stable Diffusion",
    "section": "",
    "text": "This code is based on Stable Diffusion Deep Dive notebook by @johnowhitaker"
  },
  {
    "objectID": "05-sd-exploration.html#exploring-clip-embeddings",
    "href": "05-sd-exploration.html#exploring-clip-embeddings",
    "title": "Exploring Stable Diffusion",
    "section": "Exploring CLIP embeddings",
    "text": "Exploring CLIP embeddings\nFirst, let’s simplify the unconditioning embedding code. From Stable Diffusion Deep Dive notebook, we have:\n\n# Prep text \nprompt = [\"A watercolor painting of an otter\"]\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]; max_length\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings]).half()\ntext_embeddings.shape\n\ntorch.Size([2, 77, 768])\n\n\n\nlatents = sdloop(generate_seed_latent(), text_embeddings)\nlatents_to_pil(latents.to(torch.float32))[0]\n\n\n\n\n\n\n\nbut we could simplify this with:\n\nprompts = [\"\", \"A watercolor painting of an otter\"]\ninputs = processor(text=prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    t_e = text_encoder(input_ids=inputs.input_ids.to(torch_device))[0].half()\nt_e.shape\n\ntorch.Size([2, 77, 768])\n\n\n\nlatents = sdloop(generate_seed_latent(), t_e)\notter = latents_to_pil(latents.to(torch.float32))[0];otter\n\n\n\n\n\n\n\nNotice also that:\n\ntext_encoder(input_ids=inputs.input_ids.to(torch_device))\n\nBaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n         [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n         ...,\n         [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n         [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n         [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n\n        [[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [-0.2279,  0.0401,  1.4532,  ..., -0.8664, -0.0721,  1.8452],\n         ...,\n         [ 0.4580,  0.4419,  1.0750,  ..., -1.6370, -1.5812, -0.2933],\n         [ 0.4597,  0.4174,  1.0474,  ..., -1.6088, -1.5758, -0.2907],\n         [ 0.4342,  0.4253,  1.1790,  ..., -1.6748, -1.5393, -0.3574]]],\n       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n        [ 0.2317,  0.7205,  0.9957,  ..., -2.3833, -0.6834,  0.2252]],\n       device='cuda:0', grad_fn=<IndexBackward0>), hidden_states=None, attentions=None)\n\n\ni.e. the encoder returns not only 2 vectors representing the 2 prompts in the input:\n\ntext_encoder(input_ids=inputs.input_ids.to(torch_device))[1].shape\n\ntorch.Size([2, 768])\n\n\nbut also the last_hidden_state, i.e the layer that will produce these vectors. This layer is what is added to the unet and conditions the image generation to produce something simillar to the embedding.\n\ntext_encoder(input_ids=inputs.input_ids.to(torch_device))[0].shape\n\ntorch.Size([2, 77, 768])\n\n\nThis weights will be frozen in the unet.\n\nImage embeddings\nAnother interesting point of the processor is that it can generate image embeddings as well. Let us look at it.\n\nimages = [otter]\ninputs = processor(text=prompts, images=images, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    t_e = text_encoder(input_ids=inputs.input_ids.to(torch_device))[0].half()\n    i_e = image_encoder(pixel_values=inputs.pixel_values.to(torch_device))[0].half()\nt_e.shape,  i_e.shape\n\n(torch.Size([2, 77, 768]), torch.Size([1, 257, 1024]))\n\n\nFor some reason, vision_model.embeddings and text_model.embeddings are not the directly comparable\n\ntext_encoder.embeddings, image_encoder.embeddings\n\n(CLIPTextEmbeddings(\n   (token_embedding): Embedding(49408, 768)\n   (position_embedding): Embedding(77, 768)\n ),\n CLIPVisionEmbeddings(\n   (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n   (position_embedding): Embedding(257, 1024)\n ))\n\n\n\n\nComparing Image and Text embeddings\nWe need to project these embeddings to the same space to make them comparable\n\nimage_projection = model.visual_projection.to(torch_device)\ntext_projection = model.text_projection.to(torch_device)\nprompts = [\"\", \"A watercolor painting of an otter\", \"A photograph of an astronaut riding a horse\"]\ninputs = processor(text=prompts,images = images, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")  \nwith autocast('cuda'):\n    with torch.no_grad():\n        vision_outputs = image_encoder(pixel_values=inputs.pixel_values.to(torch_device))\n        text_outputs = text_encoder(input_ids=inputs.input_ids.to(torch_device))\n        img_embeds = image_projection(vision_outputs[1])\n        txt_embeds = text_projection(text_outputs[1])\n        i_proj = img_embeds / img_embeds.norm(p=2, dim=-1, keepdim=True)\n        t_proj = txt_embeds / txt_embeds.norm(p=2, dim=-1, keepdim=True)\ni_proj.shape, t_proj.shape\n\n(torch.Size([1, 768]), torch.Size([3, 768]))\n\n\n\n# cosine similarity as logits\nlogits_per_text = torch.matmul(t_proj, i_proj.t()) \nlogits_per_image = logits_per_text.t()\n\n\nprobs = logits_per_image.softmax(dim=1);probs\n\ntensor([[0.3257, 0.3914, 0.2829]], device='cuda:0')\n\n\nThe image is more similar to the second prompt: “A watercolor painting of an otter”, as expected!\nStill, I was looking for a wider difference among the prompts probabilities."
  },
  {
    "objectID": "05-sd-exploration.html#conditioning-with-image-embeddings",
    "href": "05-sd-exploration.html#conditioning-with-image-embeddings",
    "title": "Exploring Stable Diffusion",
    "section": "Conditioning with Image Embeddings",
    "text": "Conditioning with Image Embeddings\nUnfortunately, the unet only works with the text_encoder hidden states dimensions:\n\nt_e.shape\n\ntorch.Size([2, 77, 768])\n\n\n\nlatents = generate_seed_latent()\nlatent_model_input = torch.cat([latents] * 2)\nwith torch.no_grad():\n    with autocast('cuda'):\n        npred = unet(latent_model_input, scheduler.timesteps[15], encoder_hidden_states=t_e).sample\nnpred.shape\n\ntorch.Size([2, 4, 64, 64])\n\n\n\nlatent_model_input = torch.cat([latents] * 2)\nwith torch.no_grad():\n    with autocast('cuda'):\n        npred = unet(latent_model_input, scheduler.timesteps[15], encoder_hidden_states=i_e).sample\nnpred.shape\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (257x1024 and 768x320)"
  },
  {
    "objectID": "04-CLIP-interrogator.html",
    "href": "04-CLIP-interrogator.html",
    "title": "ffc",
    "section": "",
    "text": "import sys\n# sys.path.append('src/blip')\n# sys.path.append('src/clip')\n\nimport clip\n# import gradio as gr\nimport hashlib\nimport math\nimport numpy as np\nimport os\nimport pickle\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom PIL import Image\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); device\n\ndevice(type='cuda')\n\n\n\nfrom models.blip import blip_decoder\n\n\nfrom share_btn import community_icon_html, loading_icon_html, share_js\n\n\n\nprint(\"Loading BLIP model...\")\nblip_image_eval_size = 384\nblip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'        \nblip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='large', med_config='./src/blip/configs/med_config.json')\nblip_model.eval()\nblip_model = blip_model.to(device)\n\nprint(\"Loading CLIP model...\")\nclip_model_name = 'ViT-L/14' # https://huggingface.co/openai/clip-vit-large-patch14\nclip_model, clip_preprocess = clip.load(clip_model_name, device=device)\nclip_model.to(device).eval()\n\nchunk_size = 2048\nflavor_intermediate_count = 2048\n\n\nclass LabelTable():\n    def __init__(self, labels, desc):\n        self.labels = labels\n        self.embeds = []\n\n        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n\n        os.makedirs('./cache', exist_ok=True)\n        cache_filepath = f\"./cache/{desc}.pkl\"\n        if desc is not None and os.path.exists(cache_filepath):\n            with open(cache_filepath, 'rb') as f:\n                data = pickle.load(f)\n                if data['hash'] == hash:\n                    self.labels = data['labels']\n                    self.embeds = data['embeds']\n\n        if len(self.labels) != len(self.embeds):\n            self.embeds = []\n            chunks = np.array_split(self.labels, max(1, len(self.labels)/chunk_size))\n            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None):\n                text_tokens = clip.tokenize(chunk).to(device)\n                with torch.no_grad():\n                    text_features = clip_model.encode_text(text_tokens).float()\n                text_features /= text_features.norm(dim=-1, keepdim=True)\n                text_features = text_features.half().cpu().numpy()\n                for i in range(text_features.shape[0]):\n                    self.embeds.append(text_features[i])\n\n            with open(cache_filepath, 'wb') as f:\n                pickle.dump({\"labels\":self.labels, \"embeds\":self.embeds, \"hash\":hash}, f)\n    \n    def _rank(self, image_features, text_embeds, top_count=1):\n        top_count = min(top_count, len(text_embeds))\n        similarity = torch.zeros((1, len(text_embeds))).to(device)\n        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).float().to(device)\n        for i in range(image_features.shape[0]):\n            similarity += (image_features[i].unsqueeze(0) @ text_embeds.T).softmax(dim=-1)\n        _, top_labels = similarity.cpu().topk(top_count, dim=-1)\n        return [top_labels[0][i].numpy() for i in range(top_count)]\n\n    def rank(self, image_features, top_count=1):\n        if len(self.labels) <= chunk_size:\n            tops = self._rank(image_features, self.embeds, top_count=top_count)\n            return [self.labels[i] for i in tops]\n\n        num_chunks = int(math.ceil(len(self.labels)/chunk_size))\n        keep_per_chunk = int(chunk_size / num_chunks)\n\n        top_labels, top_embeds = [], []\n        for chunk_idx in tqdm(range(num_chunks)):\n            start = chunk_idx*chunk_size\n            stop = min(start+chunk_size, len(self.embeds))\n            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk)\n            top_labels.extend([self.labels[start+i] for i in tops])\n            top_embeds.extend([self.embeds[start+i] for i in tops])\n\n        tops = self._rank(image_features, top_embeds, top_count=top_count)\n        return [top_labels[i] for i in tops]\n\ndef generate_caption(pil_image):\n    gpu_image = T.Compose([\n        T.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=TF.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n    ])(pil_image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n    return caption[0]\n\ndef load_list(filename):\n    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n        items = [line.strip() for line in f.readlines()]\n    return items\n\ndef rank_top(image_features, text_array):\n    text_tokens = clip.tokenize([text for text in text_array]).to(device)\n    with torch.no_grad():\n        text_features = clip_model.encode_text(text_tokens).float()\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    similarity = torch.zeros((1, len(text_array)), device=device)\n    for i in range(image_features.shape[0]):\n        similarity += (image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n\n    _, top_labels = similarity.cpu().topk(1, dim=-1)\n    return text_array[top_labels[0][0].numpy()]\n\ndef similarity(image_features, text):\n    text_tokens = clip.tokenize([text]).to(device)\n    with torch.no_grad():\n        text_features = clip_model.encode_text(text_tokens).float()       \n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n    return similarity[0][0]\n\ndef interrogate(image):\n    caption = generate_caption(image)\n\n    images = clip_preprocess(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features = clip_model.encode_image(images).float()\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n\n    flaves = flavors.rank(image_features, flavor_intermediate_count)\n    best_medium = mediums.rank(image_features, 1)[0]\n    best_artist = artists.rank(image_features, 1)[0]\n    best_trending = trendings.rank(image_features, 1)[0]\n    best_movement = movements.rank(image_features, 1)[0]\n\n    best_prompt = caption\n    best_sim = similarity(image_features, best_prompt)\n\n    def check(addition):\n        nonlocal best_prompt, best_sim\n        prompt = best_prompt + \", \" + addition\n        sim = similarity(image_features, prompt)\n        if sim > best_sim:\n            best_sim = sim\n            best_prompt = prompt\n            return True\n        return False\n\n    def check_multi_batch(opts):\n        nonlocal best_prompt, best_sim\n        prompts = []\n        for i in range(2**len(opts)):\n            prompt = best_prompt\n            for bit in range(len(opts)):\n                if i & (1 << bit):\n                    prompt += \", \" + opts[bit]\n            prompts.append(prompt)\n\n        prompt = rank_top(image_features, prompts)\n        sim = similarity(image_features, prompt)\n        if sim > best_sim:\n            best_sim = sim\n            best_prompt = prompt\n\n    check_multi_batch([best_medium, best_artist, best_trending, best_movement])\n\n    extended_flavors = set(flaves)\n    for _ in tqdm(range(25), desc=\"Flavor chain\"):\n        try:\n            best = rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n            flave = best[len(best_prompt)+2:]\n            if not check(flave):\n                break\n            extended_flavors.remove(flave)\n        except:\n            # exceeded max prompt length\n            break\n\n    return best_prompt\n\n\nsites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\ntrending_list = [site for site in sites]\ntrending_list.extend([\"trending on \"+site for site in sites])\ntrending_list.extend([\"featured on \"+site for site in sites])\ntrending_list.extend([site+\" contest winner\" for site in sites])\n\nraw_artists = load_list('data/artists.txt')\nartists = [f\"by {a}\" for a in raw_artists]\nartists.extend([f\"inspired by {a}\" for a in raw_artists])\n\nartists = LabelTable(artists, \"artists\")\nflavors = LabelTable(load_list('data/flavors.txt'), \"flavors\")\nmediums = LabelTable(load_list('data/mediums.txt'), \"mediums\")\nmovements = LabelTable(load_list('data/movements.txt'), \"movements\")\ntrendings = LabelTable(trending_list, \"trendings\")\n\n\ndef inference(image):\n    return interrogate(image), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n\ntitle = \"\"\"\n    <div style=\"text-align: center; max-width: 650px; margin: 0 auto;\">\n        <div\n        style=\"\n            display: inline-flex;\n            align-items: center;\n            gap: 0.8rem;\n            font-size: 1.75rem;\n        \"\n        >\n        <h1 style=\"font-weight: 900; margin-bottom: 7px;\">\n            CLIP Interrogator\n        </h1>\n        </div>\n        <p style=\"margin-bottom: 10px; font-size: 94%\">\n        Want to figure out what a good prompt might be to create new images like an existing one? The CLIP Interrogator is here to get you answers!\n        </p>\n    </div>\n\"\"\"\narticle = \"\"\"\n<div style=\"text-align: center; max-width: 650px; margin: 0 auto;\">\n    <p>\n    Example art by <a href=\"https://pixabay.com/illustrations/watercolour-painting-art-effect-4799014/\">Layers</a> \n    and <a href=\"https://pixabay.com/illustrations/animal-painting-cat-feline-pet-7154059/\">Lin Tong</a> \n    from pixabay.com\n    </p>\n\n    <p>\n    Server busy? You can also run on <a href=\"https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb\">Google Colab</a>\n    </p>\n\n    <p>\n    Has this been helpful to you? Follow me on twitter \n    <a href=\"https://twitter.com/pharmapsychotic\">@pharmapsychotic</a> \n    and check out more tools at my\n    <a href=\"https://pharmapsychotic.com/tools.html\">Ai generative art tools list</a>\n    </p>\n</div>\n\"\"\"\n\ncss = '''\n#col-container {max-width: 700px; margin-left: auto; margin-right: auto;}\na {text-decoration-line: underline; font-weight: 600;}\n.animate-spin {\n    animation: spin 1s linear infinite;\n}\n@keyframes spin {\n    from {\n        transform: rotate(0deg);\n    }\n    to {\n        transform: rotate(360deg);\n    }\n}\n#share-btn-container {\n    display: flex; padding-left: 0.5rem !important; padding-right: 0.5rem !important; background-color: #000000; justify-content: center; align-items: center; border-radius: 9999px !important; width: 13rem;\n}\n#share-btn {\n    all: initial; color: #ffffff;font-weight: 600; cursor:pointer; font-family: 'IBM Plex Sans', sans-serif; margin-left: 0.5rem !important; padding-top: 0.25rem !important; padding-bottom: 0.25rem !important;\n}\n#share-btn * {\n    all: unset;\n}\n#share-btn-container div:nth-child(-n+2){\n    width: auto !important;\n    min-height: 0px !important;\n}\n#share-btn-container .wrap {\n    display: none !important;\n}\n'''\n\nwith gr.Blocks(css=css) as block:\n    with gr.Column(elem_id=\"col-container\"):\n        gr.HTML(title)\n\n        input_image = gr.Image(type='pil', elem_id=\"input-img\")\n        submit_btn = gr.Button(\"Submit\")\n        output_text = gr.Textbox(label=\"Output\", elem_id=\"output-txt\")\n\n        with gr.Group(elem_id=\"share-btn-container\"):\n            community_icon = gr.HTML(community_icon_html, visible=False)\n            loading_icon = gr.HTML(loading_icon_html, visible=False)\n            share_button = gr.Button(\"Share to community\", elem_id=\"share-btn\", visible=False)\n\n        examples=[['example01.jpg'], ['example02.jpg']]\n        ex = gr.Examples(examples=examples, fn=inference, inputs=input_image, outputs=[output_text, share_button, community_icon, loading_icon], cache_examples=True, run_on_click=True)\n        ex.dataset.headers = [\"\"]\n        \n        gr.HTML(article)\n\n    submit_btn.click(fn=inference, inputs=input_image, outputs=[output_text, share_button, community_icon, loading_icon])\n    share_button.click(None, [], [], _js=share_js)\n\nblock.queue(max_size=32).launch(show_api=False)"
  },
  {
    "objectID": "vision.segmentation.binary.html",
    "href": "vision.segmentation.binary.html",
    "title": "Binary Image Segmentation (Beginner)",
    "section": "",
    "text": "This tutorial covers binary image segmentation with fastai. It is my self-assigned homework following Practical Deep Learning for Coders 2022 lesson 1."
  },
  {
    "objectID": "vision.segmentation.binary.html#importing-the-library-and-checking-the-dataset",
    "href": "vision.segmentation.binary.html#importing-the-library-and-checking-the-dataset",
    "title": "Binary Image Segmentation (Beginner)",
    "section": "Importing the Library and checking the dataset",
    "text": "Importing the Library and checking the dataset\n\nfrom fastai.vision.all import *\nassert torch.cuda.is_available()\n\nThe dataset chosen for the task was the AISegment.com Matting Human Datasets due to its availability at Kaggle. I downloaded locally with Kaggle cli.\n\npath = Path(\"/run/media/fredguth/datasets/binary-image-segmentation\")\n\n\n!ls $path\n\naisegmentcom-matting-human-datasets.zip  clip_img  matting  matting_human_half\n\n\nThe path contains a folder for the input image (clip_img) and another for the mattings (matting). Inside this folders, there are other folders and finally a file. The leaf file in the clip_img subfolder has a correspondent one in the matting folder.\nAs fastai get_image_file does not guarantee the order, we will need to sort them to garantee the correspondence.\n\nmattings = get_image_files(path/'matting').sorted()\nimages = get_image_files(path/'clip_img').sorted()\nlen(images)\n\n34426\n\n\nThe dataset consist of 34426 images and their correspondent mattings.\n\nOpening image files\nfastai uses Pillow to create images:\n\nimg = PILImage.create(images[144])\nmat = PILMask.create(mattings[144])\nimg.show(figsize=(3,3))\nmat.show(figsize=(3,3))\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\nNow, we will create a function to return a mask of codes, in the way MaskBlock expects.\n\ncodes = np.array(['bg', 'fg'])\n\ndef get_msk(fn):\n    p = Path(str(fn).replace('clip', 'matting').replace('matting_img', 'matting').replace('.jpg', '.png'))\n    msk = (tensor(PILImageBW.create(p))>0)*1\n    msk = msk.to(torch.uint8)\n    return PILMask.create(msk)\nf = images[4]\nm = get_msk(f)\nPILImage.create(f).show(figsize=(3,3))\nPILMask.create(m).show(figsize=(3,3), alpha=1)\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\nNow that we know the expected input and output of our model, let’s build it!"
  },
  {
    "objectID": "vision.segmentation.binary.html#training-our-model",
    "href": "vision.segmentation.binary.html#training-our-model",
    "title": "Binary Image Segmentation (Beginner)",
    "section": "Training our model",
    "text": "Training our model\nTo train a model, we’ll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model – not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it:\n\ndls = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n                   get_items=get_image_files,\n                   splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                   get_y=get_msk,\n                   item_tfms=[Resize(192, method='squish')],\n                   batch_tfms=[Normalize.from_stats(*imagenet_stats)]).dataloaders(path/'clip_img', bs=8)\ndls.vocabs = codes\nname2id = {v:k for k,v in enumerate(codes)}\ndls.show_batch(cmap='Blues', vmin=0, vmax=1, max_n=6)\n\n\n\n\nHere what each of the DataBlock parameters means:\nblocks=(ImageBlock, MaskBlock),\nThe inputs to our model are images, and the outputs are Masks (a tensor of label codes).\nget_items=get_image_files, \nTo find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path).\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nSplit the data into training and validation sets randomly, using 20% of the data for the validation set.\nget_y=get_mask,\nThe labels (y values) is obtained from the input filename by running the function get_mask.\nitem_tfms=[Resize(192, method='squish')]\nBefore training, resize each image to 192x192 pixels by “squishing” it (as opposed to cropping it).\nbatch_tfms=[Normalize.from_stats(*imagenet_stats)]\nApply this list of transforms to each batch. In this case, Normalize uses statistics from the imagenet dataset to normalize the data using imagenet as the sample. Normalization will try to distinguish values as much as possible, taking into account their occurrences in the sample.\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(3)\n\n/home/fredguth/Downloads/.anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/fredguth/Downloads/.anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.082208\n      0.065039\n      19:28\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.058268\n      0.059941\n      22:25\n    \n    \n      1\n      0.040470\n      0.057517\n      20:34\n    \n    \n      2\n      0.055856\n      0.054101\n      18:39\n    \n  \n\n\n\nNow, we can export the model for later use:\n\nlearn.export('bImgSeg.pkl')\n\nLet’s see the results of this model:\n\nlearn.show_results(cmap='Blues', vmin=0, vmax=1, max_n=4)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "# #| export\n# def foo(): pass"
  }
]